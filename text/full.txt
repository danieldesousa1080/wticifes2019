/>
<WORKSHOP DE TECNOLOGIA DA
INFORMAÇÃO E COMUNICAÇÃO
DAS INSTITUIÇÕES  FEDERAIS
DE ENSINO SUPERIOR
13a EDIÇÃO | V. 1
/gid00191
Ligia Maria C. Sousa Cordeiro (UNILAB)
Ricardo de Andrade L. Rabêlo (UFPI)
Eunice P . dos Santos Nunes (UFMT)
Anne Cristine Betoni Cardoso (UFMT)
(Organizadores)Workshop de Tecnologia da
informação e comunicação
das insTiTuições  federais
de ensino superior
13a edição | V. 1
Periodicidade anual | Cuiabá | v. 1 | p. 1 - 390 | 2019Setor Comercial Sul (SCS), Quadra 1, Bloco K, nº 30, salas 801, 802, 803 e 804,
8º andar Edifício Denasa - CEP 70.398-900 - Brasília - DF - Tel. (61) 3321-6341
Home: cgtic.andifes.org.br - E-mail: cgtic@andifes.org.brLuciano Gonda (UFMS)
Coordenador  CGTIC
Apuena Vieira Gomes (UFRN)
Vice-Coordenadora CGTICLigia Maria Carvalho Sousa Cordeiro (UNILAB) 
Coordenadora 
Ricardo de Andrade L. Rabelo (UFPI) 
Vice-coordenadorCorpo Editorial
Aline Barros Tavares (UFRR)
Alexsandro Cardoso Carvalho (UNIFESP)
Amarildo Rolim (UFC)
Ana Marques Paiva (UFMG)
Andres Menéndez (UFS)
Antonio Oseas (UFPI)
Apuena Vieira Gomes (UFRN)
Arinaldo Lopes da Silva (UFPI)
Carlos Alberto M. Basso (UNILA)
Daniel Biasoli (UFFS)
David Dutkievicz (UFOB)
Deize Paula Giusti Consoni (IFSC)
Dorgival Guedes (UFMG)
Edivaldo Cavalcante de Alburquerque Junior (UFPE)
Eduardo Maroñas Monks (UFPEL)
Esrom Bomfim (UFC)
Euclydes Gregório de Melo (UFPI)
Eunice Pereira dos Santos Nunes (UFMT)
Evelyne Avelino (UFC)
Francisco Henrique Cerdeira Ferreira (UNIRIO)
Gustavo Chiapinotto Da Silva (UFSM)
Hubert Ahlert (UFRGS)
Igor Rodrigues Vieira (UFG)
Italo C. Melo Silva (UFAC)
Jeison Santos (UFMT)
Jeniffer de Nadae (UFCA)
João Francisco de Fontoura Vieira (UFRGS)Joaquim B. Cavalcante Neto (UFC)
Jonata Braz Marim dos Santos (UFMT )
José Valdemir dos Reis Junior (UFPI)
Julio C. B. Mattos (UFPEL)
Juliana Silva Herbert (UFCSPA)
Julliany Brandão (UFFS)
Karina Wiechork (UFSM)
Laurinete Ximenes (CEFET RJ)
Luanna Lopes Lobato (UFG)
Luciano Gonda (UFMS)
Marcio Federal Duarte (UFG)
Matheus de Meneses Campanhã Souza (UFPI) 
Márcio André Souto Correia (UFC)
Nonato Rodrigues de Sales Carvalho (UFPI) 
Paulo de Almeida Afonso (UFPEL)
Pierre Correa Martin (UNIPAMPA)
Raphael Pires Ferreira (UFMG)
Roberto Araújo (UFRR)
Roberto Rosa dos Santos (UFCSPA)
Shirley da Silva Jacindo de Oliveira Cruz (UFPE) 
Teresa Maria de Medeiros Maciel (UFPE) 
Thasiana Maria Kukolj da Luz (UTFPR)
Thiago Meirelles Ventura (UFMT)
Ulisses Cavalca (CEFET-MG)
Vitor Souza Castro (UNIFESSPA)
Volnei Darino Pol (UFFS)
Willdson Gonçalves de Almeida (UFMG)Este periódico foi especialmente editado a partir de conteúdos desenvolvidos para o Workshop de Tecnologia da 
Informação e Comunicação das Instituições Federais de Ensino Superior (WTICIFES) apresentados durante a 13ª Edição do Workshop de Tecnologia da Informação e Comunicação das Instituições Federais de Ensino Superior do Brasil, realizado em Cuiabá, entre 04 e 06 de Junho de 2019, promovido pelo Colégio de Gestores de Tecnologia da Informação e Comunicação das IFES (CGTIC) e organizado pela Universidade Federal de Mato Grosso (UFMT).
Cuiabá - MT
2019Produção Gráfica:
Carlos Henrique A. Gontijo
TODOS OS DIREITOS RESERVADOS
Permitida a reprodução total ou parcial desde que citada a fonte (XIII Workshop de Tecnologia da Informação e 
Comunicação das Instituições Federais de Ensino Superior, Cuiabá-MT , Brasil).Na ausência da indicação da fonte ou expressa autorização da Instituição, é proibida a sua reprodução total ou parcial, por qualquer meio ou processo, especialmente por sistemas gráficos, microfílmicos, fotográficos, reprográficos, fonográficos ou videográficos. Vedada a memorização e/ou recuperação total ou parcial, bem como a inclusão de quaisquer partes desta obra em qualquer sistema de processamento de dados. Essas proibições aplicam-se também às características da obra e à sua editoração. A violação dos direitos autorais é punível como crime (art. 184 e §§, do Código Penal, cf. Lei no. 6.895, de 17-12-1980) com pena de prisão e multa, conjuntamente com busca e apreensão e indenizações diversas (arts. 122, 123, 124 e 126, da Lei no. 5.988 de 14-12-1973, Lei dos Direitos Autorais).Dados Internacionais de Catalogação na Publicação (CIP) 
Agência Brasileira do ISBN - B ibliotecária P riscila Pena Machado CRB-7/6971 
W926  Workshop de Tecnologia da Informação e  Comunicação 
das Instituições Federais de Ensino Superior (13. : 2019 : 
Cuiabá, MT)  
WTICIFES 2019 [recurso eletrônico] /  orgs. Ligia Maria C. 
Sousa Cordeiro ... [et. al.] . —— Cuiabá : ANDIFES, 2019. 
Dados eletrônicos (pdf).  
Evento realizado em Cuiabá, entre 04 a 06 de Junho de 2019,  
promovido pelo Colégio de Gestores de Tecnologia da Informação e 
Comunicação das IFES (CGTIC) e organizado pela Universidade 
Federal de Mato Grosso (UFMT).  
ISBN 978- 85-67619-01-9 
1.Comunicação e tecnologia. 2. Tecnologia da informação .
3.Comun
icação - Inovações tecnológicas. 4. Universidades. I.
Cordeiro , Ligia Maria C. Sousa. II. Rabêlo
, Ricardo de Andrade
L.III. Nunes
, Eunice P. dos Santos. IV. Cardoso , Anne Cristine
Betoni. V. Título.
CDD 303.4833
 Artigos da 13ª Edição do WTICIFES
A Aplicação de Tecnologias Ágeis no Desenvolvimento de Sistemas de Apoio Administrativo  ............................. 10
Daniel A. Moura, Jose O. da Silv
a, Reginaldo S. dos Anjos e Lígia M. C. Sousa
A Importância da Informatização e da Acessibilidade para o Processo de Avaliação de Desempenho
na UFMT  ....................................................................................................................................................................... 16
João V. B. Viana, Delvan L. de Souza, Sandra A. R. dos Santos, Raphael P. Ferreira e Eunice P . dos S. Nunes
Analisa UFG - Plataforma de Análise de Dados da UFG  ............................................................................................ 22
Vict
or G. Bento, Ricardo H. D. Borges,  Rogério R. Carvalho e Dannyel C. Fonseca
Análise de dados da telefonia fixa e móvel com Pentaho  ......................................................................................... 28
L
uiz F. S. Arcenio
Aplicação de Testes Automáticos com Selenium WebDriver: Um Relato de Experiência no SIGAA  ........................ 35
Alan B. de Pont
es, Anne C. O. Rocha e Raphael F. de A. Patrício
Aplicativo Siga UFMG: Relato de Experiência no Centro de Computação da UFMG  ................................................ 41
Patrícia N. Silv
a e Rainer Couto
Avaliação do Uso da Internet sem fio pelos usuários da UFAM a partir do Reconhecimento de Entidadesno Twitter..................................................................................................................................................................... 47
Rodrigo A. Costa
Case de implantação modularizada do GLPI com foco na satisfação do usuário final
 ........................................... 53
Elton P
. Rosa
Caso de migração de telefonia: a partir de uma plataforma proprietária para uma solução VoIP com
software  livre  ............................................................................................................................................................... 59
Geov
ano L. Quatrin, Volnei D. Pol e Diego S. Junges
Catálogo de Serviços: facilitando o acesso aos serviços de TI dentro das Instituições
de Ensino Superior  ...............................................................................................................................
.......... 65
Marcelo A. Santana, Italo C. L. Silva, Rômulo N. de Oliveira, Marcos J. F. Neto e João B. G. Silva
Da Virtual Machine ao Container as a Service: Tornando os Sistemas Integrados de Gestão compatíveis com 
os novos paradigmas de computação em nuvem.  ......................................................................................... 71
Je
ysibel de S. Dantas e Raphael F. de A. Patrício
Democratização do Acesso à Informação na UFS: APISistemas e Desenvolvimento Móvel Multiplataforma com Flutter
 ...............................................................................................................................
...................... 76
Alan Passos e Leonardo Bezerra
Do Papel ao Digital Implantação de Sistema de Protocolo Digital  ................................................................ 82
Marlos Ribeir
o, Renato Mendes e Walter Santos
DWD12 - Um método para criação de senhas seguras e memorizáveis  ......................................................... 87
Cárlisson B. T
. Galdino, Rômulo N. de Oliveira e Raiela Quirino Lima
Entregando recursos para aplicações multiplataforma com a API UFRGS  .................................................... 93
Abel Corr
êa Dias, Felipe Ávila dos Santos e Thiago Stein MottaSumárioFerramenta para facilitar a comunicação do Plano de Desenvolvimento Institucional  ................................ 99
Ana Carla Mac
edo da Silva e Diogo Benassuly
Funcionalidade para logoff da autenticação do Firewall da Palo Alto na UFPI
 
............................................. 105
Filipe S. Viana, Ênio R. Viana e Diego F. M. Oliveira
Gerenciamento de Logs: Implantação do Graylog como ferramenta de centralização de logs de dados na 
Universidade Federal do Amazonas
 
............................................................................................................... 110
Crisley P . Linhares, Gerson B. da Silva1, João G. A. Martinez, Vanderson da S. Rocha, Marckson M. da Silva, João B. L. Carneiro
GESCON: Agilidade e transparência na gestão dos órgãos colegiados da universidade
 
............................... 116
Vanderlin A. P . Júnior, Alexsandro C. Carvalho e Lidiane C. SilvaIftopper - Controle Dinâmico de Largura de Banda
 
....................................................................................... 122
Eduardo M. Monks e Jeronimô F. N. da RosaImplantação automatizada de sítios institucionais com uso de Docker, Git e Jenkins
 ................................. 128
João G. A. Martinez, Gér
son B. da Silva e Diogo Soares
Implantação da rede sem fio na UFCA
 
........................................................................................................... 133
Taciano P . de A Alcântara, Marcos I. F. M. da Silva e Herbert N. OnofreImplantação do Serviço Eduroam na UFAM
 
................................................................................................... 140
Marckson M. da Silva, Vanderson da S. Rocha, Crisley P . Linhares, Gerson B. da Silva, João B. L. Carneiro e João G. A. 
Martinez
Integração das Bases de Login e Senha dos Sistemas da Universidade de Brasília - UnB
 
............................. 145
Renato C. Ribeiro e Everton de V. AgilarIntegração entre LDAP e FreePBX para controle de autorização de chamadas
 ............................................. 151
Eliéz
er de Siqueira e Weber S. R. Takaki
Melhoria Organizacional com Base no Grau de Felicidade dos Colaboradores
 
............................................. 157
Teresa M. M. Maciel e Suzanna SandesMétodo Push de atribuição de tarefas utilizado no desenvolvimento de software por um time 
autogerenciável.............................................................................................................................................. 164
Danniel Rocha, Euclydes Melo, Luiz Fernando, Marcos Raniere, Matheus Souza e Taison Almeida
Moodle: Arquitetura redundante e escalável para alta demanda de acesso
 
................................................. 170
André V. F. Sousa, Edmilson A. do Nascimento, Gustavo P . G. dos Santos e Michel P . AndradeO uso de containers e máquinas virtuais para a otimização de custos e serviços no Governo Federal. Uma 
análise comparativa
 
....................................................................................................................................... 176
Felipe C. Costa Alves, Jean Caminha, Renan Susuki, Allan Gonçalves, Hernane Junior, Tierry Lincoln e Roberto Benedito
O Uso de Micro Serviços no desenvolvimento de sistemas nas Universidades e seus benefícios: um estudo de 
caso.
 
............................................................................................................................................................... 182
Tales M. Machado, Frederico A. de C. A. Gonçalves, Abelard R. Fernandes e Tiago R. Chaves
O uso prático do traffic control (tc) para evitar o colapso de links de Internet em regiões próximas a áreas 
residenciais
 
.................................................................................................................................................... 188
Rômulo N. de Oliveira,, Deive F. V. Gomes, Carlos R. A. da Silva e Icaro dos S. Silva
Padronização de tipos de documentos: um passo na direção da desburocratização
 ................................... 194
Cléber M. T
avares, Diogo G. Pereira, Eliara M. Tavares, Gustavo F. Afonso, José R. P . Ribeiro e Michel L. Alves
Planejamento de Projetos Utilizando Uma Ferramenta Baseada no Modelo PMC (Project Model Canvas)
 
..200
Daniel Biasoli e Ocimar Luiz ZolinPoC RNP/UFG: Implantação do G Suite por meio da adesão ao serviço da Plataforma Nasnuvens/RNP  ...... 206
Igor R. Vieira, Jean T
. Lima e Ricardo H. D. Borges
Portal de Dados Abertos UFG  ......................................................................................................................... 212
Bruno N. Machado
, Jhonny L.Cabral, Juarez E. Lima e Ricardo H. D. Borges
Portal de Integração UFG - APIs de serviços para integração de dados e sistemas  ....................................... 218
Andr
ey E. da Silva, Bruno N. Machado,  Jhonny L. Cabral, Juarez E. Lima, Lauro R. Gomides e Ricardo H. D. Borges
Processo de gerenciamento de riscos: um relato sobre a experiência da STI/UFF  ....................................... 224
Núbia dos S. R. S. dos Sant
os e Henrique O. U. P . de Souza
Relato de Experiência: implantação de um sistema de documentação e gestão das redes de comunicação da 
Universidade Federal de Mato Grosso  ........................................................................................................... 230
Willdson G. de Almeida, Jeison G. dos Sant
os e Jonata B. M. dos Santos
SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação de Base de Dados  ..... 236
Guilherme Ger
onimo e Gustavo Tonini
SINAPSE: Sistema Integrado de Acompanhamento de Projetos e Serviços ................................................... 245
Danniel Rocha, Euclydes Melo, Luiz Fernando, Marcos Raniere, Matheus Campanhã e Taison Almeida
SisPAC – Sistema para Levantamento e Consolidação das Necessidades do PAC  ........................................ 251
Tar
cila G. da Silva, Taiana B. Pereira, Julliany S. Brandão, Daniel F. Oliveira e Enoch C. P . L. da Silva
Sistema Eletrônico de Informações (Sei!): um estudo sobre a melhoria dos serviços públicos com apoio da 
inovação em processos  ...............................................................................................................................
...257
Ana M. M. de Paiva, Jeison G. dos Santos, Elisandra M. Zambra, Paulo A. R. de Souza e Renato Neder
Transformação de Projetos de Sistemas de Informação da UTFPR Através de Frameworks Modernos  ........ 263
Diogo A. B. P
ereira, Marcelo A. Faveri, Rui P . Leite e William H. dos Santos
UFPB-Sentinela: Um Sistema de Segurança para a Cidade Universitária  ...................................................... 269
Ir
on A. de A. Júnior, Sandro L. I. Araujo, José Augusto L. B. de C. Filho e Arthur S. A. de Melo
Uma Análise das faturas de telefonia fixa da UFMT utilizando ferramentas gratuitas de ETL e Business 
Intelligence  ...............................................................................................................................
...................... 275
Jonata B. M. dos Santos, Jean Caminha, Ana M. M. Paiva e Willdson G. de Almeida
Uma solução de automatização no processo de pagamento de bolsas de auxílio  ........................................ 281
André T
. P . Rodrigues e Raphael P . Ferreira
Universidade Digital: Gerenciamento de Ordem de Serviço  .......................................................................... 287
Mar
cos J. F. Neto, Diogo C. Silva, Ítalo C. L. Silva e Rômulo N. Oliveira
Universidade Digital: preservando e disponibilizando a produção científica através do Repositório 
Institucional  ...............................................................................................................................
.................... 293
Italo Silva, Diogo Cabral, Marcos Neto e Rômulo Nunes
Utilização da Central de Ajuda para a Tecnologia da Informação: Estratégias para Auxílio ao Usuário  ........ 299
Paulo Freire Sobrinho
Utilização de Serviço de Registro, Autent icação e Pre servação de Documentos para  a Emissão de Dip lomas 
Digitais  ...............................................................................................................................
............................ 305
Marcelo Soares, Raphael Patrício, Georgenes Lima e Jeysibel Dantas
Utilizando a Gestão de Conhecimento para facilitar o desenvolvimento em projetos de software  .............. 311
Rodrigo A. Cost
aArtigos da 4ª Edição do Encontro dos
Escritórios de Processos (EEP) - 2019
Análise do Processo de Interações Acadêmicas Através do Mapeamento de Fluxo de Valor na UFRGS  ........ 318
Marlon Soliman, Joao F
. F. Vieira, Nicolas Dentzuk, Erica K. de Oliveira, Priscilla F. dos R. Pontes e Éverson J. Santos
Aplicação da Gestão de Processos em uma Instituição Federal de Ensino Superior: o caso da Divisão de Orçamento da UFERSA
 
................................................................................................................................... 324
Lívia R. Barreto, Geisa M. R. de Vasconcelos e Amanda Braga Marques
Aplicação de BPM na gestão do TED estabelecido entre o INCRA e a UFMT
 
.................................................. 330
Maurício F. L. Pereira, Olivan Rabelo, Nilton H. Takagi, Anne C. B. Cardoso, Josiel M. FigueiredoExecução de Melhoria Contínua Baseado em BPMN e Indicadores na Gestão de Serviços de TIC: Estudo de 
caso na Diretoria de Sistemas do NTI/UFPE
 
................................................................................................... 336
Renato V. Mendes e Suzanna S. Dantas
Grau de Maturidade em Processos: um estudo da evolução da Gestão por Processos na UF JF
 .................... 342
F
ábio S. de Figueiredo, Leonardo Ciuffo, Alcimar Honório e Wagner Ramalho
Implementação de novo fluxo de processo: case aula de campo da UFMT
 
................................................... 348
Anne C. B. Cardoso, Cleiton D. da Silva, Thais F. B. da Silva, Greice de S. Arruda, Thiago M. Ventura e Leandro Costa 
Garcia
Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal
de Santa Maria - UFSM 
 
................................................................................................................................... 352
Evandro G. Flores, Frank L. Casado, Daniele M. Rizzetti, Jonas C. Macedo, Taiani B. Kienetz, Rafael F. Neves
Melhoria do Processo de Pagamento de Fornecedores do Restaurante Universitário da UFRGS
 ................. 360
João F
. Vieira, Priscilla F. Pontes, Marlon Soliman, Éverson Santos, Erica K. de Oliveira
Modelo de Maturidade em Mapeamento de Processos (M3P): Proposta e Aplicação na UFCSPA
 
................. 366
Juliana Silva Herbert, Andressa B. de Oliveira, Marilia R. Silveira, Rodrigo de F. GiglioMonitoramento de indicadores de processos com uso de Business Intelligence (BI)
 
................................... 372
Beatriz S. Seidel, Vanessa G. Kinoshita, João C. S. O. Matos, Reinilton S. Juvenal, Naícia K. F. S. B. T . Caten, Leriane S. 
Cardozo
O mapeamento de processos como elemento facilitador no levantamento e elicitação de requisitos do 
Sistema de Gestão da Universidade Federal de Juiz de Fora 
 
........................................................................ 378
Leonardo Ciuffo, Fábio S. de Figueiredo e Rafael G. V. Papa
Processo de Criação de um Novo Regulamento de Graduação Essencial para a Implantação do SIGAA
 
...... 384
Anne C. O. Rocha, Raphael F. de A. Patrício, José A. L. B. de C. Filho, Fabiana F. do Nascimento e Ayrton N. de S. Silvapágina
9
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIORPrefácio
Este volume consiste dos artigos apresentados no XIII Workshop de Tecnologia da 
Informação e Comunicação das Instituições Federais de Ensino Superio do Brasil 
(WTICIFES), realizado na cidade de Cuiabá, MT , Brasil, de 04 a 06 de junho de 2019.
O WTICIFES é um evento promovido pelo Colégio de Gestores de Tecnologia da 
Informação e Comunicação (CGTIC) das Instituições Federais de Ensino Superior 
(IFEs) para, a partir da similaridade dos problemas e desafios de gestão, incentivar a 
troca de experiências, soluções e boas práticas, com objetivo de efetivar a TIC como 
instrumento de inovação e transformação digital nas Universidades Federais.
Nesta edição, o WTICIFES traz como tema “As IFES e o desafio da integração entre 
Sistemas, Processos e Órgãos Públicos” . É evidente que diante de um mundo 
tecnológico e conectado, a sociedade espera cada vez mais que os órgãos públicos 
sejam eficientes, eficazes e transparentes na prestação de serviços. Nesse cenário, 
torna-se essencial evoluir a Gestão de Tecnologia da Informação e Comunicação, a 
fim de manter a qualidade das IFES. A quantidade exorbitante de dados criados e 
mantidos pelos órgãos públicos torna o atendimento à sociedade, de forma articulada, 
um grande desafio. Logo, a integração entre os diversos setores da administração 
pública surge como um importante aspecto a ser desenvolvido, para que assim 
as IFES consigam cumprir o seu papel social. Essa integração abrange diferentes 
níveis, uma vez que não apenas sistemas e dados podem ser compartilhados, mas 
também processos e boas práticas de gestão. É necessário ainda que as Tecnologias 
da Informação e Comunicação estejam alinhadas com as estratégias da gestão e 
integradas aos Processos para que consigam refletir a capacidade de operação dessas 
instituições.
Integrado ao XIII WTICIFES, aconteceu o IV  Encontro dos Escritórios de Processos 
das IFES, momento para compartilhar conhecimentos e práticas referentes a gestão 
por processos, onde foram abordados temas como: análise, desenho, execução, 
monitoramento e controle, visando alcançar resultados relevantes e alinhados aos 
objetivos estratégicos das IFES.
Nesse contexto, o desafio de integrar sistemas, processos e órgãos públicos foi o foco 
do XIII WTICIFES e do IV Encontro dos Escritórios de Processos das IFES.
Nós agradecemos imensamente aos autores, por compartilharem as experiências, 
soluções e boas práticas desenvolvidas nas Instituições Federais de Ensino Superior, 
especialmente, aqueles que compareceram ao evento para apresentar seus trabalhos 
e enriquecer as discussões nas sessões técnicas do WTICIFES.
Comitê Científico WTICIFES 2019
Lígia Maria Carvalho Sousa Cordeiro
Ricardo de Andrade L. Rabelopágina
10
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 10-15, junho de 2019
Capítulo I - A Aplicação de Tecnologias Ágeis no Desenvolvimento de Sistemas de Apoio AdministrativoA Aplicação de Tecnologias Ágeis no 
Desenvolvimento de Sistemas de Apoio 
Administrativo 
 Daniel A. Moura¹, Jose O. da Silva¹, Reginaldo S. dos Anjos¹, Lígia M. C. Sousa¹
¹Diretoria de Tecnologia da Informação - Universidade da Integração Internacional da Lusofonia 
Afro-Brasileira (Unilab) - 62.790-000 - Redenção – Ceará
{danielmoura,joseolinda,reginaldo.anjos,ligia}@unilab.edu.br
Resumo
Este artigo tem como objetivo expor uma experiência da aplicação de uma tecnologia ágil de desenvolvimento de 
sistemas empregada na Universidade da Integração Internacional da Lusofonia Afro-Brasileira, visando a solução 
de um problema recorrente causado pela falta de automação de um processo tido como essencial na manutenção de 
bolsas e auxílios de assistência estudantil. Dentre os resultados obtidos é possível verificar a eficiência do método 
aplicado, bem como a rapidez na entrega da solução.
1. Introdução
No contexto da Universidade da Integração Internacional da Lusofonia Afro-
Brasileira (UNILAB) são disponibilizados para comunidade acadêmica dois grupos 
distintos de sistemas que possibilitam a execução de forma automatizada das 
atividades rotineiras; o primeiro e principal bloco é composto por um sistema ERP 
(do inglês “Enterprise Resource Planning”) que gerencia todas as transações da 
instituição passando desde as atividades acadêmicas até operações puramente 
administrativas. Pode-se dizer que o ERP é um sistema integrado, que possibilita 
um fluxo de informações único, contínuo e consistente por toda a empresa, sob 
uma única base de dados. É um instrumento para a melhoria de processos de 
negócios, como a produção, compras ou distribuição, com informações on-line e 
em tempo real. Em suma, o sistema permite visualizar por completo as transações 
efetuadas pela empresa, desenhando um amplo cenário de seus negócios (CHOPRA 
e MEINDL, 2003). Chamado na UNILAB de Sistemas de Apoio Administrativo, o 
segundo bloco é caracterizado por possuir sistemas cujo objetivo é contribuir com 
tarefas secundárias ainda não disponíveis ou cujo escopo não foi contemplado 
pelo bloco principal de sistemas.
O gerenciamento de bolsas e auxílios, se enquadra no escopo do sistema 
ERP principal, o qual possui um sub-módulo responsável pelo manuseio das 
frequências e pelo pagamento online de bolsistas. Contudo, tal funcionalidade não 
estava em uso pelo cliente, tal setor utilizava-se de um controle manual por meio 
de planilhas que computavam os dados de bolsistas bem como os valores pagos. 
Tais planilhas de controle eram repassadas mensalmente para o setor financeiro 
e o mesmo digitava, um a um, esses dados via terminal (prompt de comando) em página
11
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 10-15, junho de 2019
Capítulo I - A Aplicação de Tecnologias Ágeis no Desenvolvimento de Sistemas de Apoio Administrativoum sistema estruturante responsável pelo pagamento de credores, ocasionando 
um trabalho extremamente repetitivo, sujeito a falhas e lento.
A Pró-reitoria de Administração solicitou então à Divisão de Sistemas a 
disponibilização de um software que pudesse construir, de forma automatizada, as folhas de pagamento de bolsas e auxílios da instituição. Tal folha deveria obedecer ao formato padrão imposto pelo SIAFI - Sistema Integrado de Administração Financeira do Governo Federal
1  para que a integração entre os sistemas fosse 
possível. Além disso, o prazo dado para a implementação de tal solução foi relativamente curto, cerca de 10 dias, justamente o prazo entre o pagamento dos bolsistas do mês anterior e o período de envio da folha para o mês subsequente.
Nesse artigo, será mostrado o processo de desenvolvimento de um sistema 
de apoio administrativo para automação da geração da folha de pagamento das bolsas e auxílios da UNILAB. Ao longo deste, listrar-se-á as tecnologias empregas e a motivação da aplicação destas, bem como os resultados obtidos.  
2. Métodos e ferramentas
Com base na situação problema apresentada, iniciamos um estudo 
para buscar soluções que pudessem no espaço de tempo proposto resolver a problemática supracitada, analisou-se nesta fase a aplicação da solução já disponível em nosso ERP , contudo o módulo de bolsas presente no SIPAC (Sistema Integrado de Patrimônio, Administração e Contratos) se mostrou uma alternativa inviável, pois toda a gestão de bolsas até aquela data era executada de forma manual, seria necessário portanto que a Pró Reitoria de Assistência Estudantil cadastrasse todas as bolsas vigentes em nossa instituição no prazo máximo de 10 dias, tal atividade traria alto risco em virtude do baixo tempo proposto e da alta propensão à falha humana.
Descartada a primeira opção proposta buscou-se, portanto, selecionar 
ferramentas que viabilizassem o desenvolvimento de um módulo de processamento de dados que pudesse ser disponibilizado para o usuário final de forma que, ele mesmo, acionasse a extração das informações a partir das planilhas. Dessa forma, o arquivo gerado seria importado pelo sistema estruturante cliente.
O processo de análise começou com uma consulta de mercado acerca das 
tecnologias web que possibilitassem um rápido desenvolvimento, que fossem escalares e que oferecessem uma configuração simples, porém, eficiente. Neste estudo prévio chegou-se à conclusão que as seguintes tecnologias poderiam ser adotadas para implementação deste miniprojeto: Python
2  e seu framework  
(arcabouço) web Django3; Ruby4  e seu framework Ruby on Rails5  e PHP6 .
1 In: http://www.stn.fazenda.gov.br/SIAFI2 In: https://www.python.org/3 In: https://www.djangoproject.com/4 In: https://www.ruby-lang.org/pt/5 In: https://rubyonrails.org6 In: http://php.net/página
12
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 10-15, junho de 2019
Capítulo I - A Aplicação de Tecnologias Ágeis no Desenvolvimento de Sistemas de Apoio AdministrativoCom as possíveis tecnologias a serem empregadas definidas, iniciou-se a 
análise do processo de configuração da tecnologia e de suas bibliotecas auxiliares, 
a fim de mensurar qual delas apresentaria menor dificuldade de instalação e 
necessitaria de menos recursos para ativação. Neste estudo, foi considerado alguns 
fatores importantes tais como: a compatibilidade com o sistema operacional 
adotado na instituição em servidores, curva de aprendizagem da tecnologia, 
familiaridade prévia do desenvolvedor encarregado e recursos empregados para 
ativação da solução.
O desfecho de tal análise resultou na indicação do emprego das tecnologias 
Python e Django como arcabouço para implementação da solução demandada. 
Dentre os indicativos que levaram a essa escolha, temos:
• O S.O.7 adotado apresentava suporte nativo à linguagem;
• A curva de aprendizagem era sensivelmente menor do que a das linguagens 
concorrentes;
• Apresentava bibliotecas nativas para tratar as planilhas oferecidas como 
input (entrada de dados);
• O desenvolvedor encarregado estava familiarizado com a sintaxe da 
linguagem;
• O framework Django era simples e consumia poucos recursos computacionais.
Nas subseções seguintes é apresentada uma breve descrição das tecnologias 
escolhidas.
2.1 Python
Criado em 1991 no Instituto Nacional de Pesquisa para Matemática e Ciência 
da Computação da Holanda (CWI, do holandês Centrum Wiskunde & Informatica), o 
Python foi pensado para ser uma linguagem de alta legibilidade, o que possibilita 
uma codificação mais fluida e maior facilidade na manutenção e transmissão 
do conhecimento. Para Wazlawick (2017, p. 1997), a linguagem Python busca 
minimizar a quantidade de decisões que um programador precisa fazer quando 
desenvolve um programa.
Atualmente é mantido pela Python Software Foundation e adota o modelo 
de desenvolvimento comunitário8. A missão da fundação é “promover, proteger, e 
avançar a linguagem de programação Python” (PYTHON SOFTWARE FOUNDATION, 
2019).
2.2 Pip
Pip é uma ferramenta de linha de comando encarregada de fazer o manuseio 
de pacotes e instalação de bibliotecas de software escritos em Python9 . Tal software  
foi utilizado no processo de configuração de ambiente de desenvolvimento e 
agilizou a montagem deste.
7 Sistema Operacional.
8 Desenvolvimento comunitário é uma adaptação para o termo em inglês Community Source Development 
que descreve um tipo de projeto de desenvolvimento de código aberto mantido por uma comunidade de 
desenvolvedores (independentes ou não).
9 In: https://pypi.org/project/pip/página
13
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 10-15, junho de 2019
Capítulo I - A Aplicação de Tecnologias Ágeis no Desenvolvimento de Sistemas de Apoio Administrativo2.3 VirtualEnvWrapper 
Ferramenta de linha de comando de simples sintaxe utilizada para isolar 
ambientes virtuais computacionais. VirtualEnvWrapper10  possibilita a instalação 
de múltiplas versões de bibliotecas de softwares sem que haja conflito entre elas.
2.4 Django
Framework para construção de sistemas web, sendo uma possibilidade para 
o desenvolvimento de sistemas complexos que utilizam a linguagem Python. Adota 
a filosofia “Convention over configuration”11, que possibilita uma montagem de 
sistemas com menos restrições e consequentemente mais rápida. Adota o padrão 
MVC (do inglês Model – View – Controller) para construção de aplicações web e 
possibilita, em sua estrutura, um projeto de sistema que possua múltiplas micro 
aplicações modularizando o processo de desenvolvimento.
3. A solução proposta
A aplicação desenvolvida adotou as configurações convencionadas no 
Django, as quais agrupam os sistemas em três tipos específicos de componentes: 
model, template e view.
• Model - representa o modelo de entidades presentes no sistema; nesta 
camada definimos as entidades, seus atributos e comportamentos 
específicos.
• Template - componentes do sistema que representam as telas que farão a 
interface com usuários.
• View - é a camada que possibilita o fornecimento do serviço disponível em 
nossa aplicação. Nesta basicamente encontramos métodos Python, os quais 
receberão uma requisição e encaminharão uma resposta ao cliente.
Segundo recomendação do próprio Django (framework utilizado), adotou-
se como padrão a não construção da funcionalidade propriamente dita a partir 
do controle da aplicação (View component). Ficando assim a cargo deste apenas 
receber a requisição, extrair os parâmetros passados para o processador e 
converter o resultado do processamento para download do cliente. Toda a lógica 
do processamento das planilhas gerenciais de auxílios foi concentrada em um 
único ponto, propiciando, assim, alguns ganhos:
• Diminuição da repetição de código em vários pontos da aplicação;
• Aumento da testabilidade do código;
• Ganho na legibilidade. 
10 In: https://virtualenvwrapper.readthedocs.io/en/latest/
11 “Sugere assumir valores-padrão baseados em convenções” (CRUZ, FIGUEIREDO e ÁVILA, 2011).página
14
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 10-15, junho de 2019
Capítulo I - A Aplicação de Tecnologias Ágeis no Desenvolvimento de Sistemas de Apoio Administrativo
Figura 1. Captura de tela mostrando a interface da aplicação.
Além de processar as planilhas, o script cria um espelho do resultado 
do processamento para fins de conferência por parte do cliente, que ficam 
armazenados no servidor.
Por fim, além dos componentes anteriormente citados, desenvolveu-se 
uma interface bastante simples que permite o upload das planilhas objeto de 
processamento e resultam no download de um arquivo processado.
4. Resultados Obtidos
O processo de desenvolvimento ocorreu, em sua totalidade, em apenas uma 
única semana. Antes do prazo convencionado, a aplicação foi disponibilizada na 
intranet da instituição. Convencionou-se que esta ferramenta ficaria disponível 
por períodos específicos de tempo e o cliente, por sua vez, responsabilizou-se por 
informar o calendário do pagamento de bolsistas.
Atualmente a solução está em fase de testes de homologação de requisitos, 
nas emissões de folha de pagamento dos últimos seis meses, constatou-se que 
a ferramenta é bastante eficiente, antes, o setor responsável pelo envio das 
informações necessárias ao pagamento de discentes alocavam toda a sua mão de 
obra (quatro servidores e um bolsista) por um período de cerca de oito dias úteis, 
tais servidores analisariam as planilhas de acompanhamento, validariam eventuais 
falhas não detectadas na fase de confecção do documento e enviariam esses dados 
ao sistema estruturante correspondente. Após a disponibilização da solução, um 
único servidor fica responsável por fazer a geração da folha a partir destas mesmas 
planilhas de acompanhamento e realizar a exportação dos dados para o SIAFI, tal página
15
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 10-15, junho de 2019
Capítulo I - A Aplicação de Tecnologias Ágeis no Desenvolvimento de Sistemas de Apoio Administrativoprocesso é executado em sua totalidade em 20 minutos. Constatou-se ainda a 
necessidade de construção de novas validações no script de processamento da planilha de acompanhamento de auxílios para tratar as informações de entrada e resolver eventuais problemas de encoding.
5. Conclusão
A aplicação de tecnologias ágeis em instituições com vasta gama de requisitos 
e pequenas equipes de TI possibilita a supressão de necessidades de forma mais célere. Contudo, na construção destes tipos de microssistemas de apoio, é necessário forte engajamento intersetorial a fim de estabelecer convenções que comporão as linhas base do funcionamento do serviço. Foi percebido na elaboração da solução que a instituição beneficiada teria pelo menos quatro setores impactados pela solução demandada. Devido ao curto prazo disponibilizado, a demanda seria considerada totalmente inviável caso a ferramenta adotada não propiciasse as facilidades supracitadas, tendo em vista os vários pontos de enfoque que deveriam ser tratados simultaneamente. 
Assim, evidencia-se a importância dos setores responsáveis pela TI de 
cada instituição buscarem no mercado tecnologias que possam sanar de forma satisfatória problemas pontuais na execução de atividades rotineiras. Deve-se, entretanto, observar na solução adotada os requisitos de escalabilidade, facilidade na transmissão de conhecimento, consumo de recurso, entre outros. Além disso, é essencial a manutenção da proximidade entre TI e clientes, assim o potencial produtivo das tecnologias ágeis possa ser aproveitado em sua totalidade.
Referências
CHOPRA, S.; MEINDL, P . Gerenciamento da Cadeia de Suprimentos - Estratégia, Planejamento e Operação. São Paulo: Prentice Hall, 2003.
CRUZ, M. A. D. A. S.; FIGUEIREDO, ; ÁVILA, M. D. R. Sistemas de Controle de Processos 
em RUBY ON RAILS. Tecnologia & Cultura, Rio de Janeiro, 2011. 68-69.
PYTHON SOFTWARE FOUNDATION. About Python Software Foundation. Python 
Software Foundation, 2019. Disponivel em: <https://www.python.org/psf/>. Acesso 
em: 12 Março 2019.
THE PYTHON PACKAGE INDEX (PYPI). The PyPA recommended tool for installing Python 
packages. The Python Package Index (PyPI), 2019. Disponivel em: <https://pypi.org/
project/pip/>. Acesso em: 12 Março 2019.
WAZLAWICK, R. S. História da Computação. 1. ed. Rio de Janeiro: Elsevier, 2017.16
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 16-21, junho de 2019
Capítulo II - A Importância da Informatização e da Acessibilidade para o Processo de Avaliação 
de Desempenho na UFMT
A Importância da Informatização e da 
Acessibilidade para o Processo de Avaliação de 
Desempenho na UFMT 
 João Vitor Barbosa Viana¹, D elvan Luis de Souza², Sandra Alve s Rodrigues dos 
Santos², Raphael P. Ferreira¹, E unice Pereira dos Santos Nunes ¹
¹Secretaria de Tecnologia d a Info rma ção  – Universidade Federal de Mato Grosso (UFMT)
78060-900 – Cuiabá – MT – Brasil
²Secretaria de Gestão de Pessoas  – Universidade Federal de Mato Grosso (UFMT)
78060-900 – Cuiabá – MT – Brasil
{joaoviana, delvan, sandraalves, raphael,  eunice}@ufmt .br
Resumo
Este artigo tem como objetivo apresentar o processo de criação do novo Sistema de Avaliação de 
Desempenho (SAD) dos servidores técnicos-administrativos da Universidade Federal de Mato Grosso (UFMT). 
A insatisfação dos usuários com o antigo modelo de avaliação motivou a Secretaria de Gestão de Pessoas a 
buscar por mudanças urgentes, a fim de melhorar esse serviço no âmbito da UFMT. A Secretaria de Tecnologia 
da Informação da UFMT desenvolveu o novo SAD com apoio de uma equipe multidisciplinar, aplicando a 
metodologia ágil SCRUM com técnicas de Design Participativo, visando identificar aspectos de acessibilidade e 
usabilidade, e assim melhorar a experiência do usuário e ampliar o alcance do sistema.
Palavras-chave: Avaliação de desempenho. SCRUM. Acessibilidade.
1. Introdução
O processo de av aliação de d esempe nho é um pré- requisito para 
progressão por mérito profissional na carreira d os servidores Técnicos-
Administrativos em Educação (BRASI L 2005). Dessa forma,  no ano de 
2007 a Universidad e Federal de Mato Gro sso (UFMT) adoto u a proposta de 
avaliação de desemp enho e acompanhamento d a carreira dos servidore s 
técnicos-administr ativos em educação (UFMT 2007).
Diante do contex to, a Secretaria de  Tecnologia da Informação (STI) da 
UFMT desenvolveu e implan tou no ano de 2009 a pri meira versão de um 
Sistema de Avaliação de Desem penho (SAD), a fim d e aten der os anseios da 
comunidade universitária, auto matizando o processo d e avaliação. Poré m, 
desde sua criação,  não houve inclusão de  novas funcionali dad es no SAD devido a 
vários fatores como , por exemplo, o SAD  ter sido  implementado na  linguagem 
DELPHI (versão 7 ), uma vez que se trata de u ma linguagem em desuso n o 
âmbito da UFMT. Essa versã o també m não apresent a ma is suporte pela 
fornecedora da tec nologia, conforme rela tório da Embarcad ero (2013).
Embora a UFMT e stivesse com u m sistema de a valiação de de sempenho 
em funcionamento, o trabalho de gerencia mento e acompanham ento das 
avaliações, bem como das prog ressões, dentre  outros procedimentos de 
recursos humanos, 
páginapágina
17
eram realizados por meio de planilhas eletrônicas, exigindo dos servidores da 
Secretaria de Gestão de Pessoas (SGP) um trabalho manual árduo, ocasionando 
lentidão nos processos.
Logo, o antigo modelo de avaliação e seu SAD exigiam melhorias urgentes 
como, por exemplo, os formulários de avaliação, considerados pelos usuários como 
difíceis de compreender, uma vez que a escala avaliativa não media o desempenho 
real do servidor e não era acessível para Pessoas com Deficiência (PcD) visual. O 
antigo SAD também não possuía relatórios gerenciais eficazes, sendo necessário 
realizar controles manuais por meio de planilhas eletrônicas.
 Também se aspirava por um sistema que fosse acessível em diversos 
dispositivos, inclusive smartphones e tablets; que pudesse ser acessível para PcD e 
facilitasse o gerenciamento do processo de progressão por mérito profissional dos 
servidores técnicos-administrativos de forma automatizada.
 Diante do contexto, este trabalho apresenta o processo de desenvolvimento 
do novo Sistema de Avaliação de Desempenho da UFMT , incluindo as dificuldades 
do SAD implantado anteriormente e as soluções encontradas para o novo sistema. 
O novo SAD também atende a resolução que trata sobre a reestruturação do novo 
modelo de avaliação de desempenho dos técnicos-administrativos da UFMT (UFMT 
2018).
2. Métodos
Pressman e Maxim (2011) afirmam que todo projeto de software é motivado 
por alguma necessidade de negócio. Logo, a proposta de desenvolvimento do 
novo SAD vindo da SGP , foi de aprimorar o processo de avaliação de desempenho 
estimulando o feedback do servidor com sua chefia imediata. Ademais, buscava-
se um sistema que tivesse portabilidade entre os diversos dispositivos, inclusive 
smartphones e tablets; fosse acessível a PcD; e facilitasse o gerenciamento 
do processo de progressão por mérito profissional dos servidores técnicos-
administrativos, de forma automatizada.
O Decreto 5.296/2004 (Brasil, 2004) instituiu a obrigatoriedade da 
acessibilidade em portais e sítios eletrônicos da administração pública. Então, o 
Governo Brasileiro elaborou o Modelo de Acessibilidade de Governo Eletrônico 
(e-MAG) com recomendações para implementações e adaptações dos sistemas 
governamentais. Em função dessa necessidade, a acessibilidade foi um pilar 
norteador para o desenvolvimento deste projeto, já atendendo aos padrões, para 
que pessoas com alguma deficiência também consigam interagir e realizar as 
avaliações de desempenho.
O desenvolvimento do novo SAD seguiu a metodologia ágil SCRUM com a 
adoção de técnicas de Design Participativo (DP). Camargo et al. (2014) definem o DP 
como uma metodologia ou prática de desenvolvimento de sistemas que procura a 
participação ativa de todos os envolvidos no sistema, nesse caso o Product Owner 
(PO), incluindo todas as etapas de desenvolvimento.página
18
As técnicas de Contextual Inquiry (entrevistas, depoimentos, descrição de 
cenários) e Mockups (prototipação) foram utilizadas com objetivo de aumentar a 
participação dos usuários no projeto e melhorar a aprendizagem e compreensão 
do novo SAD. Conforme afirma Muller (2002), as técnicas de DP ampliam as 
possibilidades de oferecer um sistema com a interface centrada no usuário.
A linguagem de programação adotada foi C# seguindo o padrão arquitetural 
Model View Controller (MVC) e o sistema de gerenciamento de armazenamento de 
dados é o Microsoft SQL Server. Para integração dos dados com o SIAPE utilizou-se 
Web Services implementados pela equipe de desenvolvimento da STI.
No quesito de acessibilidade foram implementadas as recomendações do 
e-MAG incluindo elementos que auxiliam a interação do usuário com o sistema 
por meio de softwares de leitores de tela. Tags como AL T para identificar imagens, 
controle para o tamanho da fonte, entre outros. O código HTML² foi criado com 
uma sequência lógica para o leitor percorrer as informações corretamente. O 
sistema é compatível aos principais navegadores.
3. Resultados
Com a implementação do novo SAD é possível levantar os principais 
benefícios que ele fornece, tais como: simplificar os processos de autoavaliação 
dos servidores técnicos-administrativos e de avaliação do mesmo pela chefia; 
controle eficaz das progressões por mérito profissional; otimização de processos 
tendo em vista usabilidade, acessibilidade e segurança; entre outros.
O grande desafio na implementação no novo sistema, foi conciliar o layout 
rico em imagens com os recursos de acessibilidade e compatibilidade com os 
vários navegadores. As sessões de DP com Pessoas com Deficiência visual, foi 
fundamental para validar os conceitos de acessibilidade no projeto.
O novo SAD conta com três perfis de acesso, sendo: Administrador; Gestor 
e Servidor. No perfil servidor o sistema disponibiliza as funcionalidades de 
autoavaliação, que inclui preencher o formulário com a sua autoavaliação e avaliar 
o ambiente de trabalho (diagnóstico das condições de trabalho). O perfil também 
permite consultar as avaliações realizadas, conforme mostra a Figura 1. A Figura 2 
mostra como ficou a nova interface de autoavaliação do SAD.página
19
Figura 1. Interface inicial do perfil de servidor.
Figura 2. Interface da autoavaliação do servidor.
O perfil Gestor disponibiliza o formulário para avaliação de cada servidor 
lotado na unidade (Figura 3), e a consulta da avaliação do servidor (autoavaliação 
e diagnóstico das condições de trabalho).página
20
Figura 3. Interface do perfil do gestor avaliando um servidor.
No perfil Administrador, o responsável administra o sistema de avaliação, 
incluindo as seguintes funcionalidades: definição do período de ciclo avaliativo; 
atualização dos formulários de avaliação; definição do peso dos fatores para 
o cálculo do resultado da avaliação. Ademais, neste perfil estão disponíveis a 
geração de relatórios das avaliações realizadas, a fim de serem utilizadas como 
parâmetro para a progressão funcional dos servidores técnicos-administrativos.
Foram realizadas sessões de DP com 26 servidores técnicos-administrativos 
de várias unidades da UFMT juntamente com um servidor com deficiência visual. 
Os resultados possibilitaram desenvolver um SAD centrado no usuário, mais 
acessível e atendendo aspectos de usabilidade. 
4. Conclusão 
Pelo exposto, é possível perceber a importância da implantação no serviço 
público de uma avaliação de desempenho inovadora, acessível em diferentes 
dispositivos, principalmente smartphones e tablets; acessível também para página
21
Pessoas com Deficiência visual e com foco em competências. Por outro lado, foi 
um projeto desafiador para toda equipe.
A avaliação de desempenho precisa ser clara e objetiva no que se espera 
do avaliado, sendo necessário o contato direto do gestor com o servidor, 
principalmente no que diz respeito ao feedback, como instrumento de construção 
de conhecimento da própria relação gestor-avaliado. Este novo modelo buscou 
inovar e promover uma efetiva gestão por resultados, adotando o diálogo como 
instrumento para evitar conflitos, promovendo o espírito coletivo nas relações de 
trabalho. Salienta-se que o SAD foi desenvolvido por uma equipe multidisciplinar 
(servidores da Secretaria de Gestão de Pessoas, desenvolvedores da STI e 
potenciais usuários do sistema), o que levou ao sucesso do projeto.
Referências
Brasil (2005). Lei nº 11.091, de 13 de janeiro de 2005. Lei do PCCTAE, Brasília, DF.
Brasil (2004). Decreto nº 5.296, de 2 de dezembro de 2004. Estabelece normas gerais 
e critérios básicos para a promoção da acessibilidade das pessoas portadoras de 
deficiência ou com mobilidade reduzida, Brasília, DF.
Camargo, L. S. A.; Fazani, A. J. (2014). Explorando o Design Participativo como Prática 
de Desenvolvimento de Sistemas de Informação. Revista de Ciência da Informação e 
Documentação. Ribeirão Preto, São Paulo. 
Embarcadero. (2013) “Supported Versions” Disponível em http://support.
embarcadero.com/article/37740.
Governo Brasileiro. Modelo de Acessibilidade de Governo Eletrônico - versão 3.1. 
Disponível em http://emag.governoeletronico.gov.br/.
Muller, M. J. A. (2002) Participatory design: the third space in HCI. L. Erlbaum Associates 
Inc., Hillsdale.
Pressman, R. S.; Maxim, B. R. (2016) “Engenharia de software – Uma Abordagem 
Profissional” . McGraw-Hill. 8º Edição. 
UFMT (2007). Resolução CONSUNI nº 17, de 25 de outubro de 2007, Cuiabá, MT .
UFMT (2018). Resolução CONSUNI Nº 18, de 26 de setembro de 2018, Cuiabá, MT .página
22
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 22-28, junho de 2019
Capítulo III - Analisa UFG - Plataforma de Análise de Dados da UFGAnalisa UFG - Plataforma de Análise de Dados da 
UFG 
Victor G. Bento², Ricardo H. D. Borges¹,  Rogério R. Carvalho²,  Dannyel C. Fonseca¹
¹Centro de Recursos Computacionais – Universidade Federal de Goiás (UFG)
Goiânia – GO – Brasil
²Secretaria de Planejamento, Avaliação e Informações Institucionais – Universidade Federal de 
Goiás (UFG) – Goiânia – GO, Brasil
{victor_goncalves,ricardoborges,rogerior,dannyelcf}@ufg.br
Resumo
A transparência pública está ligada diretamente com a eficiência na gestão e com a lisura das informações 
disponibilizadas pelos órgãos públicos. Neste sentido, a concepção de um sistema centralizado para tratamento, 
processamento e visualização de dados é fundamental, haja vista o grande volume de dados armazenados nos 
sistemas institucionais. A Plataforma Analisa UFG apresenta-se como uma proposta para atender esse objetivo, 
proporcionando uma abordagem facilitada para visualização de dados e informações por meio de painéis de 
indicadores e relatórios dinâmicos. Os resultados iniciais demonstram tanto o potencial da Plataforma quanto às 
dificuldades enfrentadas na validação dos dados. Percebe-se ainda a existência de uma maturidade institucional 
incipiente no que se refere à cultura de gestão de negócios baseada em dados.
1. Introdução
Um dos grandes desafios da gestão pública se encontra na busca pelo 
alinhamento da estratégia institucional com os anseios da sociedade e as metas 
governamentais. Para tanto, o gestor necessita de informação precisa e no tempo 
correto, para assim, tomar decisões assertivas (Leite, 2010).
A Administração Superior da Universidade Federal de Goiás (UFG) possui 
uma crescente necessidade de acesso, manipulação e visualização de dados 
dos sistemas institucionais, com a finalidade de apoiá-los no planejamento, 
monitoramento e execução das atividades; oferecer respostas aos órgãos de 
auditoria e de controle externo; e prestar contas à comunidade interna e externa 
sobre os serviços oferecidos. Entende-se que a visualização de dados tratados 
favorece o entendimento, os torna intuitivos e mais simples para comunicar 
conceitos e ideias importantes (Traina, 2001).
Os atuais relatórios disponibilizados pelos sistemas institucionais da UFG 
não suprem as necessidades de seus gestores, uma vez que o detalhamento/
agrupamento das informações é insuficiente em relação às demandas e 
necessidades do negócio. Uma saída viável para resolver o problema seria 
implementar ajustes personalizados em cada relatório/sistema para atender essas 
necessidades, porém o alto custo relacionado a esta customização e a variedade 
de solicitações inviabilizam essa abordagem.página
23
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 22-28, junho de 2019
Capítulo III - Analisa UFG - Plataforma de Análise de Dados da UFGA Plataforma Analisa UFG provê aos gestores uma visão alinhada e unificada 
dos dados que antes se encontravam armazenados em base de dados distintas dos 
sistemas institucionais. Após a centralização da base de dados é necessário que 
as informações sejam classificadas de acordo com os fatos que mais demandam 
interesse da sociedade, aumentando assim a transparência e a participação social 
nas avaliações do serviço oferecido (Figueiredo, 2014).
A Plataforma Analisa UFG é um projeto desenvolvido conjuntamente pela 
Secretaria de Tecnologia e Informação (SeTI), por meio do Centro de Recursos 
Computacionais (CERCOMP), e a Secretaria de Planejamento, Avaliação e 
Informações Institucionais (SECPLAN), contando ainda com a colaboração das 
respectivas pró-reitorias. A Plataforma tem por objetivo agregar e tratar dados, 
disponibilizar painéis com indicadores quantitativos e gerenciais, além de 
relatórios dinâmicos para atender as particularidades e necessidades de dados 
das áreas finalísticas da UFG. 
O projeto surgiu da reflexão entre técnicos e analistas da área de TI da UFG 
quanto à necessidade de disponibilização de uma plataforma de apoio à análise 
de dados. Essa reflexão logo se alinhou a antigas necessidades institucionais, 
reiteradas pela atual gestão. Inicialmente, um grupo de trabalho estudou diversas 
abordagens e ferramentas, bem como realizou visitas técnicas até chegar a definição 
de uma plataforma que atendesse os seguintes critérios: gestão e privacidade dos 
dados, autonomia tecnológica e baixo custo de implementação (todas as soluções 
integradas são software livre). 
Este trabalho está estruturado em quatro seções, sendo a seção 1 uma 
introdução com a contextualização e motivação do projeto. Na seção 2 é 
apresentada a metodologia utilizada no desenvolvimento do trabalho. Já a seção 
3 traz a apresentação dos resultados alcançados e dificuldades encontradas. Por 
fim, a conclusão do trabalho é exposta na seção 4.
2. Métodos
O desenvolvimento inicial da Plataforma Analisa UFG priorizou assuntos 
sensíveis à administração, iniciando assim uma cultura da inteligência de negócios 
(do inglês Business Intelligence) junto à Administração Superior da UFG. Em cada 
etapa, os gestores foram envolvidos no processo de levantamento e validação dos 
dados, gerando maior engajamento e aumentando a segurança do resultado final.
 A partir de um levantamento detalhado de demandas priorizadas por cada 
pró-reitoria, foi-se moldando requisitos dos painéis gerenciais, porém não existia 
uma centralização de dados, que foi ajustada com a criação de uma base única de 
dados. Projetadas em modelo dimensional, as informações dispersas em outros 
sistemas da Universidade, inclusive os legados, foram devidamente agregadas e 
centralizadas.página
24
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 22-28, junho de 2019
Capítulo III - Analisa UFG - Plataforma de Análise de Dados da UFG As bases de dados que são alimentadas por sistemas transacionais podem 
apresentar diversos vícios de programação que nem sempre representam fielmente 
uma informação. Para realizar o tratamento de informações foram implementadas 
três camadas de EL T (do inglês Extract, Load and Transform) utilizando a ferramenta 
Pentaho Data Integration, conforme descrito abaixo e apresentado na Figura 1. A 
escolha dessa ferramenta considerou o contexto e os critérios apresentados na 
Seção 1:
• Data Lake: A cópia exata dos dados de produção, desvinculando assim o 
dado de sua origem;
• Datastage: As imprecisões e ruídos oriundos do Data Lake são tratados, 
assim como as informações são melhor detalhadas para aumentar o seu 
valor semântico;
• Data Warehouse: As tabelas do Datastage são transformadas para atender 
ao fluxo do modelo dimensional traçado.
Figura 1. Processo de extração de dados
O Sistema de Gerenciamento de Banco de Dados (SGBD) utilizado no projeto 
Analisa UFG é o PostgreSQL. Ele foi escolhido devido ao fato da Universidade já 
fazer uso do mesmo em seus mais recentes projetos de sistemas, e pela existência 
consolidada de uma infraestrutura de segurança e backup dos dados para este 
SGBD na instituição.
Para a construção do Data Warehouse foi utilizada a modelagem 
multidimensional dos dados na visão estrela (star schema), com o intuito de facilitar 
sua leitura e entendimento por parte dos analistas e usuários não familiarizados 
com estruturas de banco de dados (Hokama, 2004).
Tão importante quanto o tratamento do dado é a sua exibição, uma vez que 
este evidencia os fatos através de uma apresentação gráfica clara e intuitiva. A 
visualização dos painéis e relatórios do Analisa UFG utiliza o Apache Superset, 
um sistema de fácil customização e com interface amigável, além de oferecer um 
modelo de permissões extensível que possibilita a criação de regras sobre quem 
pode acessar recursos (gráficos e relatórios) individuais e/ou o conjunto dos 
dados. A Figura 2 apresenta a proposta metodológica e de arquitetura adotada 
para a solução. página
25
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 22-28, junho de 2019
Capítulo III - Analisa UFG - Plataforma de Análise de Dados da UFG
Figura 2. Fluxo metodológico de desenvolvimento do projeto Analisa UFG
3. Resultados
A concepção da Plataforma Analisa UFG, procurou evidenciar os dados de 
gestão, oferecendo um panorama importante para o tomador de decisão, assim 
como para a sociedade que busca encontrar e acompanhar indicadores de gestão 
da Universidade.
Os painéis de indicadores fornecem uma visão macro de informações 
sobre um determinado assunto. Na Figura 3, observa-se um painel contendo um 
conjunto de informações relevantes sobre os servidores da UFG, este painel fica sob 
responsabilidade da Pró-Reitoria de Gestão de Pessoas, ou seja, esta pró-reitoria 
é encarregada de avaliar se a informação exibida no painel reflete a realidade 
dos fatos e se atende suas necessidades de indicadores. Já com os relatórios 
dinâmicos foi possível disponibilizar informações personalizadas e atualizadas 
diariamente, deixando o usuário gestor com maior autonomia na manipulação de 
relatórios, além de desonerar o órgão de TI da responsabilidade de repetidamente 
confeccioná-los.página
26
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 22-28, junho de 2019
Capítulo III - Analisa UFG - Plataforma de Análise de Dados da UFG
Figura 3. Painel de Indicadores de Servidores
Durante o desenvolvimento e validação dos dados apresentados nos painéis 
de indicadores e relatórios dinâmicos foram identificados uma série de ruídos 
relacionados à inconsistência dos dados (seja pelo cadastro incorreto ou pela 
atualização de informações não inseridas nos sistemas), o que acabou por interferir 
na interpretação correta dos fatos. Para mitigar este comportamento, estabeleceu-
se uma etapa de validação de dados junto às pró-reitorias, bem como decidiu-
se pela inserção de notas explicativas em alguns painéis, de forma a esclarecer o 
contexto de obtenção dos dados.
 Algumas pró-reitorias apresentaram dificuldade em validar seus dados 
devido ao volume de informações disponibilizadas através da Plataforma. Dessa 
forma, foi necessário buscar estratégias para qualificar os responsáveis pelos 
dados, bem como melhorar a abordagem de validação. Ao passo que a absorção 
do conceito de inteligência de negócios está ligada intimamente com o uso das 
informações como subsídio de decisão, é necessário estabelecer a cultura de 
utilização da Plataforma como instrumento fundamental para a realização das 
atividades estratégicas da Universidade.página
27
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 22-28, junho de 2019
Capítulo III - Analisa UFG - Plataforma de Análise de Dados da UFG4. Conclusão
O direcionamento da Administração Superior para uma gestão orientada 
a dados e indicadores fortalece o aprimoramento da Plataforma Analisa UFG, 
visto que aumenta a demanda por subsídios gerenciais. Entende-se ainda que a 
utilização da inteligência de negócios é um processo gradativo de mudança da 
cultura organizacional.
 Por outro lado, a Universidade precisa periodicamente apresentar 
informações para diversos órgãos de controle e gestão pública, além de responder 
pedidos de informação da sociedade, por meio do e-SIC (Serviço de Informação ao 
Cidadão). Neste sentido, a Plataforma contribuiu para aumentar a celeridade nas 
respostas e garantir uma fonte centralizada de informações, evitando respostas 
desencontradas e desatualizadas.
A Plataforma Analisa UFG, somada a outras estratégias, permitirá novas 
descobertas, não somente do potencial estratégico dos dados, mas também 
da necessidade de gestão da informação. Acredita-se que isso resultará em 
uma maturidade institucional para a análise de dados e disponibilização de 
informações, ainda incipiente na realidade universitária. A institucionalização 
de uma plataforma de apoio à  gestão estratégica demonstra diretamente a 
preocupação e compromisso da UFG com a transparência de suas ações e serviços 
prestados à sociedade. 
Os painéis de indicadores desenvolvidos nesse projeto estão públicos e 
disponíveis (analisa.dados.ufg.br). Já os relatórios dinâmicos são de acesso restrito 
às respectivas pró-reitorias. As próximas etapas do projeto incluem a evolução e 
aprimoramento da Plataforma, bem como a implementação de novos indicadores.
Referências
Leite, L. O., & Rezende, D. A. (2010). Modelo de gestão municipal baseado na utilização 
estratégica de recursos da tecnologia da informação para a gestão governamental: 
formatação do modelo e avaliação em um município. Revista de Administração 
Pública, 44(2), 459-493.
Figueiredo, V. S., & Santos, W. J. L. (2014). Transparência e participação social da gestão 
pública: análise crítica das propostas apresentadas na 1ª Conferência Nacional sobre 
Transparência Pública. Revista contabilidade e controladoria, 6(1).
Hokama, D. D. B., Camargo, D., Fujita, F., & Fogliene, J. L. V. (2004). A modelagem de 
dados no ambiente Data Warehouse. São Paulo, 32.
Traina, A. J., Traina Jr, C., Botelho, E., Barioni, M. C. N., & Bueno, R. (2001). Visualização 
de Dados em Sistemas de Bases de Dados Relacionais. In SBBD (pp. 95-109).página
28
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 28-34, junho de 2019
Capítulo IV - Análise de dados da telefonia fixa e móvel com PentahoAnálise de dados da telefonia fixa e móvel com 
Pentaho
Luiz Fernando Stopa Arcenio¹
¹Coordenadoria de Desenvolvimento de Tecnologia da Informação – Universidade Federal da 
Grande Dourados (UFGD)
Caixa Postal 322 – 79.825-070 – Dourados – MS – Brasil
luizarcenio@ufgd.edu.br
Abstract
From the need to improve access to information on telephone bills and consequently on the management of fixed and 
mobile telephony contracts, an environment was modeled with Pentaho. This environment aims to eliminate the use 
of electronic spreadsheets, which continuously present problems due to the large volume of invoice records and the 
monthly accumulation required. Pentaho contributed significantly to the analysis of these invoices, resulting in a 
better visualization of the information and management of the fixed and mobile service.
Keywords: Business Inteligence, Pentaho.
Resumo
Da necessidade de melhorar o acesso às informações das faturas telefônicas e consequentemente na gestão dos 
contratos de telefonia fixa e móvel, foi modelado um ambiente com Pentaho. Este ambiente tem por finalidade, 
eliminar a utilização de planilhas eletrônicas, que continuamente apresentam problemas devido ao grande volume 
de registros das faturas e o acumulo mensal necessário. O Pentaho contribuiu de forma significativa na análise 
destas faturas, resultando em uma melhor visualização das informações e gerenciamento do serviço de telefonia 
fixa e móvel.
Palavras-chave: Business Inteligence, Pentaho.
1. Introdução
A Universidade Federal da Grande Dourados utiliza-se dos serviços de 
telefonia fixa e móvel com o objetivo de melhorar a comunicação dos responsáveis 
pelos serviços entre si e com a comunidade acadêmica. Estes serviços são utilizados 
para comunicação através de ligações internas e externas, envio e recebimento de 
SMS e whatsapp, etc, sendo estes para o envio de informações e avisos. A análise 
da fatura dos contratos de telefonia fixa e móvel era realizada através de planilha 
eletrônica, mas devido ao volume de registros destas faturas, a análise ficava 
comprometida por lentidão na manipulação deste tipo de arquivo e na limitação 
das formas para análise das informações disponíveis.
Neste espaço de limitação a utilização do Pentaho1 possibilitou a agregação 
dos dados, bem como permitir a navegação nos mesmos e desta forma alinhados 
com as ferramentas disponíveis melhorar a análise das faturas telefônicas. Foram 
utilizados os dados do ano de 2018 da telefonia fixa e móvel para demonstrar a 
capacidade da ferramenta de facilitar a análise dos dados, bem como facilitar o 
seu acesso por todos os membros da governança de TI.
1 Segundo Barbieri (2011), é uma solução completa de Business Inteligence (Inteligência Empresarial), 
permitindo a integração de dados, exibição e mineração de dados.página
29
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 28-34, junho de 2019
Capítulo IV - Análise de dados da telefonia fixa e móvel com PentahoEste artigo tem cunho tecnológico e exploratório [Alves et al 2008], abordando 
a modelagem de Extração, Transformação e Carga - ETL e a exibição dos dados. 
As seções deste artigo foram distribuídas da seguinte forma: a seção 2 apresenta 
o desenvolvimento do projeto. A seção 3 apresenta os resultados alcançados. A 
seção 4 apresenta as conclusões finais e trabalhos futuros.
2. Metodologia
As faturas do serviço de telefonia fixa e móvel da Univ. Federal da Grande 
Dourados - UFGD são das operadoras Claro e Algar, e juntas elas geraram 113.211 
registros de movimentações telefônicas e utilização do pacote de dados no ano de 
2018. Para realizar a análise destas faturas, eram utilizadas planilhas eletrônicas, 
onde além das diversas colunas dos registros enviados pela operadora sobre a 
utilização dos serviços, eram necessários a inclusão de novas colunas na planilha. 
Estas novas colunas contêm fórmulas para realizar alguns cálculos como por 
exemplo, o tempo em minutos, diferença entre o valor do minuto no contrato e o 
valor cobrado pela operadora, conversão de tipos, classificação dos serviços, entre 
outros. Os campos existentes e os novos campos adicionados, deixam a planilha 
lenta em sua abertura, assim como na inclusão de novos cálculos, atualizações de 
dados e fórmulas, e no processo de filtragem dos dados para análise.
Em Wilson (2002), discursa sobre um conjunto de aplicações que de forma 
estruturada, produz informações através de dados que foram agrupados, 
armazenados, analisados e disponibilizados a todos os níveis organizacionais, 
essa suite é chamada de Business Inteligence - BI. O Pentaho, segundo Barbieri 
(2011), vem nesta proposta de suite para ferramenta de BI, podendo ser utilizada 
em ambiente totalmente open source, desde o sistema operacional, servidor web , 
banco de dados, acessibilidade em dispositivos móveis, capacidade de alocação 
do banco de dados em memória. 
Na montagem do ambiente para utilização do Pentaho, foram utilizados os 
seguintes recursos tecnológicos:
Banco de Dados: Postgresql2 9.5: banco de dados para armazenar os dados tratados;
Pentaho 7.1: permite acessar, integrar, manipular, visualizar e analisar os dados;
Pentaho Data Integration: para realizar a extração, transformação e carga – ETL, 
através da conexão ao banco de dados de origem, tratamento das informações e 
carga no banco de dados de destino;
Pentaho Schema Workbench: utilizado para gerar os cubos OLAP3;
Jpivot: utilizado para visualização dos dados e geração de gráficos dentro do 
Pentaho.
2 Segundo PostgreSQL (2012), é um sistema gerenciador de banco de dados (SGBD) objeto-relacional de 
código aberto e roda em todos os grandes sistemas operacionais.
3 Segundo Barbieri (2011), On-line Analytical Processing permite a manipulação dos dados de forma 
dimensional, permitindo à rotação de planos ou pivotamento do eixo das dimensões.página
30
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 28-34, junho de 2019
Capítulo IV - Análise de dados da telefonia fixa e móvel com PentahoForam utilizados das faturas apenas os dados especializados para a gestão do 
contrato, estes dados foram processados na etapa de ETL (Extração, Transformação 
e Carga) com Pentaho Data Integration. Na Figura 1 pode-se visualizar os artefatos 
utilizados para definir as fontes de dados, definição dos dados importantes, o 
tratamento e o banco de dados de destino.
Para a visualização dos dados no Pentaho, deve ser gerado o cubo OLAP de 
visualização. Para gerar este modelo foi utilizado o Pentaho Schema Workbench. Esta 
ferramenta permite gerar níveis de hierarquias entre as dimensões e as métricas 
necessárias. Na Figura 2 tem-se o esquema criado com os cubos disponíveis, suas 
dimensões e métricas.
Figura 1. Processo de extração, transformação e carg
Concluídos as etapas de ETL e geração dos cubos, os dados das faturas 
telefônicas já estão disponíveis para os colaboradores. Os dados podem ser 
manipulados através do Jpivot do Pentaho, nesta interface pode ser realizado:
• Agregação de acordo com a métrica estabelecida;
• Geração de gráficos;
• Reordenação das dimensões;
• Realizar drill-down e drill-up  na granularidade dos dados.página
31
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 28-34, junho de 2019
Capítulo IV - Análise de dados da telefonia fixa e móvel com Pentaho
Figura 2. Geração do cubo OLAP
As tarefas de manipulação do cubo são realizadas na janela “Open OLAP 
Navigator” . Podem ser alterados as colunas das métricas, a ordenação das linhas e 
colunas, escolha de filtros e itens a serem exibidos (Figura 3).
Concluído a configuração de exibição (Figura 4), os dados podem ser 
analisados e novamente alterado sua exibição de acordo de sua necessidade 
através dos ícones da barra do Jpivot. As opções disponíveis podem ser de girar 
o cubo, exportar os dados, gerar e configurar gráficos, aumentar e diminuir o 
detalhamento, visualizar os dados utilizados na exibição, entre outras funções.
Figura 3. Manipulação do cubo com OLAP Navigatorpágina
32
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 28-34, junho de 2019
Capítulo IV - Análise de dados da telefonia fixa e móvel com Pentaho
Figura 4. Exibição de dados no cubo OLAP com gráficopágina
33
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 28-34, junho de 2019
Capítulo IV - Análise de dados da telefonia fixa e móvel com Pentaho3. Resultados
Conforme os dados apresentados anteriormente, pode-se verificar a 
facilidade na obtenção de informações das faturas telefônicas de forma intuitiva. 
O ambiente possui dispositivos de segurança que permitem a autenticação dos usuários e acesso aos cubos em que lhes foram dados permissão. Os cubos podem ser gerados e salvos de forma a serem reutilizados em futuras pesquisas, além de permitir a sua exportação para os formatos: HTML, Excel, CSV, PDF e RTF. 
Para a modelagem do ambiente, foram utilizadas diversas ferramentas, e 
estas demandam conhecimento para utilizá-las. Apesar de todo o ambiente ser open source,   tendo desta forma, nenhum custo para sua aquisição, o conhecimento necessário para sua utilização, demandará investimento em capacitação. Nas análises realizadas no ambiente e nas faturas importadas, pode-se considerar como importantes resultados:
•
Identific
ação de pouca utilização de algumas linhas de telefonia móvel,
podendo estas serem melhores utilizadas;
•V
alidar os valores contratados e os valores realmente cobrados pelas
operadoras;•
 F
acilidade na geração dos relatórios e análises, consequentemente,
melhoria na gestão dos contratos;•
Identific
ação dos maiores e menores utilizadores dos serviços de telefonia;
•Dif
erenciar as quantidades de ligações intragrupo e extragrupo;
•F
ácil manipulação dos dados por sua interface web;
•A
tualização do gráfico de acordo com a manipulação dos dados.
4. Conclusão
Com o objetivo de melhorar a análise das faturas de telefonia móvel e fixa, 
o ambiente Pentaho permitiu grande facilidade na manipulação e exibição dos
dados, que anteriormente não era possível. A manipulação dos dados não requergrande conhecimento, mas entendimento entre as orientações entre linhas ecolunas.
Os resultados apresentados na seção anterior demonstram que uma vez 
carregados os dados das faturas, as informações que podem ser abstraídas melhoraram a gestão dos contratos. Com a utilização da ferramenta puderam ser respondidas diversas perguntas como, quanto cada colaborador gasta com telefonia, para onde ligou, quem ligou para ele, consumo pacote de dados, comparativo do consumo mensal, consumo por tipo de ligação, comparação entre tarifa contratada e cobrada, entre outras mais.
Para trabalhos futuros, pode-se destacar a criação de dashboard para 
visualização da evolução da utilização do serviço de telefonia fixa e móvel em página
34
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 28-34, junho de 2019
Capítulo IV - Análise de dados da telefonia fixa e móvel com Pentahoum ambiente visualmente mais resumido e elegante. Bem como a configuração 
dos metadados do cubo para a geração de relatórios automáticos através do Web 
AdHoc Query Report -WAQR, nativo do Pentaho.
Referências
Alves, Rêmulo Maia; Pádua, Clarindo Isaías Pereira da Silva e; Zambalde, André Luiz. 
(2008) “O documento científico em ciência da computação e sistemas de informação” . 
Lavras/MG: DCC/UFLA.
Barbieri, Carlos. BI2 – Business Inteligence: modelagem e qualidade. São Paulo/SP: 
Editora Elsevier, 2011.
Pentaho. Disponível em: https://help.pentaho.com/Documentation/7.1/0D0/
Pentaho_Business_Analytics. Acesso em 27/02/2019.
PDI. Pentaho Data Integration. Disponível em: https://help.pentaho.com/
Documentation/7.1/0D0/Pentaho_Data_Integration. Acesso em 27/02/2019.
PostgreSQL. Disponível em: http://www.postgresql.org.br/pages/sobre-o-postgresql.
html. Acesso em 21/02/2019.
PSW. Pentaho Schema Workbench. Disponível em: https://help.pentaho.com/
Documentation/8.2/Products/Schema_Workbench. Acessopágina
35
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 35-40, junho de 2019
Capítulo V - Aplicação de Testes Automáticos com Selenium WebDriver: Um Relato de Experiência no SIGAAAplicação de Testes Automáticos com Selenium 
WebDriver: Um Relato de Experiência no SIGAA
Alan B. de Pontes¹, Anne C. O. Rocha¹, Raphael F. de A. Patrício¹
¹Superintendência de Tecnologia da Informação (STI) – Universidade Federal da Paraíba (UFPB) 
Campus I - Lot. Cidade Universitária, PB
58051-900 – João Pessoa – PB – Brazil
{alan,caroline,raphael}@sti.ufpb.br 
Resumo
O presente artigo tem como objetivo apresentar um relato sobre a implantação de teste funcional automático de regressão pela equipe de qualidade e testes de software da UFPB. Para isso, foram adotadas algumas tecnologias open-source já existentes. Neste trabalho serão apresentadas experiências referente à utilização das ferramentas de testes em alguns módulos do Sistema Integrado de Gestão Acadêmica (SIGAA) implantado na UFPB, uma visão geral da configuração do ambiente para que os testes pudessem ser executados periodicamente, a motivação para seleção dos casos de teste para o tipo de teste em comento, os defeitos encontrados após a execução habitual destes testes, problemas enfrentados e algumas soluções encontradas.
Palavras-chave: Testes de Software; Teste de Regressão; Testes Automáticos;
1. Introdução
Os sistemas desenvolvidos para as Instituições Federais de Ensino Superior 
(IFES) cada vez mais necessitam de qualidade, pois são sistemas de grande porte 
e que atendem a diferentes áreas. Cada sistema possui um grau diferente de importância, pois dependendo do erro que ocorra na aplicação, ele pode impactar diretamente os seus usuários ou até mesmo a imagem da instituição.
Para melhorar a qualidade desses sistemas testes devem ser realizados, pois 
permitem identificar previamente a existência de erros. Teste de software é uma atividade desempenhada durante o desenvolvimento de um sistema [BSQTB, 2019]. Para cada especificação do sistema é necessário planejar a criação de roteiros de teste, que são compostos por vários casos de teste. Os casos de teste definem um conjunto de passos a serem executados no sistema e um resultado esperado, após a execução do mesmo. Desta forma, esta atividade demanda considerável tempo e esforço das equipes, diante disso iniciativas com intuito de atenuar estas condições tornam-se necessárias. 
Durante a implantação do SIGAA [UFRN, 2006] na UFPB, foram encontrados 
erros recorrentes no sistema, devido à integração entre os módulos. Algumas vezes, esses erros só eram percebidos pelos usuários, já que para identificá-los durante a etapa de desenvolvimento, seria necessário realizar testes de regressão nos módulos implantados. O teste de regressão tem como um dos objetivos garantir que o sistema continuará funcionando mesmo depois que novas versões forem lançadas [BSQTB, 2019]. Para realizar esses tipos de testes de forma manual, página
36
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 35-40, junho de 2019
Capítulo V - Aplicação de Testes Automáticos com Selenium WebDriver: Um Relato de Experiência no SIGAAem sistemas de grande porte, como o SIGAA, demandaria muito tempo, pois seria 
necessário reteste contínuo de vários módulos do sistema.
Assim, o trabalho desenvolvido pela equipe de qualidade de software da 
Superintendência de Tecnologia da Informação da UFPB (STI-UFPB) teve como 
objetivo automatizar o processo de testes funcionais, ao que concerne a realização 
do teste de regressão. 
Neste contexto, este artigo apresenta um relato de experiência relacionado 
à utilização de ferramentas para automação de testes funcionais, durante a fase 
de criação de novas funcionalidades e manutenção de software. Assim, serão 
apresentadas dificuldades referentes à utilização das ferramentas empregadas, 
a motivação para seleção dos casos de teste, os defeitos encontrados após a 
execução destes testes, problemas enfrentados e algumas soluções encontradas.
2. Métodos
Para realização da automatização dos testes funcionais, a equipe optou por 
ferramentas open-source e que permitem soluções próprias. Com isso, a decisão 
foi por utilizar o Selenium Webdriver [Selenium, 2019], que realiza chamadas 
diretas ao navegador, usando o suporte nativo do mesmo para automação, para 
isso também foi utilizado recursos XPath. O gerenciamento e controle de versão é 
feito pelo Hudson [Hudson, 2019], com ele é possível realizar integração contínua 
dos testes automáticos, o que controla o início e o término destes testes.
Outra ferramenta primordial para execução dos testes foi a utilização da 
plataforma JUnit [JUnit, 2019]. Apesar de sua expressa utilização para técnicas de 
teste unitário, o Junit também é responsável pela estruturação dos casos de teste, 
verificação das saídas e pelos testes propriamente ditos. Ela é responsável pela 
comparação dos valores esperados com valores retornados pelo sistema, e ao final 
informa se os testes obtiveram êxito ou falharam.
Conforme Figura 1, pode-se verificar o processo de execução dos testes 
automáticos. Em um primeiro momento, analistas de testes planejam os roteiros 
e criam os respectivos casos de testes. Após esta etapa, os testes são colocados no 
repositório de controle de versão e ficam disponíveis para execução. O momento 
da execução dos testes ficou condicionado ao lançamento de uma versão no 
ambiente de homologação, logo o conjunto de testes é executado neste ambiente. 
Quando algum erro é encontrado, um bug é cadastrado automaticamente para a 
equipe de manutenção de sistema, a qual poderá providenciar a correção, uma 
vez que a etapa de homologação ocorre antes da etapa de envio do código para 
produção. Caso não seja encontrado erro, o mecanismo de automação finaliza os 
testes e não toma ação alguma, mas envia um e-mail informando sobre o sucesso 
da execução.página
37
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 35-40, junho de 2019
Capítulo V - Aplicação de Testes Automáticos com Selenium WebDriver: Um Relato de Experiência no SIGAA
Figura 1. Processo demonstrando o processo adotado para execução de testes
Na Figura 2 é possível perceber o processo de análise dos testes após a 
execução. Quando são encontrados erros, o Hudson cadastra automaticamente 
os bugs em uma ferramenta de gerenciamento de defeitos e envia um e-mail para 
equipe informando que os testes falharam. Diante disso, a equipe de qualidade 
de software deve verificar o bug e a sua causa. Com isso, sendo uma mudança 
de sistema a ação tomada é de rejeitar o bug e posteriormente atualizar o caso 
de teste no Selenium WebDriver, que ficou desatualizado, o que impede que os 
testes seguintes sejam executados. Existindo um erro no sistema, a equipe deve 
reproduzi-lo, manualmente, para se certificar que o erro realmente ocorre, logo 
deve detalhar precisamente o registro do bug, para facilitar a correção daquele 
bug pela equipe de desenvolvimento. Já na situação de um erro de infraestrutura, 
quer seja erro de rede ou o sistema que não responde devido a falha de ambiente 
interno, o bug deve ser rejeitado, neste caso o bug pode servir como relatório para 
análise de infraestrutura.
Figura 2. Processo demonstrando a ação tomada após execução dos testes.
Os testes de regressão, independentemente da abordagem automatizada ou 
manual, tem sua importância, pois é realizado quando o software ou seu ambiente 
é modificado. Assim, este tipo de teste tem como finalidade tentar assegurar que 
defeitos não tenham sido introduzidos ou mascarados nas áreas não alteradas 
do software, como resultado da referida modificação. Porém, a utilização de 
testes automáticos de regressão tem como intuito permitir que a equipe de teste 
concentre esforços nos testes manuais mais críticos.página
38
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 35-40, junho de 2019
Capítulo V - Aplicação de Testes Automáticos com Selenium WebDriver: Um Relato de Experiência no SIGAA3. Resultados
Durante o desenvolvimento da estrutura de integração contínua e automação 
dos testes, algumas dificuldades foram encontradas. Nesta circunstância, merece 
destaque a forte dependência das tecnologias envolvidas, neste ponto a versão 
do Selenium Server, a versão do Junit, a versão do Java e a versão do navegador 
influenciam diretamente na obtenção de sucesso na automatização dos testes. 
Segundo documentação da SeleniumHQ, por exemplo, o Selenium 3.0 funciona 
utilizando a versão Java 8. Com relação aos navegadores, também existe grande 
dependência com a versão disponível. Então, diante deste fato, a decisão foi 
pela utilização de navegador inserido no próprio projeto repositório, em que se 
encontra os testes automáticos. Isto contribuiu para diminuir os problemas com 
dependência das versões dos navegadores, já que a mudança ocorre em curto 
espaço de tempo e muitas das vezes o navegador disponível no servidor não é 
compatível com as versões das ferramentas utilizadas no projeto. 
Outra dificuldade encontrada diz respeito aos dados físicos inseridos no 
banco, pois para execução de um determinado caso de teste, existe a necessidade 
de um cenário específico. Muitas vezes, encontrar tal condição ou mesmo preparar 
os dados, mesclando as informações com os já existentes no sistema para atender 
a esses cenários torna-se difícil, então optou-se pela abordagem de dados apenas 
fictícios e que uma vez utilizados devem ser removidos do banco de dados para 
posterior reutilização pelos casos de teste, com isso fez-se necessário conexão 
com o SGBD. 
A equipe também percebeu que a abordagem de execução de todos os roteiros 
de teste de vários módulos do sistema em escala linear e dependentes entre si, 
poderia ocasionar o problema de não execução dos módulos do fim da fila de 
execução. Apesar de uma abordagem linear dependente possibilitar a diminuição 
da repetição de dados, pois dados criados nos primeiros roteiros, poderiam ser 
utilizados para os roteiros seguintes, tem-se o contraponto de poder ocasionar a 
diminuição de execução de casos de teste por completo, já que ao encontrar uma 
falha, toda execução é interrompida e só poderá continuar caso seja removido, o 
que ocasionou o impedimento. 
Uma abordagem posterior consistiu em optar por desconsiderar a repetição 
dos dados e utilizar uma granularidade menor de execução, com a divisão da 
execução desses testes por módulos dos sistemas, logo os módulos que estão 
mapeados em um projeto são executados de maneira independente, caso haja 
parada ou não de um projeto, quer seja por um bug, ou mesmo alteração de 
código ou problema do código de teste ou infraestrutura de forma geral, o próximo 
módulo será executado. Com isso, observou-se um pequeno crescimento no tempo 
de execução, porém o alcance de execução dos testes foi aumentando. E assim fica 
perceptível que além do problema de dependência tecnológica, a dependência 
dos dados utilizados nos testes também é significativa.página
39
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 35-40, junho de 2019
Capítulo V - Aplicação de Testes Automáticos com Selenium WebDriver: Um Relato de Experiência no SIGAACom relação a complexidade do tipo de teste utilizado na automatização, 
foi verificado que testes mais robustos também demandam grande esforço para 
manutenção dos casos de teste, então a opção adotada foi utilizar testes semelhante 
aos testes exploratórios, com isso o objetivo não era uma forte verificação de 
regras de negócio, mas apenas se o fluxo principal da funcionalidade do sistema 
está conforme especificação. As ações principais são testadas observando se o 
sistema fez o que era esperado. Por exemplo, em um teste de cadastro de aluno, 
é verificado se o sistema realizou o cadastro corretamente e se está exibindo as 
mensagens corretas.
Realizando a junção de todos os projetos de teste, até o momento, obtém-
se uma bateria de testes com cerca de 240 casos de teste. Os módulos do SIGAA 
que possuem casos de teste automáticos escritos, são: Stricto Sensu, Diplomas, 
Graduação e Técnico. O tempo de execução total dos roteiros dura em média de 2 
horas e 30 minutos para testar esses módulos. Se fosse mantida a mesma quantidade 
de casos de testes, porém aumentando sua complexidade, provavelmente 
aumentaria o tempo de execução e haveria maior esforço da equipe na elaboração 
e manutenção desses testes.
Os bugs reportados pela automação de testes, nos módulos do sistema 
SIGAA, contabilizando o período de fevereiro de 2018 até fevereiro de 2019 está 
demonstrado na Tabela 1:
Tabela 1. Quantidade de bugs reportados pela automação de testes
Cadastro da Automação Quantidade
Bugs 7
Falso Positivo (Infraestrutura) 13
Falso Positivo (Alteração de Sistema) 8
TOTAL 28
Desta tabela é possível perceber que 28 bugs foram reportados pela 
automação, apesar de haver testes funcionais manuais em todo o sistema. 
Desses bugs, apenas 7 eram bugs de fato enquanto que, 21 bugs reportados 
eram falsos positivos. Neste grupo, há uma divisão entre problemas recorrentes 
da infraestrutura e os que representam modificações no sistema pela equipe de 
desenvolvimento, tornando o caso de teste desatualizado. Nesta situação, esses 8 
bugs podem fazer parte do relatório de rastreabilidade de mudanças do sistema e 
também demandam a correção do roteiro de teste automático, ao qual o caso de 
teste falhou. 
Com isso, pode-se perceber que existe a necessidade de atualização dos 
casos de testes criados na ferramenta Selenium WebDriver, já que a cada mudança 
no sistema ou em tabelas de banco de dados, os testes param de funcionar e só 
voltam a rodar automaticamente se forem corrigidos. Desta forma, é necessário 
que exista uma equipe dedicada a manter os testes automáticos funcionando.página
40
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 35-40, junho de 2019
Capítulo V - Aplicação de Testes Automáticos com Selenium WebDriver: Um Relato de Experiência no SIGAA4. Conclusão
Este artigo relata a experiência com a automação de testes funcionais 
de regressão no sistema SIGAA. Foram elencados tantos os benefícios quanto 
algumas dificuldades enfrentadas durante o processo de realização desses testes. 
Na UFPB, existe uma equipe dedicada aos testes de sistemas, o que tornou viável 
a implementação de testes automáticos e manuais paralelamente.
Com isso, os testes funcionais automático permitem a liberação da equipe 
das tarefas repetitivas e de menor complexidade, permitindo a concentração 
do time em outros tipos de testes, incluindo os manuais, que demandam maior 
concentração e atenção, principalmente para testes de funcionalidades referente 
a regras de negócio.
Porém, a escrita dos testes automáticos requer também grande esforço, quer 
seja devido ao tempo para construção dos casos de teste, quer seja de dificuldade 
enfrentada por uma equipe inexperiente na utilização de testes. Então, sua 
utilização deve levar em consideração estas variáveis. Além disso, a manutenção 
dos testes automáticos, a quantidade de falsos positivos, como demonstrado no 
trabalho, é significativa.
Outro aspecto a ser notado é observar a necessidade de realizar teste 
de regressão no sistema continuamente. Em um contexto, que não há muitas 
mudanças nos requisitos do sistema que já está em uso, a utilização de automação 
de testes pode ser desestimulada. Uma vez que se faz necessário pelo menos um 
membro da equipe de qualidade dedicado à automação. Desta forma, a utilização 
de testes funcionais automatizados de regressão pode ser utilizada, porém os 
contrapontos de sua utilização devem ser levados em consideração.
Referências
BSTQB. (2018) “Certified Tester, Foundation Level Syllabus” , Versão 2018 BR. Disponível 
em: https://www.bstqb.org.br/uploads/syllabus/syllabus_ctfl_2018br.pdf Acesso em: 
Abril/2019
Hudson-ci. (2019) Disponível em: http://wiki.eclipse.org/Hudson-ci Acesso em: 26 
fevereiro 2019.
Junit. (2019) Disponível em: https://junit.org/junit4/ Acesso em: Fevereiro/2019.
SeleniumHQ. (2019) Disponível em: https://www.seleniumhq.org/docs Acesso em: 
Fevereiro/2019.
UFRN. (2006) “Sistemas Institucionais Integrados de Gestão-SIG” , Disponível em:página
41
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 41-46, junho de 2019
Capítulo VI - Aplicativo Siga UFMG: Relato de Experiência no Centro de Computação da UFMGAplicativo Siga UFMG: Relato de Experiência no 
Centro de Computação da UFMG
Patrícia Nascimento Silva¹, Rainer Couto¹
¹Centro de Computação – Universidade Federal de Minas Gerais (UFMG)
Belo Horizonte – MG – Brazil
patricians@ufmg.br, rainerpc@gmail.com
Resumo
Este artigo descreve a experiência do Centro de Computação da UFMG no desenvolvimento de um aplicativo 
acadêmico voltado para os graduandos da UFMG. A metodologia envolveu o estudo das tecnologias existentes, 
a construção de um piloto e o desenvolvimento do aplicativo. O Siga UFMG, em sua versão inicial, reproduziu 
as funcionalidades mais acessadas nos sistemas acadêmicos da UFMG: matrículas, ocorrências, notas e faltas, 
comprovante de matrícula, plano de estudos e alguns relatórios. Foi criada uma visualização da carteira de 
identificação do aluno e notificações que sinalizam alterações nas matrículas e vida acadêmica do discente. O 
projeto incentivou a busca de novos conhecimentos pela equipe perpassando por novas linguagens de programação, 
configuração de plataformas e ambientes, desenho e análise, experiência do usuário (UX), técnicas de teste em 
emuladores e dispositivos móveis, documentação e publicação nas lojas de aplicativos. 
Palavras-chave: Aplicativo. React Native. Siga UFMG.
1. Introdução
No atual contexto tecnológico de convergência de plataformas e mídias, 
os sistemas acadêmicos demandam novas formas de acesso e usabilidade em 
ambientes mobile. O desenvolvimento de aplicativos é uma demanda crescente 
frente a quantidade de dispositivos móveis utilizadas pela população e alunos 
das universidades. Seguindo essa tendência, o Centro de Computação (CECOM) 
da Universidade Federal de Minas Gerais (UFMG) criou um projeto com o objetivo 
de desenvolver um aplicativo, com foco em seus discentes, para facilitar o acesso 
aos dados acadêmicos e funcionalidades existentes nos sistemas acadêmicos da 
Universidade. Ressalta-se que a UFMG não possuía aplicativos com informações 
acadêmicas, somente informações gerais para comunidade.
O aplicativo Siga UFMG objetivou especificamente atender ao grande público 
da UFMG, utilizando o acesso padronizado já existente pelo minhaUFMG, mantendo 
a interoperabilidade de sistemas operacionais móveis (Android, iOS) e integrando 
diferentes sistemas acadêmicos em um único aplicativo (Diário de classe, Siga, 
Pós, Autenticação, etc). Para tanto, o projeto foi desenvolvido em três etapas: (1) 
Estudo das tecnologias existentes; (2) Desenvolvimento de um projeto piloto; e (3) 
Desenvolvimento do aplicativo Siga UFMG.
Como resultados, a curto prazo, espera-se a diminuição da carga nos 
servidores da UFMG, que possui uma demanda grande no período de lançamento / 
fechamento de notas, e a modernização das tecnologias da Universidade. Estima-página
42
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 41-46, junho de 2019
Capítulo VI - Aplicativo Siga UFMG: Relato de Experiência no Centro de Computação da UFMGse também uma maior divulgação e visibilidade nacional e internacional da UFMG, 
além do reconhecimento e autonomia da equipe de tecnologia da informação que 
foi a requerente do projeto. Ademais, o Siga UFMG irá permitir que os discentes 
possam acompanhar melhor sua vida acadêmica de forma simples e rápida.
2. Método
O projeto incluiu diversas atividades relativas ao planejamento, análise e 
desenvolvimento de software que foram subdividas em três etapas apresentadas 
nas seções 2.1, 2.2 e 2.3. 
2.1. Estudo das tecnologias
Na primeira etapa do projeto foi realizado um estudo de viabilidade para 
a implementação das funcionalidades do sistema Siga e do Diário de Classe em 
uma plataforma mobile. Juntamente com este estudo foi realizada uma pesquisa 
exploratória, utilizando da análise documental, para identificar as soluções mobile  
utilizadas por outras Instituições de Ensino Superior (IES), públicas e privadas, 
para seus sistemas acadêmicos. Esta etapa foi realizada nos meses de fevereiro e 
março de 2018. Foram identificadas as tecnologias compatíveis com os sistemas 
da UFMG e observados os aplicativos das seguintes universidades: Unicap, UFU 
Mobile, UFF Mobile, UFMA Mobile, UFRPE Conectada, Puc Minas Mobile, Estácio de 
Sá (Portal do aluno) e FUMEC (Sinef).
Em um segundo momento foram definidas as funcionalidades a serem 
implementadas no ambiente mobile, com seus requisitos básicos, e a linguagem de 
programação, para realizar uma prova de conceito. Para seleção da linguagem foram 
definidos os seguintes critérios obrigatórios: ser livre e gratuita, multiplataforma 
(Android e iOS), ser utilizada em projetos grandes e relevantes, possuir algum 
tipo de documentação / suporte e possuir componentes gratuitos. A linguagem 
escolhida foi o React Native.
O React Native é um projeto desenvolvido pelos engenheiros do Facebook 
e consiste em uma série de ferramentas que viabilizam a criação de aplicações 
móveis nativas para a plataforma iOS e Android, utilizando o que há de mais 
moderno no desenvolvimento front-end, mirando no futuro. É referência no que se 
refere ao desenvolvimento mobile baseado em JavaScript (TABLELESS,  2016). O 
React Native é utilizado por milhares de aplicativos e por grandes empresas como 
Facebook, Uber, Instagram, Skype, Pinterest (REACT NATIVE, 2018). A linguagem 
permite o desenvolvimento multiplataforma (Android e iOS) e está disponível 
para Linux e Windows. As ferramentas de desenvolvimento e emuladores (Android 
Studio) são gratuitos. A linguagem é baseada em javascript possui renderizador de 
componentes e utiliza a arquitetura REST . Também permite funcionamento on-line  
e off-line e numerosos componentes que permitem a implementação de diversas 
funcionalidades, inclusive componentes que permitem a acessibilidade, como 
comando de voz.página
43
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 41-46, junho de 2019
Capítulo VI - Aplicativo Siga UFMG: Relato de Experiência no Centro de Computação da UFMGPor ser recente, a linguagem possui como ponto negativo sua escassa 
documentação. Os fóruns de discussões, com conteúdo relevante na web, são 
poucos, assim como os cursos e treinamentos (pagos e gratuitos) disponíveis no 
mercado. Além disso, a ferramenta de debug é difícil de ser utilizada, já que não 
apresenta com clareza os erros apresentados no código.
Após a conclusão do estudo, que avaliou o escopo e a tecnologia a ser utilizada 
nesta implementação, com destaque para o custo benefício, as contribuições e 
dificuldades encontradas para a solução proposta e o feeedback positivo da 
equipe em relação a viabilidade técnica, foi desenvolvido um projeto piloto com a 
linguagem React Native.
2.2. Desenvolvimento do projeto piloto
Na segunda etapa foi desenvolvido um aplicativo piloto para apresentação 
e aprovação da diretoria. A implementação foi feita na linguagem React Native, 
validada no sistema Android, e contemplou funcionalidades básicas relativas à 
vida acadêmica do aluno. O acesso ao aplicativo foi através do login e senha da 
minhaUFMG e as funcionalidades visualizadas somente por alunos de graduação. 
Nenhuma funcionalidade nova foi implementada e o aplicativo foi disponibilizado 
somente na plataforma Android, uma vez que o teste e a disponibilização do 
aplicativo em ambiente iOS dependia da compra de máquina e dispositivos 
específicos. Essa implementação foi realizada entre abril e maio de 2018.
O projeto piloto foi apresentado para diretoria e sua execução foi autorizada 
em agosto de 2018. Para tanto, antes de iniciar o desenvolvimento do aplicativo, 
outra etapa de pesquisa foi necessária, visto que o ambiente de desenvolvimento 
do React Native era algo novo para a equipe e continha configurações específicas, 
até então desconhecidas. Além disso, vale destacar que a equipe estava alocada 
em outros projetos e sua dedicação era parcial.
2.3. Desenvolvimento do aplicativo Siga UFMG
Por fim, na terceira etapa, foi iniciado o desenvolvimento do aplicativo Siga 
UFMG. O aplicativo possuía funcionalidades consideradas essenciais para os alunos 
de graduação e foi desenvolvido e testado inicialmente no ambiente Android. 
Após a conclusão da versão em Android, em novembro de 2018, foi realizado um 
teste piloto com alunos da UFMG (turma de graduação em direito e alunos que 
eram estagiários e/ou funcionários do CECOM) para validar o funcionamento do 
aplicativo em produção, sua usabilidade e receber feedbacks dos alunos. Para tanto, 
foi criada uma conta de desenvolvedor no Google Play autorizando os e-mails dos 
alunos que demonstraram interesse em participar desta validação inicial. O teste 
piloto foi realizado no período de 07 a 17 de dezembro, ainda dentro do período 
letivo de 2018, e trouxe um retorno positivo com sugestões interessantes dos 
alunos. 
Com o retorno positivo em relação ao funcionamento no sistema Android, 
era necessário verificar se o código também era executado no iOS e realizar página
44
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 41-46, junho de 2019
Capítulo VI - Aplicativo Siga UFMG: Relato de Experiência no Centro de Computação da UFMGtestes. Como a equipe não tinha experiência com o ambiente iOS e há poucas 
informações compartilhadas na internet, a configuração dos equipamentos foi 
difícil e trabalhosa. Grande parte dos elementos desenvolvidos na plataforma 
Android funcionavam, mas alguns ajustes foram necessários, principalmente em 
relação a funcionalidade de notificações. Destaca-se que o fato da linguagem 
React Native ser multiplataforma foi fundamental para otimização do tempo de 
desenvolvimento e manutenção nas duas plataformas. O registro da UFMG na loja 
de aplicativos da Apple também foi um desafio a parte, já que a conta utilizada 
inicialmente não era institucional e o processo de registro e pagamento não 
era de conhecimento dos gestores, que precisaram realizar pesquisas e buscar 
informações sobre os procedimentos a serem realizados.
3. Resultados
O aplicativo Siga UFMG seguiu uma identidade visual criada pelo CECOM, 
aprovada pelo Centro de Comunicação da UFMG. Também teve contribuições da 
Diretoria de Relações Internacionais na tradução dos termos acadêmicos utilizados 
no aplicativo. O desenvolvimento do aplicativo foi finalizado em fevereiro de 2019 e 
seu lançamento ocorreu no dia 05/04/2019. Em apenas 10 dias, após o lançamento, 
o Siga UFMG teve mais de 5 mil downloads e foi bem avaliado nas lojas com média 
de 4,8 em 5,0.
A tela principal (Home) possui as seguintes funcionalidades: Matrículas, 
Ocorrências, Plano de estudos, Percursos Curriculares, Oferta de turma, 
Documentos autenticáveis (Comprovante de matrícula). As funcionalidades 
Perfil, Diário, Eventos e Menu são apresentadas na barra inferior do aplicativo. O 
aplicativo possui a opção de selecionar outros idiomas (espanhol e inglês) e sua 
autenticação é feita através do minhaUFMG, acesso padronizado já utilizado nos 
sistemas da UFMG.
Figura 1. Telas do aplicativo Siga UFMG (tela inicial, home, diário e comprovante 
de matrícula)
Fonte: Dados da pesquisa (2019).página
45
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 41-46, junho de 2019
Capítulo VI - Aplicativo Siga UFMG: Relato de Experiência no Centro de Computação da UFMGJuntamente com o aplicativo foi criado um sistema de notificações que 
notifica eventos registrados no sistema acadêmico e envia para o aplicativo, 
utilizando o serviço Firebase da Google. Desta forma, os alunos são notificados a 
cada lançamento de nota ou ocorrências. Futuramente pretende-se utilizar esse 
sistema para notificar outros eventos e realizar comunicados aos alunos. 
4. Considerações Finais
A UFMG já possuía aplicativos com informações gerais sobre a Universidade, 
contudo o Siga UFMG foi o primeiro aplicativo para acesso aos dados acadêmicos 
pelos discentes. O aplicativo reúne as funcionalidades mais acessadas nos sistemas 
web, pelos alunos de graduação, e trás duas novas funcionalidades: a carteira de 
identificação e as notificações dos eventos. 
O projeto foi uma proposta inovadora para a instituição, já que não existia 
uma equipe com conhecimento prévio para o desenvolvimento de aplicativos, 
equipamentos disponíveis e pessoas dedicadas a essa atividade. O empenho da 
equipe foi primordial para a conclusão do projeto, pois foram inúmeras dificuldades 
encontradas. A equipe foi envolvida em todas as etapas de desenvolvimento e 
além das questões técnicas (linguagem, configuração de ambiente, modelagem 
para dispositivos móveis) questões legais sobre termos e políticas de uso e até 
questões administrativas relativas ao pagamento das lojas de aplicativos e 
compra de equipamentos tiveram o envolvimento dos analistas. Destaca-se que 
o pagamento de taxas para publicação de aplicativos, apesar de envolver valores 
baixos em relação ao custo/benefício, foi um processo no qual a administração 
não tinha domínio e exigiu um tempo para sua resolução. Contudo, ao longo do 
projeto, a equipe conseguiu superar os desafios e as limitações existentes em 
uma instituição pública. Para o caso da Apple foi possível fazer um convênio para 
utilizar uma conta institucional.
Utilizar o React Native trouxe inúmeros benefícios para a manutenção do 
aplicativo que possui o mesmo código para as plataforma Android e iOS. Foram 
necessárias apenas algumas adaptações, realizadas de forma pontual, para alguns 
componentes que não são compatíveis com ambas plataformas. O lançamento do 
aplicativo foi uma surpresa agradável aos graduandos no início de 2019. A equipe 
do CECOM tem a intenção de evoluir o aplicativo com novas funcionalidades, 
utilizando dados de geolocalização e criando versões para outros perfis: pós-
graduandos, professores, funcionários. A solicitações acadêmicas (aos colegiados) 
e o processo de matrícula também são funcionalidades relevantes que pretendem 
ser implementadas no aplicativo.
Sugere-se que projetos inovadores, que utilizem tecnologias recentes, 
entrem para o planejamento estratégico das universidades e possam ganhar seu 
merecido destaque. A criação de um núcleo de inovação e tecnologia é importante 
para a evolução tecnológica da universidade e irá destacar a autonomia da equipe 
de TI, que na maioria do tempo atende as demandas de outras Pró-Reitorias e 
raramente tem a oportunidade de propor inovações nos recursos e sistemas que 
conhece e atua diretamente. página
46
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 41-46, junho de 2019
Capítulo VI - Aplicativo Siga UFMG: Relato de Experiência no Centro de Computação da UFMGReferências
TABLELESS. React Native: Construa aplicações móveis nativas com JavaScript. 
Disponível em: <https://tableless.com.br/react-native-construa-aplicações-moveis-
nativas-com-javascript/>. Acesso em: 10 mar. 2018.
REACT NATIVE. React Native. Disponível em: <https://facebook.github.io/react-
native/>. Acesso em: 10 mar. 2018.página
47
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 47-52, junho de 2019
Capítulo VII - Avaliação do Uso da Internet sem fio pelos usuários da UFAM a partir do 
Reconhecimento de Entidades no TwitterAvaliação do Uso da Internet sem fio pelos 
usuários da UFAM a partir do Reconhecimento de 
Entidades no Twitter
Rodrigo A. Costa¹
1 Centro de Tecnologia da Informação e Comunicação – 
Universidade Federal do Amazonas (UFAM)
Caixa Postal 69080-900 – Manaus – AM – Brasil
{rodrigocosta}@ufam.edu.br
Resumo
Um dos serviços disponibilizados para a comunidade acadêmica dentro da Universidade Federal do Amazonas 
(UFAM) é o acesso à Internet sem fio (Wi-fi). Devido às características da região em que se encontra, esse acesso pode 
sofrer interferências como chuvas em excesso, rompimento de fibra óptica e, falhas na transmissão do serviço que 
provocam a sensação de má prestação no serviço por parte do Centro de Tecnologia da Informação e Comunicação 
(CTIC) Este trabalho apresenta uma análise de sentimentos feita a partir de tweets publicados por usuários da 
UFAM. Para isso utilizou-se de técnicas de Reconhecimento de Entidades para realizar a detecção e classificação de 
palavras em bases textuais coletadas do Twitter.
1. Introdução
O Reconhecimento de Entidades Nomeadas (REN) é uma subarea de estudo no 
campo de Extração de Informação, cujo objetivo consiste em identidas entidades 
nomeadas, bem como classificá-las dentro de um conjunto de categorias pré-
definidas [Jing, 2012], algumas dessas categorias são: Pessoa, Organização e Local. 
Outros tipos de entidades podem ainda ser identificadas conforme o cenário de 
pesquisa abordado. 
REN é uma técnica amplamente utilizada em Processamento de Linguagem 
Natual (PLN) e consiste na identificação de nomes de entidades-chave, presents 
na forma livre de dados textuais. Nesse sentido, a entrada para um Sistema de 
extração de entidades nomeadas é um texto em sua forma livre, e sua saída é 
um conjunto de textos anotados. É possível ainda, definir quais palavras são 
importantes dentro de um context para que possam ser classificadas de acordo 
com a necessidade [Amaral, 2012].
Com a utilização de técnicas de PLN pode-se processar uma entrada de 
texto e inferior resultados como: opniões sobre produtos, termos mais frequentes 
em uma conversa, tradução automática [Ritter et al. 2011] ou ainda, análise de 
sentimentos a partir de posts em redes sociais. Este trabalho demonstra uma 
avaliação sobre a utilização da Internet sem fio pelos usuários da UFAM, coletando 
tweets e realizando o processamento necessário para obter a correta classificação 
das entidades encontradas.página
48
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 47-52, junho de 2019
Capítulo VII - Avaliação do Uso da Internet sem fio pelos usuários da UFAM a partir do 
Reconhecimento de Entidades no TwitterO restante do trabalho está dividido como segue: Na seção 2, é apresentado 
o método proposto para o reconhecimento de entidades contendo as etapas de 
coleta de dados, pré-processamento e extração de entidades; a seção 3, apresenta 
o código utilizado para o desenvolvimento; a seção 4, contém as considerações 
finais acerca de todo o trabalho realizado juntamente com as conclusões obtidas.
2. Método Proposto
O método proposto na elaboração deste trabalho (Figura 1) consiste nas 
seguintes etapas 1) Coleta de dados provenientes do Twitter para construção da 
base de dados; 2) Pré-processamento dos dados, a fim de retirar possíveis ruídos 
dos tweets e; 3) Extração de relações utilizando técnicas para Reconhecimento de 
Entidades Nomeadas. A seguir é detalhada cada uma dessas etapas:
Figure 1. Esquema geral do método proposto.
2.1. Coleta de Dados
A base de dados utilizada neste trabalho é composta por tweets escritos em 
português, correspondente ao período de 01/01/2018 a 31/12/2018. Os tweets  
consultados para criação da base de dados são relativos à opnião dos discentes 
com relação ao serviço de Wi-fi oferecido dentro da Universidade Federal do 
Amazonas.
A busca por tweets pode ser feita de duas formas: 1) Por meio da Search API, 
existente na própria rede social e sendo acessível utilizando a linguagem Python 
ou, 2) Utilizando a opção de busca avançada.
Neste trabalho, foi utilizada a opção de busca avançada (Figura 2) que 
permite a consulta de dados por meio de diversos filtros utilizando uma query de 
busca, que é equivalente a uma expressão booleana evolvendo diferentes termos. 
A vantagem desse filtro é a possibilidade de consulta por um determinado usuário 
que fez um comentário ou, ainda, escolher para quem esses comentários foram 
destinados. A query utilizada foi “(internet OR net OR wifi OR wi-fi OR rede) AND 
ufam” e, resultou em uma base com um total de 795 tweets.
2.2. Pré-processamento
Após a etapa de coleta de dados, os tweets foram pré-processados para 
remoção de possíveis ruídos presentes nos comentários feitos pelos usuários. 
Essa atividade é importante, pois, além de deixar o tempo mais limpo, reduz-se página
49
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 47-52, junho de 2019
Capítulo VII - Avaliação do Uso da Internet sem fio pelos usuários da UFAM a partir do 
Reconhecimento de Entidades no Twitteros caracteres indesejados e consequentemente o tamanho da base de dados e 
processamento posterior. 
Esta etapa consistiu ainda em converter as palavras para minúsculo e, 
retirar todo e qualquer tipo de sinal gráfico (vírgula, hífen, ponto final, ponto de 
exclamação, ponto de interrogação, hashtags, acentuação gráfica e aspas), links, 
urls de sites e stopwords. Stopwords são palavras que não tem significado forte 
dentro de uma frase, por exemplo: né, assim, mas, etc, pois é.
2.3. Extração de Relações
Na etapa de extração de relações, é onde se procura por padrões específicos 
entre os pares de entidades que ocorrem próximos um do outro no texto, usando 
esses padrões para construir tuplas registrando as relações entre as entidades.
Figure 2. Opções para busca avançada no Twitter.
Uma vez que as entidades nomeadas já tenham sido identificadas no texto, 
é possível extrair relações existentes entre elas. Esta etapa pode ser feita fazendo 
uso de expressões regulares para extrair apenas aqueles exemplos que combinam 
com as relações que se procura.
3. Desenvolvimento e Avaliação dos Resultados
Após a realização da consulta, foi feita a limpeza dos dados (seção 2.2). Essa 
limpeza pode ser feita também através da linguagem Python com a execução de um 
script ou função. Após isso, foram iniciados os trabalhos referentes ao manuseio 
das entidades.página
50
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 47-52, junho de 2019
Capítulo VII - Avaliação do Uso da Internet sem fio pelos usuários da UFAM a partir do 
Reconhecimento de Entidades no TwitterPara o desenvolvimento desse trabalho foi utilizada a biblioteca NL TK, pois, 
contém funções e algoritmos específicos para o tratamento do texto e manuseio 
de strings e palavras. No código acima, tem-se a seguinte sequência de comandos: 
1. Leitura do arquivo e tokenização: O arquivo tweets.txt é aberto para leitura, 
através dos comandos open() e read(), em seguida, é feita a tokenização das 
palavras e caracteres presentes no texto.
2. Função para retirar caracteres especiais: Responsável por retirar caracteres 
especiais, acentuação e pontuação, para que seja possível trabalhar apenas 
com texto e palavras.
3. Função para retirar stopwords do texto: Para retirar as stopwords foi 
utilizada a base de dados existente na própria biblioteca NL TK contendo 
algumas palavras marcadas como stopwords para a língua portuguesa.
4. Expressão regular: código responsável por tentar encontrar trechos do 
texto que correspondam com o seguinte padrão: um determinante seguido 
de um adjetivo ou substantivo. Com essa expressão, podemos obter como 
resposta, por exemplo, “a internet da ufa está ruim” .
Código 1. Trecho do código utilizado para processamento
     Entrada: Base de dados (tweets.txt)
     Saída: entidades classificadas
     import nltk
     from nltk import *
     tokens = nltk.word_tokenize(open(‘tweets.txt’).read())
     frequência = FreqDist(tokens)
     textoLimpo =  re.sub(r”http§+”, ,texto).lower().replace(’’”).replace(’.’,”)  
     .replace(’;’,”).replace(’-’,”).replace(’#’,”) .replace(’’,”)
     stop = set(stopwords.words(’portuguese’)) textoSemStopwords = ([i for i in     
     texto.lower().split() if i not in stop]) filtrado = (i for i in word 
     tokenize(texto.lower()) if i not in stop)
     tagged = nltk.pos tag(tokens) gramatica = (NP: (DT)?(JJ)*(NN)) cp = 
     nltk.RegexpParser(gramatica) result = cp.parse(tagged)
A partir da execução dos códigos, seção 3, foi possível identificar e quantificar 
a relação de adjetivos utilizados pelos usuários para descrever o funcionamento e 
caracterizar o serviço de Wi-fi disponível na UFAM, visível na Figura 3, abaixo:página
51
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 47-52, junho de 2019
Capítulo VII - Avaliação do Uso da Internet sem fio pelos usuários da UFAM a partir do 
Reconhecimento de Entidades no Twitter
Figure 3. Quantidade de avaliações detectadas.
Para essa avaliação nem todos os tweets foram considerados, pois, devido à 
string de busca utilizada alguns tweets relatavam informações à cerca de serviços 
similares, tais como: manutenção de rede lógica, rede elétrica, informações 
institucionais, entre outros. Ainda foi possível registar para quais finalidades o 
serviço de Wi-fi estava sendo utilizado pelos usuários, a Figura 4, abaixo, mostra 
a porcentagem de uso desses serviços. Os números percentuais são relativos à 
quantidade de tweets que continham a presença de algum termo desejado para 
busca (adjetivos entre os serviços utilizados).
Figure 4. Serviços listados pelos usuários.
4. Resultados e Considerações Finais
Os resultados obtidos mostram que com uma análise simples de dados é 
possível descobrir, a quantidade de menções de um termo em uma base textual, 
contar a frequência desses termos e até mesmo descobrir sobre o que está sendo 
falado em uma rede social, podendo servir como uma análise preliminar do 
comportamento dos usuários.página
52
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 47-52, junho de 2019
Capítulo VII - Avaliação do Uso da Internet sem fio pelos usuários da UFAM a partir do 
Reconhecimento de Entidades no TwitterDentro da área de gestão de TI, as informações obtidas poderiam ser 
utilizadas, por exemplo: para verificar qual serviço está demandando mais recursos 
da rede lógica auxiliando na tomada de decisões por parte da Coordenação 
de Infraestrutura, criando medidas para configuração de firewall, bloqueio de 
portas ou, limitação da banda de Internet, caso necessário. Outra possibilidade 
interessante seria a de cruzar essas atividades relatadas com o respectivo 
desempenho acadêmico dos discentes ao longo dos semestres, verificando se 
esse comportamento pode estar influenciando nas notas.
Neste trabalho, a ideia geral foi utilizar técnicas simples de PLN que possam 
ajudar a estruturar a preparar uma base textual envolvendo as tarefas iniciais 
de classificação de texto, como por exemplo: tokenização, parser semântico 
e, reconhecimento de entidades. Para realizar uma análise mais completa é 
necessário utilizar técnicas avançadas de aprendizagem de máquina visando o 
treinamento e geração de um modelo que possa identificar e classificar tweets  
ou, qualquer outro tipo de conjunto de dados, de maneira automática e utilizando 
uma quantidade maior de dados.
Referências
Amaral, D. O. F. (2012). O reconhecimento de entidades nomeadas por meio de 
conditional random fields para a língua portuguesa. 
Jing,J.(2012). Information extraction from text. In Smith-Jones, A. B., editor, In Mining 
Text Data, pages 11–41. Publishing Press.
Ritter, A., Clark, S., Etzioni, O., et al. (2011). Named entity recognition in tweets: an 
experimental study. In Proceedings of the conference on empirical methods in natural 
language processing, pages 1524–1534. Association for Computational Linguistics.página
53
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 53-58, junho de 2019
Capítulo VIII - Case de implantação modularizada do GLPI com foco na satisfação do usuário finalCase de implantação modularizada do GLPI com 
foco na satisfação do usuário final
Elton P. Rosa¹
¹ Diretoria de Tecnologia da Informação (DTI) – Universidade Federal dos Vales do Jequitinhonha 
e Mucuri (UFVJM) – 39.100-000 – Diamantina – MG – Brazil
elton.pereira@ufvjm.edu.br
Resumo
Este artigo descreve a implantação do software GLPI na Diretoria de Tecnologia da Informação da UFVJM de 
forma modularizada e estruturada hierarquicamente visando uma melhor gestão de chamados sem comprometer 
a usabilidade do usuário final durante o ciclo de vida de suas solicitações. O sistema foi adaptado levando em 
consideração o refinamento realizado no catálogo de serviços da DTI, automatizando várias etapas de triagem de 
chamados e desburocratizando o processo de abertura e aprovação dos mesmos através da utilização de formulários 
personalizados. Os resultados são elencados através do feedback dos usuários, por meio da pesquisa de satisfação 
interna, e a visualização de dados gráficos providos pelas métricas do sistema.
Palavras-chave: glpi, atendimento, suporte, usabilidade, catálogo de serviços
1. Introdução
A Universidade Federal dos Vales do Jequitinhonha e Mucuri (UFVJM), com 
sede em Diamantina-MG, adotou desde 2009 um sistema de gestão administrativa 
e acadêmica cedido por outra universidade federal. Desde então, a evolução e 
manutenção do software é realizada por equipe própria da Diretoria de Tecnologia 
da Informação (DTI) da UFVJM composta por 10 Analistas de TI e o Chefe de Setor.
Este sistema é composto por vários módulos em especial o de Requisições 
de Serviços Internos. Neste, são cadastrados os setores que prestam serviços 
aos outros setores da universidade, os serviços oferecidos que em grande parte 
estavam cadastrados de forma bem genérica iniciando com o nome de manutenção 
(manutenção em computadores, manutenção em ponto de rede, etc) e a equipe 
envolvida na execução dos mesmos. 
Entretanto, para qualquer solicitação, é necessária a posterior autorização 
da Chefia Imediata do solicitante para que o serviço possa ser executado mesmo 
que não tenha custo diretamente envolvido, acarretando em grande burocracia 
no processo de solicitação e consequente demora na prestação do serviço, sem 
contar que o processo de abertura de solicitação via sistema possui em torno de 
doze etapas. Além disso, não é possível validar os dados informados pelo usuário, 
acarretando em cancelamento da solicitação quando são insuficientes e abertura 
de nova ou em constante contato telefônico. página
54
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 53-58, junho de 2019
Capítulo VIII - Case de implantação modularizada do GLPI com foco na satisfação do usuário finalDiante desse cenário, a DTI procurou alternativas em médio prazo para 
melhorar a gestão de chamados/requisições, facilitar o processo de abertura 
para os usuários e promover transparência durante todo o ciclo de vida dos 
atendimentos.
Este artigo descreve todos os procedimentos que culminaram na implantação 
do sistema de origem francesa Gestionnaire Libre de Parc Informatique (GLPI) 
[Teclib 2015] na Diretoria de Tecnologia da Informação.
2. Métodos
Para o alcance dos objetivos propostos, foram identificadas várias fases com 
destaque para quatro etapas: revisão do catálogo de serviços da DTI, escolha de 
software específico para gestão de chamados, personalização do sistema com foco 
no usuário final e capacitação da equipe de TI na solução. 
2.1. Revisão do Catálogo de Serviços da DTI
O catálogo de serviços da DTI estava bastante defasado, não condizente com 
o cenário atual. Foi realizado um trabalho minucioso no qual ocorreram várias 
reuniões e levantamentos detalhados dos serviços prestados em cada setor da 
Diretoria, inclusive identificando os dados mais comuns solicitados durante o 
processo inicial da solicitação de atendimento.
Foram estabelecidas as categorias macro do catálogo: Ajuda e Suporte 
Técnico; E-mail Institucional; Telefonia Fixa; Redes e Internet; Desenvolvimento 
de Sistemas; Servidores e Segurança da Informação. Em cada categoria, foram 
mapeados os seguintes dados: Serviço; Descrição do Serviço; Dados necessários 
para o atendimento; Regras/normas de utilização; Etapas para o processamento do 
serviço; Prazo de Atendimento; Local(is) e Forma(s) de acessar/solicitar o serviço; 
Setor/Equipe de Atendimento; Horário de Atendimento; Público-alvo e Campus 
onde o serviço é prestado.
2.2. Escolha de software específico para gestão de chamados
Um dos principais requisitos para escolha de sistema é que este fosse um 
software livre, seguido de ser construído numa stack semelhante à utilizada nos 
sistemas da UFVJM e que suportasse múltiplos setores de atendimento. Após 
estudo realizado sobre os principais softwares livres (GLPI, osTicket e OTRS), o 
GLPI foi escolhido por satisfazer todos os requisitos elencados além de possuir 
uma ampla variedade de plugins e comunidade bastante ativa.página
55
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 53-58, junho de 2019
Capítulo VIII - Case de implantação modularizada do GLPI com foco na satisfação do usuário final2.3. Personalização do sistema com foco no usuário final
O GLPI foi adaptado priorizando as etapas que envolvem interação do usuário 
no intuito de aperfeiçoar a usabilidade do sistema, melhorando sua experiência. 
Para isso, foram utilizadas como referência as 10 Heurísticas de Nielsen para Design 
de Interface de Usuário [Nielsen 1994] e aplicadas as seguintes personalizações: 
• Alteração de arquivos de tradução do sistema e de plugins;
• Aumento da fonte através de arquivos css;
• Utilização do plugin Form Creator [Teclib 2012] (Figuras 1 e 2) para a 
criação de formulários dinâmicos de solicitação de serviços baseados 
no Catálogo de Serviços, desabilitando o modo nativo do GLPI de abrir 
chamados, e eliminando a avaliação da chefia imediata do solicitante 
em grande parte das solicitações. Desta forma, o formulário é submetido 
diretamente aos setores de atendimento após o preenchimento dos 
campos configurados como obrigatórios. No momento, não há separação 
em níveis de atendimento (1, 2 e 3) sendo os serviços executados pelas 
equipes setoriais com SLAs que variam entre 2, 3, 5, 10 e 15 dias úteis;
• Criação de novos templates de notificações por-email dos eventos do 
sistema;
• Utilização do plugin More Satisfaction [Teclib, Infotel 2018] para a 
criação de pesquisa de satisfação personalizada que é enviada após o 
encerramento do chamado.
Após essas alterações, foram convidados dez usuários de setores 
diversificados para apresentação prévia do novo sistema, do processo de 
solicitação e para validação de usabilidade.
Figura 1. Página inicial para solicitação de atendimento através de formuláriospágina
56
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 53-58, junho de 2019
Capítulo VIII - Case de implantação modularizada do GLPI com foco na satisfação do usuário final
Figura 2. Exemplo de formulário sendo preenchido
A fim de atender a área gerencial, os setores da DTI foram cadastrados como 
entidades e foi instalado o plugin Dashboard [Donato 2014] que fornece vários 
relatórios e gráficos de atendimento.
2.4. Capacitação da equipe de TI na Solução
Toda a equipe de atendimento (coordenadores e técnicos) foi capacitada 
na operacionalização do sistema e destacada a importância de sempre deixar 
o usuário solicitante a par de tudo o que acontece durante o ciclo de vida do 
chamado através do envio de acompanhamentos/notificações via sistema.
3. Resultados
Os resultados foram bastante significativos para os usuários, pois o novo 
processo de solicitação de atendimento é fácil, rápido e transparente durante 
todo seu ciclo de vida. As doze etapas anteriores para solicitação de atendimento 
foram reduzidas para apenas quatro. Além disso, os usuários recebem notificações 
por e-mail para cada ação do sistema: quando o chamado é criado, quando há 
interação do Técnico, seja pedindo informações ou reportando o andamento 
do atendimento, e quando é encerrado. Em todas, há um link com o número do 
chamado em que, ao clicar sobre este, é possível também acompanhar em tempo 
real seu status junto ao sistema.página
57
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 53-58, junho de 2019
Capítulo VIII - Case de implantação modularizada do GLPI com foco na satisfação do usuário finalA DTI teve um feedback muito positivo através das respostas das pesquisas 
de satisfação (Figura 3) com índice de 98% (Figura 4) de satisfação.
Figura 3. Algumas respostas das pesquisas de satisfação
No quesito gestão, é possível mensurar os dados não apenas da DTI de forma 
genérica mas também de forma intrassetorial, exemplificados através das métricas, 
sendo alicerce para possíveis tomadas de decisões estratégicas ou na identificação 
de novas oportunidades. As equipes de atendimento passaram a atender mais 
rapidamente, num leve clima de gamificação, aumentando consideravelmente a 
produtividade, a motivação e o engajamento com o objetivo de se alcançar cada 
vez mais avaliações melhores dos usuários.
Figura 4. Métricas gerais da DTIpágina
58
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 53-58, junho de 2019
Capítulo VIII - Case de implantação modularizada do GLPI com foco na satisfação do usuário final4. Conclusão
Aversão e medo de mudanças são comuns tanto na vida pessoal quanto 
profissional. No ramo da TI, quando são realizadas de forma estruturada, 
transparente e com a participação do usuário final neste processo, podemos criar 
possibilidade de alterar seu jeito de pensar e agir, tornando-o nosso grande aliado 
nos momentos de êxito e de dificuldades, sempre objetivando o cumprimento dos 
objetivos institucionais.
A modulação dos setores da DTI através de entidades no GLPI conjuntamente 
com o aperfeiçoamento do catálogo de serviços contribuiu para uma melhor 
gestão, mais proativa que reativa, produzindo melhores resultados nos serviços 
prestados.
Em um futuro próximo, planeja-se implantar no sistema o catálogo interno 
(TI para TI), inventário de computadores, software e demais ativos de tecnologia 
da informação, além de prestar consultoria aos outros setores prestadores de 
serviços da Universidade que queiram utilizar o GLPI em seus processos.
Referências
1. Donato, S. (2014) “Dashboard plugin for GLPI - Statistics and reports for GLPI. ” , 
https://forge.glpi-project.org/projects/dashboard, Março.
2. Nielsen, Jakob. (1994) “10 Usability Heuristics for User Interface Design” , 
https://www.nngroup.com/articles/ten-usability-heuristics/, Março.
3. Teclib, Infotel. (2018) “More Satisfaction plugin for GLPI” , https://plugins.glpi-
project.org/#/plugin/satisfaction, Março.
4. Teclib. (2012) “Form Creator plugin for GLPI” , https://plugins.glpi-project.
org/#/plugin/formcreator , Março.
5. Teclib. (2015) “GLPI - Gestionnaire Libre de Parc Informatique” , https://glpi-página
59
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 59-64, junho de 2019
Capítulo IX - Caso de migração de telefonia: a partir de uma plataforma proprietária para uma 
solução VoIP com software livreCaso de migração de telefonia: a partir de uma 
plataforma proprietária para uma solução VoIP 
com software  livre
Geovano L. Quatrin1, Volnei D. Pol2, Diego S. Junges3
1,2Departamento de Redes de Telecomunicações-Universidade Federal da Fronteira Sul(UFFS)-CEP 
89815-899-Chapecó-SC-Brasil
3Campus UFFS Laranjeiras do Sul-Universidade Federal da Fronteira Sul(UFFS)
CEP 85301-970-Laranjeiras do Sul-PR-Brasil
{geovano.quatrin, volnei.pol, diego.junges}@uffs.edu.br
Resumo
A Universidade Federal da Fronteira Sul (UFFS) desde a sua implantação fazia uso de uma infraestrutura de 
telefonia híbrida, com expressivo quantitativo de ramais analógicos e tecnologia proprietária, realidade que ao 
longo dos anos apresentou dificuldades de sustentação e ampliação. Este trabalho apresenta o estudo e a solução 
adotada na Universidade Federal da Fronteira Sul para superar as dificuldades de escalabilidade e a reestruturação 
para uma infraestrutura de telefonia baseada na tecnologia Voz sobre IP (VoIP).
Palavras-chave: SIP, VoIP, Telefonia, Asterisk, Voz sobre IP.
1. Introdução
A Universidade Federal da Fronteira Sul (UFFS) é distribuída pelos três 
estados da região Sul do Brasil situada nas cidades de Chapecó/SC, Passo Fundo/
RS, Erechim/RS, Cerro Largo/RS, Realeza/PR e Laranjeiras do Sul/PR. Com essa 
distribuição geográfica, a utilização de meios de comunicações como telefonia 
e videoconferência entre as unidades é indispensável para o funcionamento 
institucional.
No contexto da telefonia, a UFFS desde o início de suas atividades implantou 
uma infraestrutura com centrais Philips Sopho IS3000 (Sopho, 2019) distribuídas 
em cada unidade da instituição, num total de 6 (seis) equipamentos. A solução 
legada  suporta ramais e entroncamentos analógicos, digitais e VoIP e por ser 
proprietária, demanda capacitação técnica, cabeamento específico e contratação 
de licenças para cada funcionalidade. Essa implementação, ao longo dos anos, 
passou a dificultar integrações, expansões, escalabilidade do serviço e elevação 
dos custos de manutenção. Isto por conta de fatores como a redistribuição do 
técnico de telefonia, as constantes fusões da marca, a dificuldade de aquisição 
de novas expansões e suporte do fabricante, escassez de peças genuínas e valores 
elevados para licenças de ramais para as centrais. Tal cenário demandou uma 
reavaliação da atual estrutura e identificação de soluções alternativas.página
60
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 59-64, junho de 2019
Capítulo IX - Caso de migração de telefonia: a partir de uma plataforma proprietária para uma 
solução VoIP com software livreA metodologia para a definição da solução foi baseada em pesquisa técnica 
de tecnologias existentes em bibliografias e no mercado. Sendo inicialmente 
levantadas as funcionalidades e características da solução legada comparada a 
soluções livres e proprietárias.
Entre as alternativas estava a adoção de centrais baseadas em software livre 
e uso de equipe interna de implantação e sustentação da solução. Esta abordagem 
decorre dos custos elevados de tecnologias proprietárias para integrar aparelhos 
telefônicos legados de diversas marcas em operação na UFFS, na dependência 
técnica e tecnológica de um único fabricante, além da necessidade de licenças de 
uso para ramais.
2. Métodos
Os métodos utilizados para definir a solução a ser adotada foram testes de 
possíveis soluções de PABX (Private Automatic Branch eXchange) em software, 
dentre elas o PABX-IP integrante do Fone@RNP , SNEP (SNEP , 2019), Elastix 
(Elastix, 2019) versão 2.5 e Issabel (Issabel, 2019) (fork opensource da versão atual 
do Elastix), obtendo-se um mapa de aderência favorável a este último, o qual 
implementa nativamente a maioria das facilidades utilizadas até então no PABX 
legado. Destaca-se também o fato de que as soluções avaliadas são baseadas no 
Asterisk, um framework de telecomunicações opensource que dispõe de vasta 
documentação mantido por uma ativa comunidade de desenvolvedores.
Primeiramente foi realizada a implantação piloto do PABX-IP Issabel na unidade 
Campus Laranjeiras do Sul, onde a solução foi instalada em uma máquina física 
com uma interface PCI KHOMP E1 (K1E1, 2019), para fins de teste de desempenho 
e avaliação das facilidades presentes. No processo foi gerada documentação da 
configuração, que serviu como base para as posteriores implantações.
3. Implantação e Resultados
Aproveitando-se da já existente infraestrutura de aplicações, onde há um 
ambiente de virtualização em cada unidade da instituição, foram realizados testes 
do PABX-IP rodando em máquina virtual na unidade Campus Chapecó e decidiu-se 
pela implantação do PABX-IP na UFFS com uso de gateway EBS E1/GSM modelo 
Khomp EBS (KHOMP , 2019), conforme ilustrado na figura 1, onde o enlace com a 
operadora de telefonia ocorre em um appliance externo conectado à mesma rede 
do PABX-IP .página
61
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 59-64, junho de 2019
Capítulo IX - Caso de migração de telefonia: a partir de uma plataforma proprietária para uma 
solução VoIP com software livre
Figura 1. Topologia presente
Na antiga e nova infraestrutura de telefonia a comunicação interna entre 
ramais de diferentes centrais ocorre via entroncamento SIP (Session Iniciation 
Protocol). No novo PABX as regras de roteamento de chamadas são mais flexíveis 
em comparação ao PABX legado, que exigia códigos de acesso para cada destinação 
desejada.
O serviço Fone@RNP (serviço prestado pela Rede Nacional de Ensino e 
Pesquisa - RNP) foi também integrado à topologia preexistente, como mostrado na 
figura 1. A infraestrutura local de cada campus segue a citada topologia, e todos os 
seis gateways transparentes (GWT) das unidades da UFFS consultam os SRL1 (SIP  
Router Local) e SRL2 instalados na UFFS.
Uma nova contratação de operadoras de telefonia fixa comutada ocorreu 
concomitantemente às novas implantações das centrais telefônicas, na qual 
para as unidades da cidade de Chapecó ocorreu a fusão das estruturas, tendo a 
contratação de apenas um enlace E1 para atender as duas unidades presentes na 
cidade. Isto aliado à reestruturação da faixa de endereçamento de rede dedicada à 
telefonia, centralização de ramais, unificação de provisionamento e centralização 
de gestão da infraestrutura em um único ponto.
O parque de telefonia da Unidade Reitoria era constituída por telefones 
analógicos e digitais (58%) e telefones VoIP (42%) e o PABX da unidade, enquanto 
o Campus Chapecó era atendido por sua própria central onde 100% dos telefones 
são VoIP . Com a implantação de um único PABX-IP para atender as duas unidades 
da cidade de Chapecó com único enlace E1, a Unidade Reitoria teve 100% de seus 
ramais migrados para VoIP . Entre as unidades foi feito uso do enlace de dados ponto página
62
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 59-64, junho de 2019
Capítulo IX - Caso de migração de telefonia: a partir de uma plataforma proprietária para uma 
solução VoIP com software livrea ponto preexistente que permitiu que os ramais da Reitoria estejam autenticados 
e provisionados no PABX-IP localizado no campus Chapecó.
A fim de facilitar a migração utilizou-se de um mecanismo de provisionamento 
para os telefones VoIP já em uso na instituição. Sendo que no parque de telefones 
institucionais há três fabricantes diferentes, cada um utilizando uma forma de 
provisionamento particular. Para tanto, foram implementados mecanismos e 
templates com base nas especificações dos fabricantes. Sendo assim, possível 
integrar de forma rápida e transparente todos os aparelhos em funcionamento, 
conforme ilustrado na figura abaixo.
Figura 2. Topologia de provisionamento
Na solução de provisionamento os equipamentos:
• Para equipamentos do fabricante Grandstream, o provisionamento 
acontece via templates de arquivos de configuração recebidos via TFTP 
(Trivial File Transfer Protocol), das quais suporta o uso da opção tftp-
server-address do serviço DHCP (Dynamic Host Configuration Protocol) 
(Internet, 2018), que possibilita ao telefone receber o endereço IP e o 
endereço do servidor TFTP  (Grandstream, 2012) no qual encontra-se sua 
configuração de funcionamento.
• Nos telefones analógicos foi adotado o uso de adaptadores de telefone 
analógico (ATA) para a integração dos telefones antigos à nova solução. 
Nesse caso o provisionamento acontece de forma direto ao servidor de 
provisionamento fazendo consultas ao template via TFTP .
• Já os equipamentos do fabricante Siemens (Unify, 2016), o provisionamento 
acontece via templates em XML (eXtensible Markup Language). E neste 
caso foi implementado script na linguagem PHP para a realização do 
provisionamento.página
63
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 59-64, junho de 2019
Capítulo IX - Caso de migração de telefonia: a partir de uma plataforma proprietária para uma 
solução VoIP com software livreDentre as vantagens da utilização de um PABX-IP em software livre está a 
flexibilidade na implantação/implementação de facilidades ou necessidades. 
Uma das possibilidades existentes que será implantada na nova infraestrutura é 
o monitoramento das ligações e dos ramais, visto que o PABX-IP Issabel, a solução 
do fone@RNP e o EBS E1 possuem interface SNMP para monitoramento.
O PABX-IP implantado roda sobre uma plataforma Linux, tornando-se 
possível realizar a captura de pacotes de sinalização SIP (Rosenberg, 2002) para 
análise de disponibilidade e falhas. Para tal utilizou-se a solução Homer, onde um 
agente realiza a captura do tráfego SIP no PABX e após envia a captura para um 
servidor central, onde roda um servidor Kamailio com módulo HEP (HEP , 2012), 
que processa e armazena as transações SIP em uma base de dados para posterior 
análise. No caso da UFFS o Homer foi utilizado para diagnósticos de falhas no 
PABX-IP e vícios na utilização (ativação indesejada de teclas, como “Não Perturbe” , 
“Redirecionamento de chamadas” , etc) dos telefones VoIP .
Figura 3. Ferramenta de diagnóstico
 A implantação da solução como descrita possibilita para a instituição a 
independência tecnológica no âmbito de telefonia, proporcionando possibilidade 
de melhorias conforme as necessidades, visto a solução estar baseada 
principalmente em software livre. Também ressalta-se o fato da solução estar 
hospedada na infraestrutura de virtualização da UFFS que possibilita agregar todos 
os recursos de contingência e segurança em funcionamento (como redundância, 
backup de configurações).
4. Conclusão
A solução apresentada trouxe resultados importantes que já podem 
ser percebidos, dentre eles, está a disponibilização de novos ramais, que 
anteriormente eram limitados em função dos custos, o domínio técnico da página
64
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 59-64, junho de 2019
Capítulo IX - Caso de migração de telefonia: a partir de uma plataforma proprietária para uma 
solução VoIP com software livretecnologia implantada que tem possibilitado integrações e agilidade na solução 
de problemas. Aliado a isso, estão previstas melhorias futuras a serem agregadas 
no âmbito do monitoramento fim a fim da infraestrutura, a integração de lista 
telefônica automática e desenvolvimento de ferramenta de cruzamento de dados 
de ligações, custos e uso.
Referências
Elastix (2019), “Elastix 2.5.0 – Resease Notes” . Disponível em https://www.elastix.org/
blog/changelogs/elastix-2-5-0-resease-notes/.
Grandstream (2012), “SIP Device Provisioning Guide” . Disponível em https://www.
grandstream.com/sites/default/files/faq/gs_provisioning_guide_public.pdf .
Internet Systems Consortium (2018), “dhcp-options” . Disponível em https://www.isc.
org/wp-content/uploads/2018/02/dhcp44options.html.
Issabel (2019), “Issabel IP PBX: Unified Communications Software Platform” . Disponível 
em https://www.issabel.com/
K1E1 (2019), “K1E1 – 300E – simgle E1 network interface board” . Disponível em https://
www.khomp.com/pt/produto/k1e1-spx/.
Khomp (2019), “EBS Modular SPX” . Disponível em https://www.khomp.com/pt/
produto/ebs-modular-spx .
Rosenberg, J., H. Schulzrinne, G. Camarillo, A. Johnston, J. Peterson, R. Sparks, M. 
Handley, and E. Schooler (2002), “SIP: Session Initiation Protocol, ” RFC 3261.
SNEP (2019), “SNEP – Plataforma de Telefonia Inteligente” Disponível em https://snep.
com.br/.
Sopho (2019), “SOPHO iS3000 - Advanced hybrid PBX switching systems” . Disponível em 
https://library.e.abb.com/public/317c84cb86b15f8bc1257bb3003667f4/SOPHO%20
iS3000%20Advanced%20Hybrid%20PBX%20Switching%20System1.pdf .
Unify (2016), “OpenStage Desk Phone IP Provisioning Interface Developer Guide” . 
Disponível em https://wiki.unify.com/images/c/c7/OpenStage_Provisioning_
Interface_Developer’s_Guide.pdf .página
65
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 65-70, junho de 2019
Capítulo X - Catálogo de Serviços: facilitando o acesso aos serviços de TI dentro das Instituições 
de Ensino SuperiorCatálogo de Serviços: facilitando o acesso 
aos serviços de TI dentro das Instituições de 
Ensino Superior
Marcelo A. Santana¹, Italo C. L. Silva¹, Rômulo N. de Oliveira¹, Marcos J. F. Neto¹, João 
B. G. Silva
1Núcleo de Tecnologia da Informação -- Universidade Federal de Alagoas (UFAL)
Campus Arapiraca -- Arapiraca -- AL -- Brasil
{marcelo.almeida,italocarlo,marcos.neto,romulo}@nti.ufal.br
Resumo
A Governança de TI surgiu com o propósito de alinhar a TI aos negócios, promovendo assim um maior controle dos 
ativos de TI e melhorando o gerenciamento de serviços. No caso específico da UFAL campus Arapiraca, foi adotada 
a biblioteca ITIL, instanciada no GLPI. Apesar dos ganhos percebidos na implantação do sistema de service desk, 
verificou-se que ainda existe uma dificuldade por parte da comunidade acadêmica em identificar os serviços os 
quais ela desejava atendimento. Assim, o objetivo do trabalho é desenvolver um catálogo de serviços de TI, integrado 
ao service desk já existente, garantindo uma maior autonomia aos usuários para registrar suas demandas de 
forma simples, rápida e prática. Como resultados preliminares, podemos destacar a redução do tempo de espera no 
atendimento e diminuição do número de ligações referente à solicitação de serviços.
Palavras-chave: Governança, ITIL, Service Desk.
1. Introdução
A Governança de TI (GTI) surgiu com o propósito de alinhar a TI aos 
negócios, promovendo assim um maior controle dos ativos de TI, melhorando o 
gerenciamento de serviços e até mesmo priorizando iniciativas de TI. Com cerca 
de 700 ativos de TI e uma comunidade acadêmica no campus de cerca de 4000 
membros, sentiu-se a necessidade de implantar um conjunto de boas práticas de 
GTI. Para tal, foi adotada a biblioteca que compila melhores práticas usadas para 
o gerenciamento de serviços de tecnologia da informação Information Technology 
Infrasctructure Library [Filho 2012] (ITIL), com foco no gerenciamento de serviços 
de TI [Maxwell 2017].
Para instanciar o modelo foi adotado o software Gestionnaire Libre de Parc 
Informatique (GLPI)1. A preferência pela ferramenta vem dos recursos que ela já 
traz em seu portfólio, tais como: Gestão dos Ativos, Gerenciamento de Problemas, 
Gerenciamento de Mudanças, Gerenciamento de Projetos, Solicitações de 
acompanhamento de chamados abertos usando interface web ou e-mail, regras 
de negócios ao abrir chamados (personalizável por entidade) e Service Level 
Target (SL T) com escalonamento (personalizável por entidade). No GLPI, tomando 
como base o Plano Diretor da Tecnologia da Informação [UFAL 2017], o catálogo de 
serviços de TI foi registrado e estava sendo usado internamente pela equipe de TI.
1 http://glpi-project.org/página
66
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 65-70, junho de 2019
Capítulo X - Catálogo de Serviços: facilitando o acesso aos serviços de TI dentro das Instituições 
de Ensino SuperiorCom a implantação do ITIL ocorreram alguns ganhos, dentre eles podemos 
citar: o alinhamento entre TI e negócio, melhoria na qualidade dos serviços, 
redução de custos e melhoria na eficiência e eficácia dos processos. Contudo, havia 
por parte da comunidade acadêmica algumas dificuldades referentes a solicitação 
de serviço a equipe de TI, como por exemplo informar para a equipe de TI qual 
serviço eles desejavam, ou solicitar atendimento a equipe de TI de problemas 
que eles mesmos poderiam solucionar através de uma simples orientação, sem a 
necessidade de entrar em contato com o setor.
As constantes interrupções nas atividades da equipe de TI na tentativa de 
solucionar a demanda do usuário causavam impacto negativo nos índices de 
produtividade. Isso motivou a equipe a avaliar a forma com que os serviços eram 
conduzidos. Foi decidido pela criação de uma camada dentro do processo de 
gestão de serviços que capturasse, de forma mais intuitiva, a demanda do usuário 
ou instruísse o usuário na solução do problema, reduzindo, assim, a necessidade 
de intervenção humana nesta etapa.
Sendo assim, o objetivo deste trabalho é apresentar a criação de uma 
camada extra, acima do service desk e integrada ao GLPI, que facilitasse o usuário 
na escolha do serviço TI presente no catálogo de serviços, através da melhoria 
na usabilidade, dispondo informações adicionais para um melhor entendimento 
e ocultando outras não tão importantes assim. Ela funcionaria semelhante a um 
menu de restaurante, onde as opções estariam dispostas de maneira organizada, 
categorizada e o usuário escolheria com base na sua necessidade. Todo o 
relacionamento entre esta camada e o GLPI, atualizações de catálogo seriam 
totalmente transparentes para o usuário. 
2. Metodologia
De acordo com Gaseta [Gaseta 2012] dentre os objetivos da governança de 
TI está o de alinhar a TI aos negócios, pois em muitas organizações existem uma 
lacuna entre o que os usuários esperam dos serviços de TI e o que realmente 
a TI pode oferecer. Nesse propósito, as etapas da metodologia utilizada para 
implantação do catálogo de serviços integrado ao sistema de service desk na UFAL 
Campus Arapiraca foram planejadas e serão descritas nas seções subsequentes.
2.1. Catálogo de Serviços
Como item imprescindível na prestação de serviços em tecnologia, o catálogo 
de serviços de TI ajuda a documentar e gerenciar os serviços que a área de TI se 
compromete a entregar. Se, na ponta final da cadeia, o usuário apenas usufrui 
dos serviços prestados pela instituição, no início do processo, esses serviços são 
sustentados por uma infraestrutura de TI que precisa garantir todas essas entregas 
[Opservices 2018].
A partir deste ponto de vista, após a implantação da GTI [Maxwell 2017] ficou 
evidenciado a necessidade de realizar a melhoria no processo já em produção, foi página
67
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 65-70, junho de 2019
Capítulo X - Catálogo de Serviços: facilitando o acesso aos serviços de TI dentro das Instituições 
de Ensino Superiorpercebida a oportunidade de incluir, dentro do processo, o catálogo de serviços. 
Tal catálogo não se restringe apenas aos serviços que são executados pela equipe 
de TI, mas também uma apresentação descritiva dos recursos alocados para 
entrega deste serviço. Dentre os objetivos que a implementação do catálogo de 
serviços traz, podemos destacar uma, que se refere a definir uma imagem clara 
dos serviços prestados a comunidade acadêmica. Fazendo com que a comunidade 
acadêmica saiba exatamente quais são os serviços oferecidos pela equipe de TI.
 
Figura 1 Proposta de mudança. Fonte: elaborado pelo autor
A Figura 1 compara o cenário atual com o cenário proposto por este trabalho. 
O processo para abertura e solução de problemas acontecia da seguinte forma: 
o usuário entra em contato com o service desk através de diversos meios de 
comunicação, tais como: e-mail, telefone, presencialmente. O problema seguia o 
fluxo de processo proposto por Maxwell [Maxwell 2017] e, a depender do problema, 
um chamado era aberto no GLPI, com auxílio do catálogo de serviço, pela equipe 
de TI.
À medida que houve um aumento na demanda dos serviços, verificou-se que 
este tipo de atendimento era muito custoso, trazia muitos problemas recorrentes 
ou até mesmo causava interrupção no trabalho, uma vez que o técnico responsável 
tinha que parar o que estava fazendo para realizar o atendimento. Além disto, 
poderia ocorrer certa frustração caso o usuário entrasse em contato com setor e 
o funcionário responsável por resolver aquele tipo de demanda não estivesse no 
setor, porque na visão do usuário somente o contato direto com aquela pessoa 
poderia resolver o problema dele.
Levando-se em consideração tais aspectos, foi proposto uma reestruturação 
na forma com que a equipe de TI lida com as demandas dos usuários. Neste novo 
modelo, caso necessário a solicitação do serviço será feita diretamente pelo 
usuário, através de uma ferramenta web, que será totalmente integrada com a 
solução implantada para gestão da GTI.página
68
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 65-70, junho de 2019
Capítulo X - Catálogo de Serviços: facilitando o acesso aos serviços de TI dentro das Instituições 
de Ensino SuperiorPara definir o modelo adotado, foi realizado um levantamento junto a 
comunidade acadêmica, com intuito de estreitar a relação e identificar possíveis 
necessidades. Em um segundo momento, foram verificadas soluções já adotadas 
em outras instituições de ensino, com o propósito de identificar possíveis 
recursos que poderiam ser interessantes para a solução em questão. Durante o 
processo de reestruturação do catálogo de serviços foi necessário o engajamento 
dos subsetores de manutenção, redes e desenvolvimento. Reuniões internas 
aconteceram para que analistas e técnicos definissem, de forma objetiva e clara, 
todos os serviços oferecidos por cada setor. Após isso, foi realizada a atualização 
da documentação, onde foi concebido o portfólio de todos serviços os serviços 
oferecidos pelo setor de TI.
2.2. Desenvolvimento
Em paralelo a definição do catálogo de serviços, foi realizado o processo de 
desenvolvimento da interface para incorporar ao processo de gestão de serviços. 
Primeiramente foi realizado um levantamento de requisitos junto à comunidade 
acadêmica. Dentre eles, como requisitos não-funcionais podemos destacar a 
portabilidade, disponibilidade, usabilidade e interoperabilidade. É extremamente 
importante garantir essa interoperabilidade entre os sistemas ocorra de forma 
transparente para o usuário final. 
A implementação da integração entre sistemas aconteceu através da 
comunicação via API do próprio GLPI, utilizando o protocolo LDAP [LDAP 2018] 
como meio de autenticação entre os sistemas. O PHP Laravel [LARAVEL 2018] 
foi utilizado para o desenvolvimento da interface. Ele destaca-se como sendo 
um framework de desenvolvimento rápido para PHP , livre e de código aberto. A 
manutenção do catálogo de serviços ficará a cargo dos técnicos de TI, bem como o 
mapeamento entre os serviços do GLPI e o do referido catálogo.
Dessa forma, a solução funciona da seguinte maneira: (1) inicialmente, o 
usuário navega pelo catálogo de serviços até encontrar o serviço desejado; (2) em 
seguida, ele deve selecionar o serviço; (3) Ao selecionar o serviço, informações 
úteis sobre o mesmo serão disponibilizadas ao usuário; (4) Caso a demanda do 
usuário não seja satisfeita ou necessite de uma análise especializada, ele solicitará 
a abertura do chamado (5); Uma vez criado o chamado, a requisição (ou incidente) 
será registrada e encaminhada diretamente para a equipe técnica responsável. 
A Figura 2 mostra uma comparação entre a abertura de chamado diretamente 
no GLPI e a nova solução. Podemos notar que na abertura através do GLPI, muitos 
campos são exibidos, o que poderia confundir o usuário. Já o catálogo de serviços 
ficou mais claro e objetivo, permanecendo apenas os campos relevantes e com 
textos explicativos, possibilitando aos usuários a abertura dos chamados de forma 
fácil, intuitiva e com poucos cliques. 
 página
69
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 65-70, junho de 2019
Capítulo X - Catálogo de Serviços: facilitando o acesso aos serviços de TI dentro das Instituições 
de Ensino Superior
Figura 2 Comparação entre as telas para criar chamado. Fonte: elaborado pelo 
autor
2.3. Testes
Pretende-se realizar o teste de usabilidade para avaliar não só a experiência 
dos usuários no momento em que interagem com o sistema, mas também como 
as funcionalidades e as regras de negócio impactam na interação entre usuário e o 
sistema. Busca-se testar neste caso se o tipo de informação, a linguagem, o design 
das páginas e a plataforma tecnológica utilizados proporcionam a eles a melhor 
experiência possível de uso. 
2.4. Conscientização dos Usuários
Umas das etapas que exige mais destreza é a conscientização de toda 
comunidade acadêmica. Nessa etapa deverá ser realizada uma reunião com a 
comunidade acadêmica para mostrar a importância da utilização desse sistema 
e o quanto a universidade ganhará com esse novo procedimento e que não se 
trata de uma simples implantação de sistema, mas de uma mudança cultural, 
representada pela forma com que o usuário se relaciona com a equipe de TI.
3. Resultados
Mesmo com o trabalho em fase de implantação foi possível obter alguns 
resultados significativos. Dentre eles podemos destacar:
•  abertura de chamado de forma mais prática e eficiente realizada pelos 
usuários;página
70
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 65-70, junho de 2019
Capítulo X - Catálogo de Serviços: facilitando o acesso aos serviços de TI dentro das Instituições 
de Ensino Superior•  redução do tempo de espera no atendimento, uma vez que serviços 
simples não dependem mais da disponibilidade dos técnicos das unidades 
para o registro;
•  diminuição do número de ligações referente à solicitação de serviços;
•  aumento na disponibilidade dos técnicos.
4. Conclusão
Este trabalho se propôs a mostrar que é possível implantar através das boas 
práticas de governança de TI por meio do ITIL uma camada extra, acima do service 
desk, integrada ao GLPI e transparente ao usuário. Essa solução facilitará que 
os usuários registre as ocorrências de TI de acordo com o catálogo de serviços, 
através da melhoria na usabilidade. Funcionando como um menu de restaurante, 
onde as opções estão dispostas de maneira organizada, categorizada e o usuário 
escolheria com base na sua necessidade.
Vale ressaltar a importância de um projeto dessa magnitude em termos de 
impactos para a instituição como um todo. Não se trata de uma simples implantação 
de sistema, mas de uma mudança de cultura, representada pela forma com que 
o usuário se relaciona com a equipe de TI. Portanto, a condução de um projeto 
dessa dimensão exige o envolvimento direto da administração central em todas 
as etapas.
Referências
Filho, F. C. (2012). ITIL v3. Escola Superior de Redes, 1th edition.
Gaseta, E. R. (2012). Fundamentos de Governança de TI. Escola Superior de Redes, 1th 
edition.
LARAVEL (2018). LARAVEL - the php framework for web artisans. https:// laravel.com/. 
Acesso em Março 2018.
LDAP , O. (2018). OPEN LDAP - community developed ldap software. http://www. 
openldap.org/. Acesso em Março 2018.
Maxwell, L. (2017). Maxwell - implantação de governança de ti na UFAL campus 
Arapiraca com foco no gerenciamento de serviços do nti.
Opservices (2018). Opservices - catalogo de serviços de ti. https://www. opservices.
com.br/. Acesso em Abril 2017.
UFAL (2017). UFAL - plano diretor de tecnologia da informação. http://tinyurl.com/
y2lgmboy . Acesso em Abril 2017.página
71
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 71-75, junho de 2019
Capítulo XI - Da Virtual Machine ao Container as a Service: Tornando os Sistemas Integrados de 
Gestão compatíveis com os novos paradigmas de computação em nuvemDa Virtual Machine ao Container as a Service: 
Tornando os Sistemas Integrados de Gestão 
compatíveis com os novos paradigmas de 
computação em nuvem.
Jeysibel de Sousa Dantas¹, Raphael Freire de Araújo Patrício²
1,2Superintendência de Tecnologia da Informação – Universidade Federal da Paraı ́ ba (UFPB) 
Endereço Postal 58051-900 – Campus Universitário I, João Pessoa - PB – Brasil
{jeysibel,raphael}@sti.ufpb.br
Resumo
Apesar dos benefícios da computação em nuvem, os departamentos de TI ainda são céticos quanto aos benefícios e 
relutantes em migrar seus sistemas para novas abordagens.  Este artigo tem a intenção de descrever uma estratégia 
bem sucedida para migração de aplicações e servidores J2EE para novas arquiteturas baseadas em conteinerização 
e computação em nuvem. A partir de um estudo de caso, este artigo explora o processo de migração de uma 
implementação convencional baseada em máquinas virtuais para uma orientada à computação em nuvem, a partir 
de uma perspectiva pós implementação.
Abstract
Despite the benefits of cloud computing, IT departments are still skeptical of the benefits and reluctant to migrate 
their systems to new approaches. This paper intends to describe a successful strategy for migrating applications and 
J2EE servers to new architectures based on containerization and cloud computing. From a case study, this article 
explores the process of migrating from a conventional virtual machine-based implementation to a cloud-oriented 
one, from a post-implementation perspective.
1. Introdução:
A Computação em nuvem é hoje amplamente aceita como um novo paradigma 
para entrega e consumo de recursos de TI, devido a suas características de prover 
serviços de TI: sob demanda, de forma ágil, confiável e escalável,  sem as limitações 
relacionadas a uma infraestrutura tradicional. Apesar dos significantes benefícios 
da computação na nuvem, as organizações ainda são relutantes em adotar soluções 
baseadas na nuvem tendo medo dos riscos e problemas inesperados como queda 
de performance e indisponibilidade dos serviços. Um fator de piora deste cenário 
é que, apesar de haver um número crescente de estudos na área a maioria destes 
estudos são conceituais e os empíricos baseando-se em opiniões que divergem 
diametralmente, conforme o nível de familiaridade do autor com CaaS e SaaS.
A primeira pergunta a ser respondida por este artigo é: Por que migrar serviços 
funcionais e operantes em arquitetura tradicional para uma nova arquitetura, 
baseada em nuvem? Certamente um dos maiores motivadores para esta mudança 
foram as limitações do modelo tradicional em uso:  Dificuldades para atualizações 
de software sem tornar o serviço indisponível, mal gerenciamento dos recursos 
de hardware, dificuldade de instrumentar e monitorar o estado de aplicações e página
72
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 71-75, junho de 2019
Capítulo XI - Da Virtual Machine ao Container as a Service: Tornando os Sistemas Integrados de 
Gestão compatíveis com os novos paradigmas de computação em nuvemprincipalmente o descompasso entre os requisitos gerados pela introdução dos 
novos modelos de integração contínua/entrega contínua e dificuldade de atender 
tais requisitos, dadas as limitações.
Este trabalho tem o intuito de descrever, a partir da resposta a esta pergunta, a 
estratégia utilizada para migrar os Sistemas Integrados de Gestão da Universidade 
Federal da Paraíba, de forma transparente ao usuário final, de uma infraestrutura 
convencional para uma abordagem de computação em nuvem privada.
2. Método
Uma vez elencada a real necessidade de mudança de paradigma, fora a vez 
de colocar em prática um plano de trabalho para a concretização dos objetivos, tal 
plano foi dividido em 7 etapas: 1) Estudo da arquitetura pré-existente de deploy 
dos sistemas; 2) Escolha de tecnologias que viabilizem a construção de uma prova 
de conceito; 3) Validação da Prova de Conceito; 4) Migração de Ambientes de 
Staging para a nova arquitetura; 5) Migração de Ambiente de Produção para a nova 
arquitetura em caráter experimental; 6) Aprimoramento de Arquitetura para uso 
de computação em cluster; 7) Entrega definitiva de serviços em nova Arquitetura.
2.1 Estudo da Arquitetura Pré-existente
Este projeto começou com o estudo da arquitetura pré-existente de deploy 
dos sistemas. Nesta etapa foi analisada a forma como o sistema estava disposto 
entre várias máquinas virtuais e os relacionamentos entre tais entes para um 
perfeito funcionamento do serviço. Foram elencados requisitos funcionais e não 
funcionais, interdependência de sistemas e subsistemas e o fluxo de informação. 
2.2 Escolha de Tecnologias
Com a definição clara dos requisitos mínimos que a nova infraestrutura 
deveria ofertar para uma transição exitosa e transparente ao usuário final, se fez 
necessária a definição das tecnologias de conteinerização e de orquestramento 
de serviços que deveriam ser adotadas nesta nova pilha. Entre as escolhidas 
inicialmente para avaliação tivemos: Docker Engine, Docker Compose, Kubernetes, 
OpenShift  e Open Stack.
Após um Período de avaliação das tecnologias onde agrupamos cada uma 
por curva de aprendizado, escala de uso, custo de propriedade e facilidade de 
uso foram descartadas: Kubernetes, devido a necessidade de uma maior curva de 
aprendizado e provimento de soluções em uma escala maior do que a desejada 
para satisfazer nossos requisitos; OpenShift, por ser uma solução proprietária cujo 
custo de aquisição de licenças poderia inviabilizar a execução do projeto e Open 
Stack pela dificuldade maior de configuração e operação da infraestrutura. Sendo 
assim, foram escolhidos: Docker Engine como tecnologia de conteinerização e 
Docker Compose como orquestrador.página
73
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 71-75, junho de 2019
Capítulo XI - Da Virtual Machine ao Container as a Service: Tornando os Sistemas Integrados de 
Gestão compatíveis com os novos paradigmas de computação em nuvem2.3 Validação da Prova de Conceito
Passada a fase de escolha e estudo das tecnologias adotadas, iniciou-se a 
construção de uma prova de conceito que inicialmente se baseou em uma réplica, 
em termos de especificações de hardware, dos ambientes de Staging.
Nesta etapa, foram construídos containers que encapsularam os serviços de 
Proxy HAProxy, Banco de dados Postgresql e do JBoss. Atenção especial fora dado 
ao JBoss que passou por um processo de otimização de perfomance.
Durante o processo de otimização de performance do JBoss, foi percebido 
que, para melhorar a gestão de recursos de hardware e propiciar uma efetiva 
orquestração do serviço, maximizando assim os ganhos de performance e 
escalabilidade com a nova abordagem, seria necessário mudar a forma de deploy 
das aplicações.
Dessa forma quebramos o paradigma de termos poucos JBoss em execução 
com muitos sistemas em deploy e com alto consumo de memória RAM, para uma 
estratégia com um número maior de Servidores em execução com um único 
sistema em execução e um perfil de memória RAM mais baixo.
2.4 Migração dos Ambientes de Staging 
Com a prova de conceito devidamente validada, os ambientes de staging 
foram migrados para a nova pilha, de forma a se comportarem transparentemente 
para o usuário final a despeito das profundas mudanças ocorridas internamente 
no design de serviço.
2.5 Migração do Ambiente de Produção em Caráter Experimental
Passados cerca de 9 meses, com a nova arquitetura validada e com as 
eventuais discrepâncias corrigidas, os  sistemas de produção foram migrados 
em caráter experimental, sendo escolhidos dois sistemas de uso menos intenso 
para operação efetiva, ficando os demais em caráter de standby prontos para uso 
eventual.
2.6 Aprimoramentos da Arquitetura para Uso em Modo Cluster
Após um Período de cerca de 5 meses em testes, sem a ocorrência de 
comportamentos inesperados ou incidentes de segurança, o projeto foi escolhido 
como substituto da pilha de serviços em operação. 
Nesta etapa, devido a um maior amadurecimento no uso com a plataforma 
e as novas tecnologias de SaaS e suas evoluções, o orquestrador foi substituído 
de Docker Compose para Docker Swarm, haja vista a evolução natural das 
tecnologias e os ganhos de escalabilidade e disponibilidade, devido ao conceito 
de cluster computacional implementado pelo Docker Swarm. Também neste ciclo, 
foi adicionado o Traefik como descobridor de serviços, responsável por fazer o 
roteamento interno entre serviços.página
74
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 71-75, junho de 2019
Capítulo XI - Da Virtual Machine ao Container as a Service: Tornando os Sistemas Integrados de 
Gestão compatíveis com os novos paradigmas de computação em nuvem2.7 Entrega Definitiva de Serviços em Nova Arquitetura 
Após um Período adicional de dois meses validando a arquitetura dos 
serviços, utilizando o novo orquestrador, os serviços foram colocados em operação 
em caráter definitivo e os serviços ora substituídos passaram a rodar em standby, 
para ser acionados em caso de contingência.
Figura 1. Diagrama representativo da disposição dos serviços em modo cluster 
a partir do uso de Docker Swarm como orquestrador. Exibe ainda as interações 
necessárias para responder com sucesso a um usuário tentando acessar o SIGAA, 
sistema integrado de gestão de atividades acadêmicas.
3. Resultados
Após a efetiva implantação dos sistemas sob nova arquitetura puderam ser 
observados os seguintes ganhos em comparação com a arquitetura anterior: 1) 
Possibilidade de update de versão de sistema sem reiniciar todos os sistemas o 
que promovia indisponibilidade global do serviço; 2) Processo de atualização 
escalonado, container a container, com possibilidade de rollback automático 
em caso de falha na tarefa; 3) Escalabilidade de serviço de acordo com o número 
médio de usuários de cada sistema; 4) Possibilidade de rápida transferência de 
recursos de hardware para alocação em sistemas que enfrentem situações de pico 
de acesso; 5) Definição de níveis de serviço que devem ser cumpridos pelo software 
orquestrador; 6) Checagem periódica e automatizada da saúde dos containeres em 
execução com finalização obrigatória de instâncias que apresentem performance 
degradada; 7) Containeres e serviços são transientes por natureza, desta forma, página
75
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 71-75, junho de 2019
Capítulo XI - Da Virtual Machine ao Container as a Service: Tornando os Sistemas Integrados de 
Gestão compatíveis com os novos paradigmas de computação em nuvemcom a devida proteção dos dados, o sistema torna-se resiliente a várias formas 
conhecidas de ataques cibernéticos; 8) Manutenções programadas ocorrem com 
downtime reduzido devido a possibilidade de desligamento parcial do cluster sem 
comprometer a operação globalmente.
Outro importante ganho foi com relação a possibilidade de telemeteria dos 
sistemas em operação. Todos os containeres implantados operam com exposição 
de métricas e armazenamento destas em um TSDB, desta forma é possível 
detectar com maior rapidez, por meio de sistemas de monitoramento, padrões 
de anormalidade bem como detectar com facilidade a origem do problema ou 
subsistemas com uma degradação de desempenho. 
Até a presente data, não houve incidentes de segurança que foram causados 
por vulnerabilidades da infraestrutura por si própria, ao contrário, a já citada 
coleta de métricas tem sido eficaz em detectar pontos de atividade incomum e 
contribuir para a rápida resposta dos times de DevOps e Segurança. 
4. Conclusão e Trabalhos futuros
Diante de todo o processo empregado desde a concepção, execução até 
a entrada em operação dos sistemas vê-se o notório ganho de flexibilidade, 
escalabilidade, agilidade e segurança(por meio da auditabilidade e do caráter 
transiente dos sistemas em execução).
Para trabalhos futuros, sugere-se a implantação de sistemas de 
provisionamento automático de infraestrutura de forma a complementar 
o provimento automatizado de serviços, além de melhores abordagens de 
manipulação, backup e recovery dos dados consumidos e produzidos pelos 
serviços em operação. Outra sugestão seria analisar os dados de telemetria 
armazenados, utilizando técnicas de Big Data, a fim de melhor inferir padrões 
sobre os sistemas e seu uso a fim de prover melhorias sob demanda e concentrar 
esforços de desenvolvimento em áreas estratégicas.
Referências
T . Boillat and C. Legner, “Why Do Companies Migrate Towards Cloud Enterprise 
Systems? A Post-Implementation Perspective, ” 2014 IEEE 16th Conference on Business 
Informatics, Geneva, 2014, pp. 102-109.
M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson, 
A. Rabkin, I. Stoica, and M. Zaharia, “A View of Cloud Computing, ” Commun. ACM, vol. 
53, no. 4, pp. 50–58, 2010, Tradução nossa.
T . Dillon, C. Wu, and E. Chang, “Cloud Computing: Issues and Challenges, ” presented 
at the 24th IEEE International Conference on Advanced Information Networking and 
Applications, 2010, pp. 27–33, Tradução nossa.
A. Benlian, T . Hess, and P . Buxmann, “Drivers of SaaS-Adoption – An Empirical Study 
of Different Application Types, ” Bus. Inf. Syst. Eng., vol. 1, no. 5, pp. 357–369, 2009, 
Tradução nossa.página
76
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 76-81, junho de 2019
Capítulo XII - Democratização do Acesso à Informação na UFS: APISistemas e Desenvolvimento Móvel
Multiplataforma com FlutterDemocratização do Acesso à Informação na 
UFS: APISistemas e Desenvolvimento Móvel 
Multiplataforma com Flutter
Alan Passos¹, Leonardo Bezerra²
Superintendência de Tecnologia da Informação (STI) – Universidade Federal de Sergipe (UFS) Av. 
Marechal Rondon, s/n, Jd. Rosa Elze - São Cristóvão - Sergipe – Brasil
alanpassossi@gmail.com, leonardobsjr@ufs.br
Resumo
Este trabalho descreve a evolução da tecnologia no âmbito educacional e administrativo da Universidade Federal de 
Sergipe (UFS) e como esta avançou na democratização dos dados por meio da APISistemas e de aplicações móveis. 
Trata também acerca da estratégia na escolha do Flutter como framework para desenvolvimento mobile.
Abstract
This work describes the evolution of technology in the educational and administrative scope of Universidade Federal 
de Sergipe (UFS) and the progress to data democratization through APISistemas and mobile applications. Also 
depicts the strategy of Flutter as a framework choice for mobile development.
Palavras-chave: Desenvolvimento móvel, Desenvolvimento móvel multiplataforma, Flutter, Frameworks 
multiplataforma.
1. Introdução:
Nos últimos vinte anos, a Universidade Federal de Sergipe (UFS) passou 
por uma ampla informatização de seus processos tanto administrativos quanto 
acadêmicos. O desenvolvimento inicial, como em várias outras instituições, foi 
baseado em um conjunto de sistemas isolados. Contudo, em 2011 a UFS fez um 
convênio com a Universidade Federal do Rio Grande do Norte para utilização de 
um sistema integrado (SIGs) contemplando as áreas acadêmica e administrativa.
Os SIGs informatizaram a maioria dos processos da UFS, cuja comunidade 
gira em torno de 35 mil pessoas. Esta quantidade de usuários, aliado ao elevado 
número de processos e deficiências de infraestrutura computacional, faz com que a 
experiência de uso dentro do dos SIGs não seja uniformemente eficiente; existem, 
por exemplo, módulos consideravelmente mais sobrecarregados que outros em 
razão do número de usuários. Por ser uma aplicação de grande porte, muitas vezes 
a otimização desses módulos não é uma tarefa simples ou rápida. Outro aspecto 
dos SIGs é que eles não são, em geral, responsivos, ou seja, não se ajustam ao 
tamanho da tela do dispositivo que está sendo acessado, o que dificulta seu uso 
através de celulares e tablets.
A solução que a UFS encontrou para mitigar os problemas acima citados 
foi a criação de aplicações móveis que contemplem os processos mais utilizados página
77
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 76-81, junho de 2019
Capítulo XII - Democratização do Acesso à Informação na UFS: APISistemas e Desenvolvimento Móvel
Multiplataforma com Flutterpela comunidade. Entretanto, para dar suporte às vindouras aplicações móveis, a 
universidade precisou estruturar a forma de acesso às suas informações através 
de uma interface de serviços, a qual foi batizada de APISistemas.
2. APISistemas
A APISistemas é uma API RESTful [Gao et al. 2011], criada com dois objetivos 
principais: fomentar a criação de aplicações móveis pela UFS e democratizar o 
acesso aos dados públicos da Universidade, sendo essa última uma exigência da 
Controladoria Geral da União (CGU), em razão da promulgação da Lei de Acesso à 
Informação (Lei 12.257/2011) [Brasil 2011].
A ideia de criar uma API para acesso a dados das instituições não é uma ideia 
nova, mas já é uma realidade em outras instituições como Universidade Federal do 
Rio Grande do Norte (UFRN), bem como no Governo Federal.
Após identificar as tecnologias que melhor se aplica a uma API RESTful foi 
dado início a criação da APISistemas. Diante dos resultados, a API foi desenvolvida 
em Java 8, a fim de manter a compatibilidade com os sistemas da UFS. Utiliza o 
formato Javascript Object Notation (JSON), uma linguagem de representação de 
dados baseada em um subconjunto da linguagem de programação JavaScript, 
para codificar a saída dos serviços [JSON 2019].
O mecanismo de autenticação da API é o OAuth2 [OAUTH2 2019], que é 
um dos mecanismos de autenticação mais utilizados na internet, por empresas 
como Google e Facebook. Consiste em um padrão de autenticação que envolve 
um a autorização de acesso a um recurso, pelo dono do recurso, a um terceiro, 
sem que sejam fornecidas as credenciais do dono do recurso a esse terceiro. 
Utilizando o APISistemas como exemplo, o protocolo permite a um aplicativo 
de terceiros acessar informações pessoais de um aluno da UFS, mediante sua 
autorização junto à UFS (representada pelo APISistemas), sem que seja necessário 
o fornecimento das credenciais do aluno nos sistemas da UFS no aplicativo. Dessa 
forma, não somente o aplicativo recebe as informações que precisa para funcionar 
(mediante autorização), como o aluno tem mais garantias quanto à segurança de 
suas credenciais.
Atualmente a UFS possui dois aplicativos móveis consumindo os vários 
serviços disponibilizados pela API. Até o momento são 15 endpoints, num total 
de 42 serviços relativos tanto à parte acadêmica quanto à parte administrativa da 
instituição.
Existem alguns outros projetos que tem como backend a API, além dos 
aplicativos supracitados. Dentre eles, vale a pena destacar o projeto de Dados 
Abertos, que é a construção de um portal de consulta de dados público baseado 
no CKAN [CKAN 2016], com o objetivo de atender, de forma direta, às exigências 
dos órgãos de controle.página
78
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 76-81, junho de 2019
Capítulo XII - Democratização do Acesso à Informação na UFS: APISistemas e Desenvolvimento Móvel
Multiplataforma com Flutter3. Aplicações móveis
Com a popularização dos aparelhos móveis, a UFS precisou inovar para 
atender às necessidades de um público que tem nos smartphones a principal 
ferramenta de acesso à serviços. O sistema acadêmico é atualmente o que possui 
maior quantidade de acessos e foi verificado pela Superintendência de Tecnologia 
da Informação - STI, em meados de 2018, que quase 50% do acesso se deu por 
meio de dispositivos móveis. Desse total, em torno de 40% vieram de dispositivos 
Android.
Para tentar atender essa demanda, a STI iniciou em 2018 o desenvolvimento 
de aplicativos móveis na plataforma Android. A escolha dessa tecnologia se deu em 
razão da facilidade no acesso às ferramentas de desenvolvimento, bem como pela 
predominância do sistema operacional Android no conjunto de acesso advindo 
dos dispositivos móveis aos SIGs. 
A primeira aplicação produzida pela STI foi o Caixa Postal Mobile, uma 
interface de recepção de mensagens e notificações advindos dos SIGs. Após o Caixa 
Postal, o STI produziu o eUFS. Este segundo aplicativo possibilita aos discentes do 
ensino médio, graduação e pós-graduação realizarem consultas acadêmicas do 
SIGAA, proporcionando acesso rápido às principais funcionalidades dos SIGAA. No 
final de fevereiro de 2019 as aplicações contavam com mais de 5000 instalações e 
mais de 1800 acessos diários, mesmo sem um lançamento oficial na Google Play, 
pois ainda estão em versão beta .
Após a publicação dos aplicativos, começaram a surgir demandas por parte 
dos usuários e do restante da comunidade acadêmica acerca do suporte para 
iOS. Para evitar manter duas versões do mesmo aplicativo, optou-se por passar a 
desenvolver os aplicativos fazendo uso de frameworks multiplataforma. 
4. Desenvolvimento Multiplataforma - Flutter
O desenvolvimento de aplicativos nativos torna o desenvolvimento complexo 
e custoso, em razão da necessidade de especialistas em cada uma das plataformas 
e da necessidade de manutenção de várias bases de código diferentes [ABRANCHES 
2018]. 
Neste contexto surgiram as aplicações híbridas, onde ocorre o desenvolvimento 
de uma aplicação única que pode ser utilizada em diferentes sistemas operacionais. 
Isso é possível em razão dos frameworks para desenvolvimento de aplicações 
multiplataforma, que são incumbidos de empacotar o código-fonte para as 
diferentes plataformas-alvo.
Podemos identificar em [ABRANCHES 2018], que há um leque de frameworks  
disponíveis que permitem o desenvolvimento híbrido. Os mais utilizados pelos 
desenvolvedores são React Native e Ionic. Segundo [INFOQ, 2019], em 4 de 
dezembro de 2018 o Google lançou a primeira versão estável do Flutter, que é 
um framework para desenvolvimento Android e iOS de código aberto que possui 
contribuições de centenas de desenvolvedores de todo o mundo [Google 2018].página
79
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 76-81, junho de 2019
Capítulo XII - Democratização do Acesso à Informação na UFS: APISistemas e Desenvolvimento Móvel
Multiplataforma com FlutterTanto o React Native quanto o Ionic utilizam uma camada chamada Bridge. 
Essa camada é responsável por fazer a comunicação entre código nativo e o 
Javascript. Com o Flutter, há um ganho na performance já que a comunicação com 
o código nativo é feita de maneira síncrona, sem a necessidade da Bridge.
Dessa forma, o aplicativo Caixa Postal Mobile foi recriado em Flutter a fim de 
suportar as duas plataformas mais utilizadas no mercado, Android e iOS. Por ter 
menos funcionalidades que o eUFS, o Caixa Postal Mobile serviu de experimento 
para realização de testes dessa nova tecnologia. 
Após a realização de pesquisas acerca das arquiteturas mais utilizadas 
juntamente com o Flutter, optou-se por utilizar a arquitetura chamada Business 
Logic Component (BLoC). O BLoC trabalha com gerenciamento de estado e faz 
o acesso de dados de um local central em seu projeto, e é recomendada pelos 
desenvolvedores do Google [SURI, 2018].
A ideia básica do BLoC é separar a regra de negócio da interface, bem como 
trabalhar com alterações de estados dos widgets individualmente, ou seja, as 
alterações se aplicam apenas ao componente que está sendo manipulado. Isso 
permite, entre outras coisas, uma melhor divisão do desenvolvimento entre a 
equipe; enquanto uma parte pode trabalhar na interface, outra parte pode trabalhar 
na regra de negócio e ao fim do desenvolvimento, realizar a integração de ambos 
os códigos. Essa separação permite uma redução no tempo de desenvolvimento e 
uma melhor estruturação do código-fonte.
5. Resultados
Foi identificado, por meio do repositório de controle de versão, que o 
desenvolvimento com o Flutter levou menos tempo do que quando foi desenvolvido 
usando o Android nativo. O Caixa Postal Mobile nativo iniciou em outubro de 
2016 e foi finalizado em agosto de 2017, levando 10 meses de desenvolvimento. 
Entretanto, em paralelo com criação do aplicativo nativo, também estava sendo 
desenvolvida a APISistemas o que acabou por elevar bastante o tempo de 
desenvolvimento já que ajustes no aplicativo foram sendo feitos à medida que a 
APISistemas era modificada.
O mesmo projeto desenvolvido em Flutter foi iniciado em janeiro de 2019 
e finalizado em fevereiro de 2019, um período de apenas 2 meses, com a mesma 
equipe que fez o aplicativo nativo. 
Sabemos que a métrica baseada no controle de versão não é efetivamente 
verdadeira e só teremos uma real comparação quando refizer o aplicativo eUFS, 
previsto para o primeiro semestre de 2019. Todavia, levando em consideração 
ainda que o Flutter possibilita a geração para múltiplas plataformas, podemos 
concluir que a reescrita dos aplicativos foi uma escolha que se mostrou acertada 
em razão do ganho de eficiência na implementação das aplicações.página
80
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 76-81, junho de 2019
Capítulo XII - Democratização do Acesso à Informação na UFS: APISistemas e Desenvolvimento Móvel
Multiplataforma com Flutter6. Conclusão
Objetivando atender aos anseios da comunidade, a universidade vem 
promovendo a democratização dos seus serviços e das informações contidas 
nas bases. Em função desses objetivos, foi criada uma infraestrutura (chamada 
de APISistemas) para dar suporte ao desenvolvimento móvel, que culminou nos 
aplicativos eUFS e Caixa Postal Mobile, os quais promoveram uma diversificação 
no acesso à muitas das funcionalidades utilizadas nos SIGs, adequando-se à 
realidade mobile. 
Para que os aplicativos possam ser executados em diferentes plataformas a 
STI decidiu por refazê-los com o framework Flutter. Com sua utilização foi possível 
minimizar alguns problemas relativos ao desenvolvimento de aplicativos móveis, 
dentre os quais se destacam a minimização dos custos, a redução de tempo de 
desenvolvimento e a divisão de tarefas entre a equipe interface e de negócio, além 
de conseguir chegar a toda a comunidade que utiliza o sistema operacional iOS.
Após a obtenção dos resultados, o STI pretende seguir usando Flutter para 
os projetos futuros de aplicativos móveis. Dessa forma, os desenvolvedores 
podem entregar os produtos funcionais e com design padronizado (utilizando 
componentes do framework) em um período menor de tempo se comparado com 
o desenvolvimento nativo em duas plataformas distintas.
Para trabalhos futuros, o STI pretende expandir os serviços da APISistemas e 
desenvolver novos aplicativos móveis.
Referências
ABRANCHES, J. (2018) - Aplicativos e desenvolvimento mobile híbrido x nativo. 
https://imasters.com.br/desenvolvimento/aplicativos-e-desenvolvimento-
mobile-hibrido-x-nativo , 11 jun. 2011. Disponível em: https://imasters.com.br/
desenvolvimento/aplicativos-e-desenvolvimento-mobile-hibrido-x-nativo . Acesso 
em: 08 mar. 2019.
BIRCH, Joe. Stateful or Stateless widgets? https://flutterdoc.com/stateful-or-
stateless-widgets-42a132e529ed, 24 maio 2018. Disponível em: https://flutterdoc.
com/stateful-or-stateless-widgets-42a132e529ed. Acesso em: 1 mar. 2019.
Brasil (2011). Lei nº 12.527. http://www.planalto.gov.br/ccivil_03/_ato2011-
2014/2011/lei/l12527.htm, 18 nov. 2011. Disponível em: http://www.planalto.gov.br/
ccivil_03/_ato2011-2014/2011/lei/l12527.htm. Acesso em: 01 mar. 2019.
CKAN. https://ckan.org/, 2 Nov. 2016. Disponível em: https://ckan.org/. Acesso em: 15 
abr. 2019.
Google (2018). Flutter 1.0. https://developers.googleblog.com/2018/12/flutter-10-
googles-portable-ui-toolkit.html, 4 dez. 2018. Disponível em: https://developers.
googleblog.com/2018/12/flutter-10-googles-portable-ui-toolkit.html. Acesso em: 05 
mar. 2019.página
81
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 76-81, junho de 2019
Capítulo XII - Democratização do Acesso à Informação na UFS: APISistemas e Desenvolvimento Móvel
Multiplataforma com FlutterINFOQ. Google faz lançamento da primeira versão estável do Flutter, um toolkit 
cross-platform mobile. https://www.infoq.com/br/news/2019/01/flutter-1.0-
released, 23 jan. 2019. Disponível em: https://www.infoq.com/br/news/2019/01/
flutter-1.0-released. Acesso em: 11 mar. 2019.
JSON - Introducing JSON. https://www.json.org/. Disponível em: https://www.json.
org/ . Acesso em: 01 mar. 2019
OAUTH2. https://oauth.net/2/. Acesso em: 22 abr. 2019.
SURI, Sagar. Architect your Flutter project using BLOC pattern. https://medium.com/
flutterpub/architecting-your-flutter-project-bd04e144a8f1, 26 ago. 2018. Disponível 
em: https://medium.com/flutterpub/architecting-your-flutter-project-bd04e144a8f1. 
Acesso em: 28 fev. 2019.página
82
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 82-86, junho de 2019
Capítulo XIII - Do Papel ao Digital Implantação de Sistema de Protocolo DigitalDo Papel ao Digital Implantação de Sistema de 
Protocolo Digital
Marlos Ribeiro¹,², Renato Mendes¹, Walter Santos²
¹Núcleo de Tecnologia da Informação – Universidade Federal de Pernambuco (UFPE) Av. Prof. 
Moraes Rego, 1235 
Cidade Universitária, Recife – PE – Brasil – CEP: 50.670-901
²Centro de Informática – Universidade Federal de Pernambuco (UFPE) Av. Jornalista Aníbal 
Fernandes, s/n
Cidade Universitária – Recife – PE – Brasil – CEP: 50.740-560
{marlos.ribeiro, renato.mendes}@ufpe.br, wfs@cin.ufpe.br
Resumo
Este trabalho busca relatar a experiencia de implantação do módulo de protocolo do Sistema Integrado de Patrimônio, 
Administração e Contratos na Universidade Federal de Pernambuco. Trazendo para os atos administrativos da 
UFPE um paradigma digital, ZERO papel, 100% digital.
1. Introdução:
A tecnologia e a internet tem evoluído em uma velocidade sem precedentes, 
principalmente no seu papel social de possibilitar acesso à informação de forma 
mais global, ágil, precisa e com menor custo. Isto tem impactado a sociedade 
proporcionando a criação de soluções na gestão documental que permitem 
evitar o acúmulo de papéis dispensáveis e possibilitam o envio e recebimento de 
informações de maneira praticamente instantâneas.
Nesse contexto, a Presidência da República normatiza, via decreto [BRASIL 
2015b], o uso do meio eletrônico para a realização do processo administrativo 
no âmbito dos órgãos e das entidades da Administração Pública Federal (APF), 
autárquica e fundacional. Em ato consecutivo, os ministros da Justiça e do 
Planejamento, Orçamento e Gestão estabeleceram as diretrizes gerais obrigatórias 
na esfera dos órgãos e entidades da APF no tocante as atividades de protocolo a 
partir de 2016 [BRASIL 2015a].
Em sincronia com essas mudanças, a Universidade Federal de Pernambuco 
(UFPE) iniciou em 2012 uma série de visitas à Universidade Federal do Rio Grande 
do Norte (UFRN) para estudar os Sistemas de Gestão Integrada (SIGs) desenvolvidos 
pela Superintendência de Informática - UFRN (SINFO/UFRN). Por ser uma suite 
que congrega uma variedade de módulos que sistematizam diversas atividades 
administrativas e ser amplamente usada em diversos órgãos governamentais, a 
UFPE decidiu implanta-la na instituição. Em 2014 deu-se início ao processo de 
implantação do Sistema Integrado de Patrimônio, Administração e Contratos 
(SIPAC) na UFPE.página
83
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 82-86, junho de 2019
Capítulo XIII - Do Papel ao Digital Implantação de Sistema de Protocolo DigitalNeste trabalho focaremos na implantação do módulo de protocolo. Na 
seção 2 abordamos o método que institucionalmente a UFPE, apoiada pelas suas 
unidades de gestão de Tecnologia da Informação e Comunicação (TIC) (Pró-Reitoria 
de Comunicação, Informação e Tecnologia da Informação (PROCIT) e Núcleo de 
Tecnologia da Informação (NTI)), planejou e executou atividades para obter uma 
implantação profícua deste sistema. Na seção 3 apresentamos os resultados 
obtidos com a implantação e concluíımos na seção 4 com as lições aprendidas do 
processo e expectativas para o futuro.
2. Metodologia
A Implantação do módulo de protocolo, foi um dos projetos do Programa 
de Implantação do SIPAC. Neste programa, cada módulo era tratado como um 
projeto e seguia um plano de macro tarefas pré definido como descrito na Tabela 
1. Estes módulos foram priorizados levando-se em consideração 2 fatores: 1) 
interdependências, 2) necessidades de negócio.
Muitos atores tiveram participação no processo de implantação dos sistemas 
SIG. Neste processo temos:
• NTI - é o órgão suplementar da UFPE responsável por realizar a gestão de 
infraestrutura de software e hardware da instituição, assim como pesquisar, 
desenvolver, executar e participar de projetos em Tecnologia de Informação 
e serviços de informática;
• Escritório de Processos (EP) - é a unidade da UFPE que tem o objetivo de 
atuar como um agente de mudança na instituição, promovendo uma maior 
cooperação e integração entre as diversas áreas da Universidade;
• Consultor dos SIGs - a UFPE, além de ter a SINFO/UFRN como parceira em 
todo o processo, contratou uma empresa especializada na implantação deste 
sistema, reforçando a equipe do projeto nas atividades técnicas;
• Usuários Chave - são os servidores de referência nas áreas de negócio que 
se destacam pelo notável saber dos processos operacionais;
• Central de Serviços de TIC (CSTIC) - é a unidade do NTI focada em executar 
o atendimento e suporte aos usuários dos serviços de tecnologia corporativa 
da UFPE;
• PROCIT - é a Pró-reitoria da UFPE que direciona, articula, executa e 
monitora as políticas e ações com as unidades de comunicação, informação 
e tecnologia da informação, atuando transversalmente na UFPE.
• Usuários - todos os usuários afetados pelo sistema.página
84
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 82-86, junho de 2019
Capítulo XIII - Do Papel ao Digital Implantação de Sistema de Protocolo DigitalTabela 1: Atividades para implantação de um módulo dos SIG’s
# Atividade Descrição Envolvidos
1. Estudo inicialUm estudo inicial do módulo é executado para 
compreender seus procedimentosNTI, EP e 
Usuários 
Chave
2.Apresentação geral do 
móduloO Consultor dos SIGs apresenta uma explanação 
geral do módulo, suas funcionalidades e 
principais regrasConsultor, NTI, 
EP e Usuários 
Chave
3.Verificação de 
aderênciasLevantamento de divergências entre o 
modus operandi da UFRN e UFPE. Potenciais 
customizações são especificadasEP , NTI e 
Usuários 
Chave
4. Consulta técnicaCaso hajam possíveis customizações, é 
solicitado ao Consultor dos SIGs que guie a 
equipe nas possíveis alterações no co ́digo do 
sistemaNTI e 
Consultor
5.Preparação do 
móduloInstalação do módulo, implementações de 
customizações priorizadas e documentação dos 
processos de nego ́cio com o novo sistemaNTI e 
Consultor
6.Preparação de BD e 
AmbientesMigração de dados de sistemas legados, 
configuração dos ambientes de homologação, 
testes e treinamentoNTI e 
Consultor
7.Homologação do 
móduloUma bateria de testes é executada para validar a 
instalação e customizações do móduloUsuários 
Chave e NTI
8. Capacitação da CSTICA CSTIC é capacitada para atender as futuras 
demandas de usuários do móduloNTI, CSTIC e 
EP
9.Treinamentos dos 
usuáriosOs principais usuários do módulo são 
capacitadosNTI e Usuários
10. Preparar produçãoPreparação do ambiente de produção para 
publicação de versão final do móduloNTI
11.Comunicação e 
liberaçãoO público alvo que será afetado pelas mudanças 
a serem publicadas é notificadoPROCIT e NTI
12. Produção AssistidaAcompanhamento da equipe do projeto durante 
o período inicial de produção para efetuar um 
pronto atendimento a qualquer incidenteCSTIC e NTI
3. Resultados
O objetivo fundamental planejado para ser atingido com a implantação do 
módulo de protocolo na UFPE foi a de possibilitar, a partir de janeiro de 2019, que 
todos os atos administrativos relacionados às atividades de protocolo regulados 
pela lei [BRASIL1999] pudessem ser realizados através de sistema informatizado 
sem a necessidade de manipulação física de papéis. Além de substituir o sistema 
legado, o SIG@Processo, que atuava exclusivamente na sistematização do registro 
da tramitação dos processos.
Os processos anteriores a 2019 tramitarão em meio físico até o fim de seu 
prazo recursal legal. Pessoas que não tem acesso ao SIPAC que desejarem abrir página
85
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 82-86, junho de 2019
Capítulo XIII - Do Papel ao Digital Implantação de Sistema de Protocolo Digitalalgum processo podem fazê-lo dirigindo-se a alguma unidade administrativa da 
instituição com os documentos necessários para que os servidores possam fazer a 
devida digitalização, autenticação e registro do processo, devolvendo um número 
protocolar para acompanhamento do mesmo no portal público do SIPAC.
O lançamento oficial do módulo para toda a UFPE foi em 10 de janeiro de 
2019 [ASCOM/UFPE 2019]. Em preparação para este dia, uma série de sessões 
de treinamentos para os servidores foram realizadas em todas as unidades 
administrativas durante o último trimestre de 2018. Com esta implantação, com 
todas as customizações e ajustes organizacionais implementados, a UFPE junta-
se às primeiras instituições a adotar integralmente o Processo Administrativo 
Eletrônico, atendendo integralmente ao Decreto [BRASIL 2015b].
É importante salientar os 3 pontos mais relevantes na implantação deste 
módulo: 1) a importância do registro atualizado do organograma, lotação e 
responsabilidades dos servidores nos Sistema Integrado de Administração 
de Pessoal (SIAPE). Estes cadastros são fundamentais para que, depois de 
implantado, o módulo possibilite uma tramitação de processos precisa e eficiente; 
2) a implementação de mecanismo de assinatura digital com uso de certificado 
digital emitido no âmbito da Infraestrutura de Chaves Públicas Brasileira (ICP-
Brasil), atendendo o artigo 6o do decreto [BRASIL 2015b]; e 3) o engajamento 
da gestão executiva do responsável pelo protocolo da instituição. Dando todo 
o direcionamento legal e institucional, assim como executando com muita 
criteriosidade as validações de requisitos demandados para o módulo, que trouxe 
maior robustez às ações de implantação.
Adicionalmente, alguns dos aspectos monitorados são os ganhos de 
velocidade nos atos resolutivos registrados nos processos e a redução dos custos 
acumulados dos mesmos, assim como a redução do impacto ambiental que a 
virtualização proporcionou. Com a emissão zero de papéis, já se percebe a influência 
em diversos pontos que faziam parte da cadeia de valor do serviço de protocolo. 
Como exemplo, pode-se citar a dispensa do serviço de malote, estancamento da 
crescente necessidade de espaço físico nas unidades administrativas e no arquivo 
central. A dispensa desses itens barateiam o custo do processo.
A UFPE, nos últimos 5 anos, tinha uma média de 50 mil processos abertos 
anualmente. Para projetar a economia anual com o uso do processo digital, 
podemos tomar por exemplo a composição material de um processo físico, que 
na média possui 10 páginas. A partir dos valores de compra desses materiais, 
podemos chegar a seu custo unitário e então calcular o seu custo final (ver Tabela 
2).
Em contra partida, observa-se a oneração da plataforma computacional para 
suportar esta solução, cuja a demanda aumenta com a nova de uso que o novo 
sistema gera.página
86
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 82-86, junho de 2019
Capítulo XIII - Do Papel ao Digital Implantação de Sistema de Protocolo DigitalTabela 2: Composição do custo de um processo físico
Item Qtd. Valor unitárioValor p/ 
processo Qtd. Processos 
digitaisValor 
economizado 
Capa 1 R$ 0,50
R$ 0,95 50.000 R$ 47.500,00Etiqueta de Capa 1 R$ 0,05
Etiqueta de páginas 10 R$ 0,01
Folhas A4 10 R$ 0,03
4. Conclusão
A mudança de paradigma no processo de trabalho daqueles que tem o 
processo administrativo como ferramenta de trabalho diária foi e está sendo 
uma mudança significativa e desafiadora. Os vários riscos elencados para este 
projeto, tais como o não atingimento total dos usuários em treinamento, o registro 
desatualizado de lotação dos servidores nos órgãos específicos, curta janela de 
abertura do SIAPE para manutenção cadastral dos servidores, foram ao longo da 
execução deste projeto controlados a contento e não impactaram a implantação 
e uso do novo sistema. Deve a isso o comprometimento e alinhamento das várias 
equipes envolvidas, desde a alta administração à equipe de TI.
Elementos como um plano de comunicação abrangente para toda 
a comunidade acadêmica, execução de várias sessões de treinamentos 
descentralizadas, domínio da legislação e acompanhamento frequente da alta 
administração e engajamento da diretoria diretamente responsável pelo protocolo 
são alguns dos fatores críticos que contribuíram para o sucesso do projeto.
Espera-se que com o uso do novo módulo, a UFPE ganhe mais maturidade ao 
trabalhar com os processos digitais, possibilitando assim uma maior celeridade 
em tratar seus assuntos administrativos, com maior transparência e eficiência.
Um planejamento do mapeamento de cada assunto de processo e como 
este deve tramitar pela universidade, implementar o mecanismo de integração 
com o barramento de serviços do Processo Eletrônico Nacional (PEN) e implantar 
uma orquestração dos esforços da TI alinhados às estratégias organizacionais 
utilizando alguma abordagem de arquitetura corporativa como o The Open Group 
Architecture Framework (TOGAF) ou Framework de Arquitetura Corporativa para 
Interoperabilidade no Apoio à Governança (FACIN), são objetos de estudo para 
trabalhos futuros.
Referências
ASCOM/UFPE (2019). Liberação do Módulo Protocolo do SIPAC é antecipada para esta 
quinta-feira (10) - Agência de Notícias da UFPE - UFPE.
BRASIL(1999). Presidência da República – Casa Civil – Subchefia para Assuntos 
Jurídicos - lei no 9.784 , de 29 de janeiro de 1999.
BRASIL (2015a). Portaria Interministerial do Ministério da Justiça e do Planejamento, 
Orçamento e Gestão no 1.677, de 7 de outubro de 2015.
BRASIL (2015b). Presidência da República - Casa Civil - Subchefia para Assuntospágina
87
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 87-92, junho de 2019
Capítulo XIV - DWD12 - Um método para criação de senhas seguras e memorizáveisDWD12 - Um método para criação de senhas 
seguras e memorizáveis
Cárlisson B. T. Galdino¹, Rômulo N. de Oliveira¹, Raiela Quirino Lima¹
¹Núcleo de Tecnologia da Informação – Universidade Federal de Alagoas (UFAL)
Cidade Universitária – Recife – PE – Brasil – CEP: 50.740-560
(carlisson,romulo)@nti.ufal.br, raiela.lima@arapiraca.ufal.br
Resumo
Senhas de usuário são um elemento importante na segurança da informação, podendo comprometer a segurança de 
qualquer sistema. Este artigo apresenta o método DWD12 para geração de senhas seguras e memorizáveis. Baseado 
no Diceware, o método DWD12 foi desenvolvido para aprimorá-lo e vem sendo aplicado internamente no Núcleo de 
Tecnologia da Informação (NTI) do Campus Arapiraca da Universidade Federal de Alagoas (UFAL).
Palavras-chave: Diceware, senhas, geração de senhas, segurança, passphrases.
1. Introdução:
Com o crescimento da tecnologia, é muito comum a necessidade de acesso 
a espaços virtuais restritos, tais como contas de e-mail pessoal, redes sociais, 
ambientes empresariais e lojas virtuais. A principal forma de restringir este acesso 
tem sido com uso de senhas. A senha frequentemente abre acessos em sistemas 
que, caso sejam acessados por terceiros, podem trazer consequências sérias, 
que vão desde a perda de informações importantes ou particulares de usuários a 
fraudes e sabotagens. 
Em 2012 um cluster com 25 placas de vídeo já podia testar 350 bilhões de 
senhas por segundo [Reisinger 2012]. Segundo [Burnett 2015], em 2014 senhas 
como “password” e “123456” ainda estavam no topo das senhas mais utilizadas. 
Fica claro que muitas pessoas não pensam seriamente sobre a importância de se 
construir boas senhas e sobre as consequências quando não o fazemos. Senhas 
fracas podem ser facilmente memorizadas, mas também podem ser rapidamente 
descobertas. Senhas com caracteres aleatórios, mesclando letras maiúsculas e 
minúsculas, números e caracteres especiais, têm uma alta confiabilidade, mas 
são facilmente esquecidas pelo usuário. Aqui está o problema: como construir 
senhas realmente boas e seguras, com poucas chances de serem esquecidas? O 
método apresentado por este artigo é proposto como solução para tal, permitindo 
a criação de senhas ao mesmo tempo seguras e fáceis de memorizar. 
Para compreender melhor o método proposto neste trabalho, lembremos 
alguns métodos que costumam ser bastante utilizados para a criação de senhas. 
Por exemplo: página
88
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 87-92, junho de 2019
Capítulo XIV - DWD12 - Um método para criação de senhas seguras e memorizáveis• Escolha do usuário: o usuário simplesmente define a senha. Dependendo 
de quanto o usuário esteja instruído sobre os riscos e preocupações que se 
deve ter com segurança, pode resultar em senhas potencialmente fracas, 
como “senha”,  “ 12345” , palavras de dicionário, nomes de parentes e outras 
expressões igualmente fáceis de serem deduzidas. 
• Alterado pelo usuário: baseado no método anterior. O usuário aplica 
transformações na senha, aumentando sua qualidade. Assim, uma senha 
que poderia inicialmente ser a expressão “senha” pode se tornar “s3nh@” 
ou “sen(H4)” . Este método melhora a qualidade da senha, mas crackers 
podem automatizar tentativas de descoberta de senha combinando listas de 
palavras com uma lista das modificações mais comuns. 
• Mnemônicos do usuário: este método consiste em escolher uma frase e 
então selecionar só um caractere de cada palavra. Por exemplo, “A vingança 
nunca é plena, mata a alma e a envenena” se tornaria “vnpmae” . Outras 
posições podem ser utilizadas ao invés da primeira (primeira letra da segunda 
sílaba, por exemplo: “gcntmv”). Este método reduz a qualidade da senha 
por só utilizar caracteres alfabéticos, ainda mais quando se parte de frases 
conhecidas. Assim, a despeito de quanto possa parecer seguro, este método 
pode ser considerado falho. 
• Aleatório: se o objetivo é segurança máxima, a solução é usar uma 
senha grande e aleatória. Dois problemas aqui. Primeiro, obviamente 
senhas aleatórias oferecem muita dificuldade de memorização por um 
humano. Segundo, computadores não são perfeitamente aptos a gerar 
números aleatórios (esse tipo de geração aleatória por computadores é 
chamada de pseudoaleatória). Há muitos métodos para se gerar um número 
pseudoaleatório, com maior ou menor qualidade, de modo que você precisa 
saber se o método utilizado é confiável, sob risco de criar uma senha fácil de 
ser descoberta (além de difícil de ser memorizada).
Um método alternativo, de onde este trabalho buscou inspiração, é o Diceware  
[Reinhold 2019], que consiste no uso de dados comuns (de 6 lados)1, combinados 
com uma extensa lista de palavras. Há 7.776 palavras nesta lista, que tem uma 
versão oficial e outras versões alternativas (por exemplo, uma lista de palavras em 
Português). Algumas propostas de aprimoramentos ao Diceware já foram feitas, 
como em [Aversa 2019], que propõe uso de um conjunto de dados diferente, e 
[Carnut and Hora 2005], que propõe outro conjunto de palavras. 
O princípio é: sorteie palavras da lista e as junte em uma frase, de modo a 
construir uma senha longa e forte, mas fácil de lembrar. Tradicionalmente não é 
recomendado o uso de “palavras de dicionário” , mas o Diceware as usa de uma 
forma segura. Embora senhas Diceware consistam em frases que geralmente não 
fazem sentido, elas continuam fáceis de se lembrar. Não ter sentido é parte da sua 
qualidade, uma vez que frases coerentes tendem a ser mais facilmente deduzidas. 
Por exemplo, uma senha gerada a partir do método Diceware e composta por 6 
palavras é como “cinza tosco alemã espinha poria guiado” , que é muito difícil de 
ser adivinhada ou quebrada. 
1 O uso de dados físicos soluciona o problema da pseudoaleatoriedade, oferecendo aleatoriedade real.página
89
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 87-92, junho de 2019
Capítulo XIV - DWD12 - Um método para criação de senhas seguras e memorizáveis2. Método
O método DWD12, proposto neste trabalho, é oferecido como uma variante 
do Diceware, seguindo o mesmo princípio: sorteio de palavras por um processo 
aleatório que, unidas, componham uma senha segura e memorizável. Traz, porém, 
alguns aprimoramentos: 
• Dados de 12 lados. embora dados de 12 lados (conhecidos como D12) sejam 
mais difíceis de encontrar, eles permitem organizar melhor o procedimento. 
Alternativamente, dados de 12 lados podem ser trocados por cartas ou por 
combinações de dados comuns de 6 lados ou mesmo por moedas2. Esta desta 
mudança em relação ao Diceware que vem o nome DWD12 (DiceWare + D12). 
• Uso de tomos. Ao invés de uma lista única de palavras, utiliza-se várias 
listas menores de palavras, referidas como tomos. Cada tomo oferece 1.728 
palavras e são necessários alguns tomos para construir uma boa senha. 
Idealmente, 12 tomos. Isso oferece muitas palavras a mais do que o Diceware  
e é possível selecionar os tomos a partir de uma lista ampla de tomos, o que 
interfere na qualidade da senha gerada por afetar sua entropia (o que será 
tratado na Seção 3). 
• Lista secreta de palavras. Em casos especiais, como em uma organização 
que precise de senhas especialmente seguras ou gere senhas com frequência 
pode ser produzido e introduzido um tomo secreto, com palavras não 
públicas. É recomendado ter um tomo especial, mesmo que não seja 
necessariamente secreto. Um tomo em outro idioma, por exemplo, pode 
servir de tomo especial. 
• Listas de palavras temáticas. O uso de múltiplas listas menores de palavras 
torna viável que os tomos sejam temáticos. Um tomo com palavras extraídas 
de obras de Machado de Assis, por exemplo. Esta característica não constitui 
uma vantagem técnica, mas torna o DWD12 mais amigável e atrativo. 
• Rolagens com significado. Diferente de Diceware, cada rolagem de dados 
de DWD12 tem um significado: a primeira rolagem seleciona o tomo; a 
segunda seleciona a página; a terceira seleciona a seção; a quarta rolagem 
seleciona de fato a palavra. Assim, cada tomo tem 12 páginas, com 12 seções, 
com 12 palavras.
3. Resultados
Para mensurar os resultados e comparar o método DWD12 com outros, 
convém entender um pouco o conceito de entropia. Entropia é uma forma usual de 
mensurar a qualidade de uma senha, tendo relação direta com o quão complexo é 
o caminho para deduzí-la. Um alto número de entropia significa uma senha mais 
difícil de ser quebrada. Senhas de baixa entropia podem parecer fortes, mas são 
fáceis de serem descobertas por um atacante. 
2 Neste caso, é necessário 1 dado de 6 e um sorteador binário, que pode ser o par ou ímpar de um segundo 
dado ou uma moeda. O resultado binário é escolhido para somar 6 ou não ao resultado do primeiro dadopágina
90
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 87-92, junho de 2019
Capítulo XIV - DWD12 - Um método para criação de senhas seguras e memorizáveisSegundo [Weber 2013] entropia de uma senha é o número de senhas 
possíveis de se obter com as regras utilizadas, mas expresso na forma logarítmica 
de base 2 (a unidade utilizada é chamada de “bit de entropia”). Por exemplo, uma 
senha de 3 dígitos oferece 1.000 possibilidades, de modo que sua entropia seria 
l2(1000)=9,966 . 
Quando é dito que uma senha segura precisa atingir 128 bits de entropia, 
significa que o número de senhas possíveis com o conjunto de caracteres utilizado 
e a quantidade de caracteres que a senha tem precisa ser de aproximadamente 
3.4×1038. Com uso de números, isso é alcançado com senhas de 39 dígitos; com 
caracteres alfanuméricos, 28 caracteres; acrescentando caracteres maiúsculos, 
23 caracteres; se incluirmos também 20 caracteres especiais possíveis, a senha 
precisa ter 21 caracteres. Se algum segmento da senha não é aleatório e, em vez 
disso, utiliza uma regra simples (número de telefone, palavras de dicionário, etc), 
a entropia cai. 
Palavras podem ser utilizadas para construir senhas grandes. É possível 
calcular entropia por caractere (usando estatísticas de sequências de caracteres 
em palavras escritas). Técnicas similares a Diceware usam uma lista pública de 
palavras e a forma mais fácil de alguém tentar quebrar esta senha é com uso da 
mesma lista. Ao reunir várias palavras em uma senha, porém, pode-se alcançar 
altos valores de entropia. O método Diceware oferece 7.776 palavras, assim uma 
senha de 1 palavra tem 12,92 bits de entropia. Com 6 palavras, porém, a entropia 
atinge 77,55 bits. Aleatoriedade é um aspecto fundamental para a entropia. Caso 
a frase utilizada como senha tenha algum significado, [Grignetti 1964] calcula 9,8 
bits de entropia por palavra, já que a palavra seguinte se torna mais previsível. 
A Tabela 1 compara DWD12 com Diceware em número de palavras, entropia e 
tamanho da senha para se alcançar 64 ou 128  bits de entropia.
Diceware 1 tomo 4 tomos 6 tomos 12 tomos
Tipo de dado d6 d12 d12 d12 d12 
Rolagens de dados 5 3 4 4 4 
Palavras possíveis 7.776 1.728 6.912 10.368 20.736 
Entropia 
aproximada12,92 10,75 12,75 13,34 14,34 
Senha de 64 bits 5 6 5 5 5 
Senha de 128 bits 10 12 10 10 9 
Tabela 1: Comparativo do Diceware com o DWD12, utilizando 1, 4, 6 e 12 tomos
Uma vez dispondo de 12 tomos, cada um composto por 1.728 palavras, sem 
repetição de palavras, foram geradas 6 senhas aleatórias, que são aqui analisadas. 
Um tomo com palavras em Inglês foi utilizado como tomo especial. 
A Tabela 2 mostra as senhas construídas com DWD12, comparando entropia 
de várias formas: a entropia da coluna DWD12 é a entropia quando o atacante 
sabe que DWD12 foi aplicado e conhece os 12 tomos utilizados em sua geração; 
entropia cega analisa a entropia quando o atacante não sabe nada sobre a senha página
91
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 87-92, junho de 2019
Capítulo XIV - DWD12 - Um método para criação de senhas seguras e memorizáveis(tentando 82 possibilidades por caractere); entropia limitada estima a entropia 
com o atacante sabendo que não foi utilizado caractere maiúsculo nem caractere 
especial (apenas caractere de espaço). 
Considerando-se que em Português (assim como em qualquer outro idioma), 
as letras não aparecem aleatoriamente: há regras. Depois de um “N” , por exemplo, 
há um número limitado de caracteres possíveis, de modo que a entropia real é 
muito menor do que a que foi utilizada como referência para caracteres alfabéticos. 
Shannon [Shannon 1951] estima em 2,62 bits a entropia por letra para palavras 
em Inglês, uma vez que a próxima letra é predizível. Este valor foi utilizado como 
referência estimada para palavras em Português e é exibido na coluna Língua da 
Tabela 2. 
Observe-se que a entropia para quando o atacante sabe que foi utilizado o 
DWD12, conhecendo os tomos utilizados, é a menor na Tabela 2 e que esta situação 
tende a não ser muito comum. Desta forma, pode-se dizer que uma senha gerada 
com entropia DWD12 satisfatória terá uma entropia real ainda maior, reforçando a 
qualidade das senhas geradas. 
O método DWD12 foi aplicado no setor de Redes do Campus Arapiraca. As 
senhas foram geradas inicialmente com 4 tomos, sendo um secreto, utilizando-se 
um dado de 12 lados. Posteriormente, foi implementado um gerador de senhas 
aleatórias em shellscript. Senhas foram geradas para serviços específicos ou para 
grupos de serviços similares. Considerando que nem todos os sistemas permitem 
senhas longas, foi desenvolvido um método alternativo, mesclando palavras 
sorteadas com alterações igualmente aleatórias. Este método alternativo foi 
utilizado em tais situações.    
 
Palavras/Caracteres Senha Entropia 
DWD12 Cega Limitada Língua
6/57 reestabelecer carreira fareja 
tabuleiros
Urucará preparou86,0 362,4 294,7 149,3 
5/35 ralar denied quiz moviam 
televisivo 71,7 222,5 180,9 91,7 
4/28 elite plotei escreve enfiado 57,4 178,0 144,8 73,4 
9/78 garimpeiro açúcar somos 
sapatos riachos
funeral inaugurando serrinha 
funcionar129,0 495,9 403,3 204,4 
10/80 xale explosão atender flato 
tempinho 
cantaremos merino arquipélago 
marcada
exige143,4 508,6 413,6 209,6 
2/15 longos fandango 28,7 95,4 77,5 39,3 
Tabela 2: Estimativa de entropia para senhas de exemplopágina
92
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 87-92, junho de 2019
Capítulo XIV - DWD12 - Um método para criação de senhas seguras e memorizáveis4. Conclusão
DWD12 tem potencial de produzir senhas seguras e memorizáveis. 
Utilizando-se tomos secretos (ou tomos especiais, com palavras não usuais), as 
senhas se tornam ainda mais difíceis de serem quebradas. Quanto mais tomos 
forem construídos e publicados, mais difícil será quebrar senhas, mesmo que o 
usuário utilize apenas 12 tomos, isso porque o atacante não saberá quais 12 foram 
utilizados. 
Este método é bastante útil no contexto de segurança de redes e sistemas 
computacionais e pode ser utilizado para proteger projetos de pesquisa, ativos de 
redes, emails pessoais, etc. Já está em uso na Universidade Federal de Alagoas. 
Futuramente pode ser implementado um gerador de senhas que utilize o 
método DWD12, para ser incorporado a sistemas da universidade e utilizado nos 
momentos em que uma senha aleatória precise ser gerada para o usuário. 
Referências
QUOTE Aversa, D. (2019). How to generate passphrases with an rpg dice set. https://
www.davideaversa.it/2016/05/generate-passphrase-rpg-dice-set/. Acesso em abril de 
2019. 
Burnett, M. (2015). Is 123456 really the most common password? http://tinyurl.com/
burnett-15. Acesso em fevereiro de 2019. 
Carnut, M. A. and Hora, E. C. (2005). Improving the diceware memorable passphrase 
generation system. Proceedings of the 7th Symposium on Security in Informatics. 
Grignetti, M. C. (1964). A note on the entropy of words in printed english. Information 
And Control 7, 7(3):304 – 306. Editora Elsevier. 
Reinhold, A. G. (2019). The diceware passphrase home page. http://world.std.
com/~reinhold/diceware.html. Acesso em fevereiro de 2019. 
Reisinger, D. (2012). No password is safe from this new 25-gpu computer cluster. http://
tinyurl.com/reisinger-12. Acesso em fevereiro de 2019. 
Shannon, C. E. (1951). Prediction and entropy of printed english. The Bell System 
Technical Journal, 30:50 – 64. 
Weber, J. (2013). Password strength/entropy: Characters vs. words. . Acesso em abril 
de 2019.página
93
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 93-98, junho de 2019
Capítulo XV - Entregando recursos para aplicações multiplataforma com a API UFRGSEntregando recursos para aplicações 
multiplataforma com a API UFRGS
Abel Corrêa Dias, Felipe Ávila dos Santos, Thiago Stein Motta
Centro de Processamento de Dados
Universidade Federal do Rio Grande do Sul (UFRGS)
{abel,felipe.santos,thiago}@cpd.ufrgs.br
Abstract
The popularization of mobile devices motivated developers to rethink the way applications are designed, taking into 
consideration a separation of the duties of the frontend and backend layers so that they can evolve independently. 
This paper presents the UFRGS API, a service to provide HTTP easy-to-use endpoints to be consumed by applications 
of the Federal University of Rio Grande do Sul (UFRGS).
Resumo
A popularização dos dispositivos móveis motivou os desenvolvedores a repensar a maneira com que as aplicações 
são projetadas, levando em consideração uma separação de deveres das camadas de frontend e backend de modo que 
elas possam evoluir independentemente. Este artigo apresenta a API UFRGS, um serviço que provê endpoints HTTP 
de fácil utilização para serem consumidos pelos aplicativos da Universidade Federal do Rio Grande do Sul (UFRGS).
1. Introdução
Os dispositivos móveis estão cada vez mais presentes no cotidiano das 
pessoas e o avanço das tecnologias envolvidas é fundamental para tornar o 
acesso ubíquo à informação uma realidade. O desenvolvimento de aplicações 
multiplataforma traz alguns desafios de infra-estrutura, escalabilidade, segurança 
e comunicação. Aplicações em diferentes plataformas deverão ter acesso a 
recursos através de chamadas em APIs (application programming interface).
Dentre os trabalhos relacionados, o TAO é uma API para o modelo de dados 
sociais baseado em grafo, capaz de processar bilhões de operações de escrita por 
segundo e provendo acesso a petabytes de dados [Bronson et al. 2013]. Outro 
exemplo mais recente é o webservice InterSensor [Chaturvedi e Kolbe, 2019], 
voltado para aplicações de smart cities, visa estabelecer interoperabilidade entre 
diferentes sensores, através de uma API que reúne dados de simulações, bancos 
de dados, arquivos e plataformas IoT . Nesse artigo apresentamos a API UFRGS, que 
visa disponibilizar serviços na web  para utilização em aplicações multiplataforma.
Este artigo está organizado conforme segue: a Seção 2 apresenta os conceitos 
utilizados no desenvolvimento da API UFRGS; a Seção 3 relata o desenvolvimento 
da API UFRGS, sua aplicação, utilização atual na universidade; a Seção 4 discute 
sobre os resultados e aponta novas direções para o desenvolvimento de novas 
funcionalidades.página
94
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 93-98, junho de 2019
Capítulo XV - Entregando recursos para aplicações multiplataforma com a API UFRGS2. Métodos Utilizados
Nessa seção serão apresentados conceitos introdutórios sobre o framework  
para autorização OAuth 2.0 e o modelo de referência para o controle de acesso 
baseado em papeis (RBAC, role based access control), necessários para compreensão 
do trabalho.
2.1. Framework Oauth 2.0
A especificação do OAuth 2.0 [Hardt 2012] permite que aplicações de terceiros, 
aqui chamadas de clientes, tenham acesso limitado a serviços HTTP em nome do 
próprio usuário, doravante dono do recurso. O OAuth acrescenta uma camada de 
autorização e separa o papel do cliente e do dono do recurso. O cliente obtém um 
token de acesso que concede acesso limitado aos recursos.
A Figura 1-a ilustra um fluxo abstrato para obtenção de acesso a recursos. O 
servidor de autorização emite o token de acesso e verifica a identidade e o escopo 
do cliente que está acessando determinado recurso e o servidor de recurso provê, 
através de endpoints, os recursos para as aplicações.
Figura 1. a) Abstração do fluxo de autorização do Oauth
b) Hierarquia do controle de acessos no RBAC
2.2. Role Based Access Control (RBAC)
O role based access control (RBAC) é um modelo administração de segurança, 
proposto pelo National Institute of Standards and Technology (NIST) [Sandhu et 
al. 2000] que se propõe a ser padronizável, escalável, independente de sistema e 
com projeto lógico. O modelo RBAC é organizado em quatro níveis cumulativos, 
onde cada novo nível inclui os requerimentos do nível anterior. São eles: flat, 
hierarchical, constrained e symmetric RBAC. Nesse trabalho será abordado o 
General Hierarchical RBAC, que é o modelo no qual a nossa implementação está 
baseada.página
95
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 93-98, junho de 2019
Capítulo XV - Entregando recursos para aplicações multiplataforma com a API UFRGSNo modelo flat RBAC, os usuários são atribuídos a papeis, e estes estão 
vinculados às permissões de acesso. O RBAC requer que essas relações entre 
usuários e papeis seja do tipo muitos-para-muitos. A Figura 1-b ilustra o modelo 
hierarchical, que adiciona o suporte a estruturação hierárquica dos papeis e 
permissões ao modelo flat.
O objetivo da implementação do RBAC como mecanismo de controle de 
acesso em conjunto com o framework OAuth 2.0 visa simplificar a administração 
de políticas de acesso entre usuários e as aplicações clientes.
3. API UFRGS
Uma das motivações para o desenvolvimento da API UFRGS é a necessidade 
de aprimorar os processos de trabalho da Universidade frente a novos desafios, 
como a popularização dos dispositivos móveis, integração com dispositivos 
eletrônicos, provimento de dados de maneira segura, transparência da informação, 
entre outros.
3.1. Desenvolvimento da API
A API UFRGS foi projetada no estilo arquitetural REST (REpresentational 
State Transfer) [Fielding 2000] para guiar o projeto de aplicações para a Internet 
moderna, e implementada utilizando o framework web Yii 2.0 [LLC 2019] com 
a linguagem de script PHP . Atualmente, REST também é usado para descrever 
qualquer interface web que utiliza JSON, XML ou HTML e HTTP para troca de 
mensagens entre aplicações.
Para assegurar políticas de acesso diferenciadas a cada perfil de usuários, a 
API UFRGS utiliza o modelo RBAC integrado ao framework OAuth 2.0 relacionando 
suas entidades para gerar um modelo que permita gerenciar os níveis de acesso 
dos papeis do usuário e dos clientes.
A Figura 2 ilustra o modelo resultante dessa integração. O token de acesso 
possui nível de permissão representado pelos seus escopos. Uma solicitação de 
token é feita através de um cliente, e este deve estar cadastrado para receber 
pedidos para os escopos na solicitação. O escopo, por sua vez, está associado a 
uma permissão de usuário. Se um usuário está realizando a solicitação de token, 
então o escopo da solicitação deve estar dentro das permissões do seu papel.página
96
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 93-98, junho de 2019
Capítulo XV - Entregando recursos para aplicações multiplataforma com a API UFRGS
Figura 2. Modelo de Integração entre o framework Oauth e RBAC
3.2. Histórico e utilização atual da API
A API começou a ser desenvolvida em setembro de 2015, mas teve seu 
desenvolvimento acelerado em novembro de 2015, porque as licenças concorrentes 
do Sistema Autônomo de Bibliotecas da UFRGS (SABi) atingiram o pico máximo de 
utilização em virtude da ampliação do uso de dispositivos móveis pelos alunos.
Como não havia uma API que possibilitasse a comunicação entre os aplicativos 
e a base de dados da Universidade, a primeira versão do primeiro aplicativo da 
UFRGS    [Wink et al. 2017], que já contava com a possibilidade de fazer renovação 
automática de livros na biblioteca, realizava a comunicação com os servidores 
da Universidade através de interpretação de chamadas HTTP de URLs. Assim, a 
renovação automática era feita por um acesso à URL do SABi, simulando o acesso 
de um usuário, o que ocupava uma licença. Com o aumento do número de usuários 
do aplicativo, as licenças eram todas ocupadas e o servidor começava a retornar 
erros nas requisições feitas via web, o que era um grande contratempo para as 
bibliotecas da Universidade.
Isso enfatizou a necessidade de uma API para prover tais dados e, rapidamente, 
foi providenciada uma versão estável da API, apenas com as chamadas necessárias 
para a renovação automática de livros no SABi, e esta foi publicada em maio 
de 2016, juntamente com versões atualizadas dos aplicativos, agora fazendo 
requisições à API.página
97
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 93-98, junho de 2019
Capítulo XV - Entregando recursos para aplicações multiplataforma com a API UFRGS
(a)
 (b)
 (c)
 (d)
(e)
Figura 3. Clientes da API: a) Livros da biblioteca - UFRGS Mobile, b) Nenhum 
empréstimo de livro – UFRGS Mobile, c) Tíquetes do RU – UFRGS Mobile, d) Tela de 
login do app do Inventário, e) Frontend web da aplicação Brique UFRGS
A Figura 3 apresenta algumas das aplicações cliente da API UFRGS. A maioria 
das aplicações são para dispositivos móveis, contudo a API também provê dados 
para frontend web.
A aplicação UFRGS Mobile é uma das mais antigas e que possui mais recursos, 
desde compartilhamento de notícias e cardápio dos restaurantes universitários e, 
a partir da integração com a API, acessa os empréstimos de livros dos alunos, bem 
como seus tíquetes do restaurante universitário. A aplicação Inventário é utilizada 
para o levantamento anual dos bens da universidade. A aplicação ‘’Brique UFRGS’’ 
é uma das mais recentes e ainda está em desenvolvimento, possibilita o anúncio 
de bens que estão ociosos nos departamentos. Assim, os interessados poderão 
disponibilizar os bens sob sua responsabilidade e solicitar bens neste mural. A API 
UFRGS implementa as regras de negócio das aplicações, que serão disponibilizadas 
nas plataformas web, mobile (iOS e Android) ou dispositivos de IoT .página
98
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 93-98, junho de 2019
Capítulo XV - Entregando recursos para aplicações multiplataforma com a API UFRGS4. Considerações finais e trabalhos futuros
A criação da API UFRGS permitiu aprimorar os métodos de trabalho, separando 
os deveres dos desenvolvedores de backend e frontend. Também facilitou o reuso 
de código em diferentes aplicações através do compartilhamento de endpoints  
HTTP .
O processo de desenvolvimento de software melhorou bastante com a 
inclusão de testes de API para evitar que possíveis alterações nos endpoints  
acarretem em problemas nas aplicações já existentes. Deve-se agora trabalhar a 
cultura interna e treinamento para que a API seja amplamente utilizada nas novas 
aplicações e na migração de sistemas legados.
Dentre as aplicações em desenvolvimento estão o acompanhamento 
de frequências dos alunos em aula e para a jornada de trabalho do servidor. 
Futuramente o acesso a API UFRGS deverá ser ampliado, permitindo que a 
comunidade acadêmica desenvolva aplicações obtendo acesso através do 
framework OAuth 2.0.
Referências
Bronson,    N.,    Amsden,    Z.,    Cabrera,    G.,    Chakka,    P .,    Dimov,    P .,    Ding,    H.,    Ferris,    
J.,    Giardullo,    A.,    Kulkarni,    S.,    Li,    H.,    Marchukov,    M.,    Petrov,    D.,    Puzar,   L.,   
Song,   Y.   J.,   and Venkataramani,   V.   (2013). Tao: Facebook’s distributed data store 
for the   social graph. In Proceedings of the USENIX Annual Technical Conference 
2013. USENIX. https://research.fb.com/publications/tao-facebooks-distributed-data-
store-for-the-social-graph-2/
Chaturvedi, K. and Kolbe, T . H. (2019).  Towards establishing cross-platform 
interoperability forsensors in smart cities. Sensors, 19(3):562.
Fielding, R. T . (2000). Architectural Styles and the Design of Network-based Software 
Architectures. PhD thesis, University of California, Irvine. AAI9980887.
Hardt,  D.  (2012).   The oauth 2.0 authorization framework. RFC 6749,  RFC Editor. 
http://www.rfc-editor.org/rfc/rfc6749.txt
LLC, Y. S. (2019). Guia definitivo para yii 2.0. https://www.yiiframework.com/doc/
guide/2.0/pt-br
Sandhu, R. S., Ferraiolo, D. F., and Kuhn, D. R. (2000).  The NIST model for role-based 
access control:  towards a unified standard.  InFifth ACM Workshop on Role-Based 
Access Control,RBAC 2000, Berlin, Germany, July 26-27, 2000, pages 47–63.
Wink, A. S., Mota, T . L., and Motta, T . S. (2017).  A criação de uma equipe de 
desenvolvimento de aplicativos para dispositivos móveis na ufrgs.  InX WTICIFES 
2017, Gramado-RS. http://hdl.handle.net/10183/142234página
99
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 99-104, junho de 2019
Capítulo XVI - Ferramenta para facilitar a comunicação do Plano de Desenvolvimento InstitucionalFerramenta para facilitar a comunicação do 
Plano de Desenvolvimento Institucional
Ana Carla Macedo da Silva¹, Diogo Benassuly¹
¹PROPLAN – Universidade Federal do Pará (UFPA)
Belém – Pará– Brasil
{acms, benassuly}@ufpa.br
Abstract
O planejamento estratégico tem-se tornado de fundamental importância para as organizações públicas, incluindo as 
instituições de ensino superior, as quais devem apresentar seus planos de desenvolvimento institucional ao Governo 
Federal. Visando comunicar o seu conteúdo a todas as unidades da Instituição, foi projetada e implementada uma 
ferramenta para o registro do plano, o lançamento dos resultados e o acompanhamento por toda sua vigência. Para 
validar a ferramenta, foi realizado um estudo de caso na Universidade Federal do Pará.
Palavras-chave. Sistemas de informação, Plano de Desenvolvimento Institucional
1. Introdução
O planejamento estratégico é fundamental para as organizações públicas, a 
fim de que os recursos públicos sejam aplicados de forma sustentável, atendendo 
as necessidades da sociedade. Neste sentido, o Decreto no. 5.773, de 09/05/2006, 
estabelecido pelo Governo Federal, determina que, as Instituições de Ensino 
Superior (IESs) do sistema federal de ensino, ao requererem o credenciamento 
de seus cursos de graduação e sequenciais, apresentem o seu Plano de 
Desenvolvimento Institucional (PDI), cujo roteiro deve obedecer ao mesmo 
dispositivo legal.
Uma metodologia que vem sendo utilizada com sucesso nas organizações 
públicas para implementação do PDI é o Balanced Scorecard (BSC) que visa integrar 
objetivos estratégicos, metas, indicadores e iniciativas estratégicas para o alcance 
e cumprimento da visão e da missão institucional, respectivamente (UFPA, 2018). 
Ocorre que, após a elaboração do PDI, é importante divulgá-lo na Instituição, 
determinando a contribuição de cada unidade para alcançar os objetivos definidos.
Desta forma, foi desenvolvida uma ferramenta para facilitar a comunicação 
do PDI às unidades operacionais, a fim de que sejam capazes de determinar a 
quais objetivos estratégicos sua missão está vinculada. Este artigo está divido 
em: Métodos, no qual é apresentada a metodologia utilizada para implementar 
a ferramenta; Resultados, em que é descrito um estudo de caso nas unidades da 
Universidade Federal do Pará e Conclusão, em que são analisadas as vantagens e 
desvantagens da abordagem da ferramenta, assim como trabalhos futuros.página
100
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 99-104, junho de 2019
Capítulo XVI - Ferramenta para facilitar a comunicação do Plano de Desenvolvimento Institucional2. Métodos 
Os requisitos para o sistema foram definidos em reunião com o apoio técnico 
e, de pessoas envolvidas tanto na elaboração quanto na avaliação do plano de 
desenvolvimento institucional (PDI). A proposta foi, ao invés de, registrar apenas 
os dados do PDI, disponibilizar o cadastro e acompanhamento da execução dos 
Planos de Desenvolvimento das Unidades (PDUs) para um período de quatro anos, 
isto é, um período menor do que o de validade do plano institucional. 
Assim, ficou estabelecido que o sistema permitiria às unidades selecionarem 
os objetivos estratégicos para o seu PDU, definir indicadores ou escolhê-los a 
partir de uma cesta de indicadores, constantes do PDI ou não; definir as metas 
destes indicadores; vincular indicadores aos objetivos, e, posteriormente, lançar 
os resultados dos indicadores com uma análise crítica e a situação das iniciativas. 
As unidades responsáveis por indicadores do PDI devem acatar a meta definida no 
mesmo. 
A partir dos requisitos, duas ferramentas foram analisadas. Enquanto o 
ForPDI (SANT’ANA et al, 2017) implementa o PDI por meio de projetos e indicadores 
numéricos; o sistema requeria espelhar a contribuição das subunidades e registro 
da análise crítica dos resultados semestralmente. Já o GEPLANES (2016) trabalha 
com os indicadores pactuados para um ano de exercício, enquanto um dos 
requisitos era controlar o prazo de validade do PDU (4 anos) e o do PDI (9 anos).
O diagrama de processos da Figura 1, implementado na ferramenta Bizagi 
(2018), baseada em BPMN (Business Process Management Notation), que é o 
padrão criado pelo Object Management Group, útil para apresentar um modelo 
para públicos-alvo (BPM-CBOK, 2013), descreve melhor o fluxo de informações do 
sistema.página
101
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 99-104, junho de 2019
Capítulo XVI - Ferramenta para facilitar a comunicação do Plano de Desenvolvimento Institucional
Figura 1. Processo de registro e lançamento do PDU
3. Resultados
O estudo de caso foi realizado na Universidade Federal do Pará, em que 
o módulo do PDU foi disponibilizado no sistema SisRAA (Sistema de Registro 
de Atividades Anuais), sistema projetado e desenvolvido pela Diretoria de 
Informações Institucionais (DINFI), que registra e consolida as informações por 
meio de formulários Web. 
A Figura 2 exibe o Painel Tático do documento (plano) selecionado, com as 
perspectivas e os objetivos estratégicos do PDI, uma vez que o perfil da unidade 
em questão possui indicadores do PDI de sua responsabilidade. No entanto, 
a unidade pode adicionar (botão Adicionar novo objetivo estratégico) novos 
objetivos, constantes do seu Plano, mas pertencentes ao plano de desenvolvimento 
institucional.página
102
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 99-104, junho de 2019
Capítulo XVI - Ferramenta para facilitar a comunicação do Plano de Desenvolvimento Institucional
Figura 2. Painel Tático
Após adicionar um objetivo, ao optar por vinculá-lo a um indicador, é 
apresentada uma lista com todos os indicadores (Figura 3), que ainda não possuem 
vínculo, a priori são exibidos somente os relacionados à cesta de indicadores 
(pré-definidos), caso haja necessidade de cadastrar um indicador específico da 
unidade, utiliza-se a funcionalidade “Incluir novo indicador” .
Figura 3. Lista de indicadores
No cadastro de novo indicador (Figura 4-a), é necessário informar o nome 
do indicador, a fórmula de cálculo e sinalizar a sua interpretação, ou seja, se 
possui uma perspectiva crescente ou decrescente. Posteriormente, as metas 
são definidas (Figura 4-b) para os anos de vigência do documento e o tipo (valor 
absoluto ou porcentagem). Ressalta-se que não é permitida a alteração de metas 
de indicadores do plano de desenvolvimento institucional.página
103
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 99-104, junho de 2019
Capítulo XVI - Ferramenta para facilitar a comunicação do Plano de Desenvolvimento Institucional
Figura 4. Formulário para cadastrar indicador/meta
É imprescindível que cada indicador possua iniciativas (ações) por meio das 
quais se pretende atingir as metas definidas. O formulário requer informar o nome 
da iniciativa e ano de início, posteriormente deve-se vinculá-la ao indicador. 
O lançamento dos resultados é realizado em um formulário (Figura 5), no 
qual se insere o resultado alcançado e a análise crítica. Neste mesmo formulário, 
são lançados a situação (andamento normal ou com atraso) da iniciativa e os 
fatores que influenciaram tal situação. O período de lançamento é semestral e 
é especificado no sistema, ao se definir um calendário de lançamento para cada 
semestre.
Figura 5. Formulário de lançamento do resultado
Foi implementado também o Painel de Medição (Figura 6), que exibe dados 
do indicador, assim como sua série histórica anual, a análise crítica e a situação 
das iniciativas e seus fatores determinantes. Também estão disponíveis relatórios 
de acompanhamento tanto do cadastro quanto do lançamento de resultados. página
104
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 99-104, junho de 2019
Capítulo XVI - Ferramenta para facilitar a comunicação do Plano de Desenvolvimento Institucional
Figura 6. Painel de medições
4. Conclusão
O planejamento estratégico é de fundamental importância para as 
organizações públicas que almejam alcançar objetivos institucionais, considerando 
que as IESs devem apresentar seus planos de desenvolvimento institucional ao 
Governo Federal, foi implementada uma ferramenta para o registro e lançamento 
dos resultados deste plano, com a finalidade de facilitar a comunicação do mesmo 
no interior de toda a Instituição, visando disseminar a todas as unidades o seu 
conteúdo.
 Na UFPA, o sistema tem propiciado: a validação e discussão do mapa tático 
das unidades operacionais, uma vez que o PDU deve estar em conformidade com 
o Guia de Elaboração do PDU para ser lançado no sistema; autoconhecimento 
da subunidade a respeito do seus produtos; revisão das práticas para atingir 
os resultados avaliados por meio da análise crítica e identificação dos fatores 
que contribuem para os resultados positivos ou negativos alcançados em cada 
iniciativa/ação para atingir a meta do indicador; a visualização do comportamento 
do indicador por meio do gráfico da série histórica, auxiliando tomadas de 
decisão. 60% das unidades já cadastraram o seu PDU. Brevemente, as tarefas de 
repactuação e cancelamento/edição de indicadores serão implementadas, já que 
será uma demanda resultante das Reuniões de Avaliação Tática e Estratégica, assim 
como relatórios de suporte à avaliação que estão sendo fornecidos inicialmente 
por meio de consultas diretas ao banco de dados pela equipe técnica.
Referências
Bizagi (2018). Referência rápida de BPMN, disponível em www.bizagi.com
BPM CBOK. Guia para o Gerenciamento de Processos de Negócio – Corpo Comum de 
Conhecimento. ABPMP CBOK V3.0, 2013.
GEPLANES Entreprise, Manual de Operação, versão 3.0, Jordanna Malena da Silva... et 
al, Revisão 2016.
Tomás Dias Sant’Ana...[et al]. PLANO DE DESENVOLVIMENTO INSTITUCIONAL (PDI) - 
Um guia de conhecimentos para as Instituições Federais de Ensino, 2017
UNIVERSIDADE FEDERAL DO PARÁ (UFPA), Guia para Elaboração Gestão Avaliação do 
Plano de Desenvolvimento da Unidade (PDU), versão 1.2, 2018.página
105
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 105-109, junho de 2019
Capítulo XVII - Funcionalidade para logoff da autenticação do Firewall da Palo Alto na UFPIFuncionalidade para logoff da autenticação do 
Firewall da Palo Alto na UFPI
Filipe S. Viana¹, Ênio R. Viana¹, Diego F. M. Oliveira¹
¹Superintendência de Tecnologia da Informação – STI – Universidade Federal do Piauí (UFPI)
Caixa Postal 64049-550 – Teresina – PI – Brazil
{filipesoaresviana,enio,diego}@ufpi.edu.br
Resumo
Os firewalls da nova geração (Next Generation Firewalls) são ferramentas capazes de criar uma camada a mais 
de segurança no acesso à Internet, permitindo também um gerenciamento do uso da rede privada por meio de 
logins individualizados. Este artigo apresenta a estratégia adotada para implementar a funcionalidade que executa 
a desconexão de um usuário conectado ao Captive Portal do firewall utilizado na UFPI, funcionalidade essa 
não fornecida diretamente pela solução, mas disponibilizada indiretamente via API do sistema de firewall. Esta 
solução resultou em maior garantia na autenticidade dos usuários na rede interna da instituição, mantendo um 
rastreamento mais fidedigno de seus acessos.
1. Introdução:
A Universidade Federal do Piauí (UFPI) utiliza como solução de segurança da 
sua rede de computadores, o firewall [Kurose e Ross 2013] fornecido pela empresa 
Palo Alto Networks (PA). Uma de suas funcionalidades possibilita a necessidade de 
autenticação de um usuário na rede para que, após identificado, possa ter acesso 
à Internet. Esta autenticação é necessária em conformidade com a legislação 
vigente, notadamente o Marco Civil da Internet [BRASIL, Lei Nº 12.965].
Após autenticado, o dispositivo se mantém ativo por um determinado tempo 
em sessão, identificando o usuário responsável por qualquer atividade na rede, 
sem a possibilidade de desconexão do mesmo, visto que a solução da PA não 
disponibiliza nativamente esta opção. A instituição possui vários computadores 
públicos, o que torna necessário uma opção de logoff, visto que, ao se autenticar, 
o usuário manter-se-ia em sessão e seria identificado erroneamente pelos acessos 
realizados por terceiros.
Em decorrência da necessidade de uma funcionalidade para desconectar 
a sessão de um usuário, foi desenvolvida uma solução que realiza esta tarefa se 
utilizando de uma Application Programming Interface (API) [Woods et al. 2011] 
fornecida pelo dispositivo da PA que, a partir dele, são executados comandos 
reconhecidos pelo aparelho sendo, um deles, para realizar a remoção da sessão do 
usuário . Para isso, foram necessários alguns passos para garantir a sua execução 
com exatidão e com segurança, descritos neste artigo.página
106
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 105-109, junho de 2019
Capítulo XVII - Funcionalidade para logoff da autenticação do Firewall da Palo Alto na UFPI2. Metodologia Adotada
2.1. Motivo
A motivação para a escolha da solução de autenticação, via firewall, abordada 
nesse artigo, advém de um conjunto de características inerente a infraestrutura de 
rede de computadores da instituição. Em suma, foram ponderados diversos fatores 
como: a dimensão da rede (número de ativos de rede, de Vlans e de usuários), 
sua heterogeneidade (Wi-fi, fibra óptica, cabeamento metálico, diversidade de 
fabricantes), a ausência de um serviço de domínio de rede e a escassez de recursos 
humanos. Esses tópicos foram considerados e analisados na decisão de adquirir 
um appliance que resolveria de maneira centralizada as demandas de segurança, 
robusto o suficiente para não se tornar um gargalo, mas sim, otimizar as taxas de 
transmissão. Então, a autenticação via Captive Portal [Palo Alto 2018] se mostrou, 
a curto prazo, a solução mais viável.
2.2. Dados Necessários
A API disponibilizada pelo sistema do firewall da PA, chamada PAN-OS, 
inicialmente requisita uma chave que é gerada por um usuário com a devida 
permissão [Palo Alto 2018].  Preferencialmente, é criado um usuário com 
permissões limitadas de acesso que realiza a geração da chave de acesso à API, 
o qual é configurado com uma regra de perfil com as permissões necessárias 
para a geração da chave, objetivando assim uma maior segurança. Neste caso 
foi habilitado o “Operational Requests” , uma regra específica para uso da API do 
sistema.
Outro item utilizado para a execução do comando é o endereço IP (Internet 
Protocol) do dispositivo na rede interna. O valor foi coletado utilizando a linguagem 
de programação javascript [Flanagan 2012], com o código executado no dispositivo 
do ator que deseja se autenticar. A coleta do endereço pelo lado do cliente se deve 
pela existência dos campi fora de sede que são acessados via VPN (Virtual Private 
Network), o que torna inviável a coleta do IP pelo servidor, visto que, nesses casos, 
seria capturado um valor de endereço público do campus, como mostrado na 
Figura 1. página
107
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 105-109, junho de 2019
Capítulo XVII - Funcionalidade para logoff da autenticação do Firewall da Palo Alto na UFPI
Figura 1. Fluxograma de logoff do usuário
2.3. Conhecimentos Essenciais
Além de conhecimentos em rede de computadores implicitamente 
necessários, outro conhecimento necessário para implementar a solução proposta 
neste artigo é a programação de computadores, usada para acessar a API do 
firewall. API é uma interface de comunicação onde serviços disponibilizados por 
terceiros podem ser acessados e utilizados, podendo ser desenvolvida de várias 
formas e para diversas finalidades. 
Uma das funções da API fornecida pelo sistema PAN-OS é o recebimento de 
chamadas para executar linhas de comandos no sistema do firewall, comando este 
que pode ser a remoção da sessão do usuário autenticado na rede. Utilizamos a 
linguagem de programação PHP [Tatroe et al. 2013] para acessar a API e realizar os 
execução dos procedimentos.
2.4. Execução
Com a chave de acesso da API e o endereço IP do usuário, montaram-se duas 
URLs (Uniform Resource Location) que enviam os comandos operacionais via HTTP 
os quais, ao serem executados no sistema da PA, removem o usuário da sessão. Os 
comandos são:
• https://hostname/api/?type=op&key=key&vsys=vsys1&cmd=<clear><user-
cache><ip>ipnumber</ip></user-cache></clear>página
108
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 105-109, junho de 2019
Capítulo XVII - Funcionalidade para logoff da autenticação do Firewall da Palo Alto na UFPI• https://hostname/api/?type=op&key=key&vsys=vsys1&cmd=<clear><user-
cache-mp><ip>ipnumber</ip></user-cache-mp></clear>
Os dois comandos realizam a remoção do usuário de dois processadores 
dedicados às funções de segurança que funcionam em paralelo. Os comandos 
são chamados de Management Plane(MP) e Data Plane(DP), cada um com 
processadores, memórias e discos rígidos dedicados. O MP é usado para gestão do 
firewall da PA provendo a configuração, relatório de logs e atualização de rotas. O 
DP consiste em três tipos de processos: Security Matching Processor, Security 
Processor e Network Processing.
O primeiro comando executado é o clear user-cache responsável por remover 
cache do usuário do DP e o segundo, o clear user-cache-mp, é usado para remover 
o usuário do MP . Os dois funcionam como uma redundância, fazendo com que, 
caso a sessão seja removida somente em um dos planos, o outro a recupere após 
o próximo acesso do usuário. Considerando este fato, foi-se necessário a execução 
dos dois comandos.
Para acionar esse recurso disponibilizamos um link no site institucional da 
UFPI [UFPI 2019] que possibilita acessar a funcionalidade descrita nessa sessão. 
Simplificamos ao máximo para o usuário a sua utilização, disponibilizando somente 
um botão “Desconectar” que realiza todo o procedimento automaticamente. 
Vale ressaltar que tal funcionalidade é funcional em qualquer plataforma, tanto 
computador desktop ou dispositivos móveis, todas se utilizando do mesmo 
procedimento descrito.
3. Resultados
Sem a opção de remoção da sessão do usuário, o registro de acesso de cada 
usuário poderia ser erroneamente registrado no firewall. Por exemplo, após o uso 
de um computador público, a sessão do usuário seria mantida e acessada por um 
segundo usuário que, ao praticar atos ilícitos, poderia incriminar o primeiro de 
forma errônea, inviabilizando qualquer investigação. 
A implantação da funcionalidade apresentada neste artigo garantiu ao usuário 
a remoção de sua sessão. Mesmo com este cuidado, foi constatado que isso não 
torna qualquer situação irrefutável, visto que vários usuários além de esquecerem 
de sair da sessão ao fim de suas atividades, outros compartilham suas senhas com 
terceiros. Resultou-se então na implantação de um outro projeto que limitou a 
quantidade de sessões, funcionalidade essa possibilitada pela desconexão, visto 
que sem ela a quantidade de sessões de um usuário logo atingiria seu limite.  
Deve-se também considerar que, mesmo com o maior acesso ao servidor 
de firewall da PA devido a execução dos comandos necessários, não se constatou 
um aumento significativo de processamento que pudesse influenciar o seu 
funcionamento.página
109
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 105-109, junho de 2019
Capítulo XVII - Funcionalidade para logoff da autenticação do Firewall da Palo Alto na UFPI4. Conclusões 
A importância de um profissional em desenvolvimento no setor de 
infraestrutura se mostrou muito importante para resolução de situações como 
essa demonstrada neste artigo. Investir na capacitação em programação, mesmo 
que em um nível intermediário, pode ser considerado essencial, visto que várias 
situações específicas podem ser resolvidas de forma mais rápida e eficaz se 
utilizando desta área de atuação. 
Outro fator importante a considerar é a importância de conhecer todas as 
tecnologias fornecidas por uma solução, além da que ela se propôs a oferecer. 
Mesmo quando não se encontra uma solução direta para um problema ou 
necessidade, outras ferramentas podem, indiretamente, solucioná-las.
Referências
Kurose, J. e Ross, K. (2013), Redes de Computadores e a Internet: Uma Abordagem 
Top-Down, Pearson Universidades, 6ª edição. 
Woods, D., Jacbson, D. e Brail, G. (2011), APIs: A Strategy Guide, O’Reilly Media, 1ª 
edição. 
BRASIL, Lei nº 12.965, de 23 de abril de 2014. Estabelece princípios, garantias, direitos 
e deveres para o uso da Internet no Brasil. Disponível em : <http://www.planalto.gov.
br/ccivil_03/_ato2011-2014/2014/lei/l12965.htm>. Acesso em: 15 de março de 2019.
Palo Alto, (2018) , “PAN-OS® and Panorama™ API Guide” , Disponível em: <https://docs.
paloaltonetworks.com/pan-os/8-1/pan-os-panorama-api.html>. Acesso em: 15 de 
março de 2019.
Flanagan, D. (2012), JavaScript: O Guia Definitivo, Bookman , 6ª edição. 
Tatroe, K., MacIntyre, P . e Lerdorf, R. (2013), Programming PHP: Creating Dynamic Web 
Pages , O’Reilly Media, 3ª edição.
UFPI, (2019) , Site Institucional da Universidade Federal do Piauí, Disponível em: 
<http://ufpi.br/>. Acesso em: 15 de março de 2019.página
110
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 110-115, junho de 2019
Capítulo XVIII - Gerenciamento de Logs: Implantação do Graylog como ferramenta de centralização 
de logs de dados na Universidade Federal do AmazonasGerenciamento de Logs: Implantação do Graylog 
como ferramenta de centralização de logs de 
dados na Universidade Federal do Amazonas
Crisley P. Linhares¹ , Gerson B. da Silva¹ , João G. A. Martinez¹ , Vanderson da S. Rocha¹ , 
Marckson M. da Silva¹ , João B. L. Carneiro¹
¹Centro de Tecnologia da Informação e Comunicação – Universidade Federal do Amazonas 
(UFAM)
CEP 69.080-900 – Manaus – AM – Brasil
{crisleylinhares,gbs,jgam,vanderson,marcksonms,jcarneiro}@ufam.edu.br
Resumo
Este trabalho descreve o processo de implantação do Graylog como solução para a problemática da centralização 
dos logs gerados na rede interna da Universidade Federal do Amazonas (UFAM), dado a necessidade de melhorar 
o controle sobre estes dados, que ficavam dispersos nos ativos e aplicações instalados na rede. O uso do Graylog 
torna mais simples o processo de auditoria e a identificação de diversos eventos na rede da universidade, através da 
consolidação, análise e gerenciamento destes logs uma vez centralizados.
Palavras-chave: Graylog, logs, rede, centralização, gerenciamento.
1. Introdução
Logs são gerados a todo momento dentro de redes por diferentes hosts e 
aplicações, sendo um poderoso aliado dos profissionais de TI na identificação e 
resolução de problemas [MÜLLER 2013]. Quando estes dados são negligenciados, 
vários aspectos de segurança são afetados tais como: confidencialidade, 
integridade e disponibilidade das informações.
A pesquisa se justificou na problemática existente na UFAM, onde os logs  
não eram armazenados de forma centralizada, ocasionando dispersão destes nos 
próprios hosts que os geravam. Muitas vezes estes logs eram perdidos durante 
a reinicialização dos ativos ou aplicações, causando grande impacto sobre a 
administração da rede, que perdia sua capacidade de auditoria sobre eventuais 
incidentes ocorridos.
Partindo deste pressuposto, quando os logs estão seguros, aumenta-se as 
chances de sucesso na correlação e identificação de padrões para rever incidentes 
ocorridos na rede. Para alcançar estes objetivos, existe a recomendação de se 
estabelecer um sistema de logs centralizado e dedicado exclusivamente para a 
coleta, registro e análise de eventos [CANSIAN 2001].
Armazenar e gerenciar logs é importante do ponto de vista onde a informação 
é considerada um ativo significativo e essencial para os negócios de qualquer 
organização, como é o caso da UFAM. Portanto, estas informações devem ser página
111
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 110-115, junho de 2019
Capítulo XVIII - Gerenciamento de Logs: Implantação do Graylog como ferramenta de centralização 
de logs de dados na Universidade Federal do Amazonasgerenciadas e protegidas através de medidas adequadas em um ambiente de 
negócios interconectado. Assegurar a segurança destes dados garante, portanto, 
a continuidade dos negócios da Universidade e minimiza riscos e ameaças dentro 
da rede interna.
Diante deste contexto, este trabalho tem como objetivo geral demonstrar 
o processo de implantação do Graylog como aplicação de centralização e 
gerenciamento da massa de dados coletados na rede, através de uma interface 
gráfica intuitiva e funcionalidades que agregassem informações sobre a situação 
atual da rede, permitindo sua auditoria sempre que necessário. 
2. Métodos
Para a correta implementação do Graylog são necessárias três aplicações 
distintas que se comunicam entre si: o Graylog-Server, o Elasticsearch e o MongoDB.
O Graylog-Server recebe os logs dos recursos instalados na rede, os encaminha 
para o Elasticsearch, que armazena e indexa as entradas geradas e os reencaminha 
para o Graylog-Server, que consume estes dados através da sua interface web. 
O MongoDB, por sua vez, indexa no banco de dados as configurações internas e 
eventuais personalizações da instância instalada do Graylog.
Figura 1. Esquema representativa da arquitetura de comunicação do Graylog.
Para a implantação do Graylog foram utilizadas três máquinas virtuais 
Ubuntu 16.04, sendo um servidor para cada aplicação. 
No servidor do Elasticsearch, definiu-se a porta 9200 para a comunicação 
do cluster interno com o Graylog-Server, além de parametrizar o nome deste 
para o correto encaminhamento dos logs entre as aplicações. No MongoDB foi 
configurada a porta 27017 para a comunicação com o Graylog-Server, que por sua 
vez foi parametrizado segundo as configurações definidas nas demais aplicações.
Para cada host da rede, configurou-se o arquivo correspondente ao protocolo 
rsyslog ou syslog, um pequeno script de encaminhamento dos seus logs para o 
servidor do Graylog-Server, através da porta UDP/11001, uma vez que o Graylog página
112
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 110-115, junho de 2019
Capítulo XVIII - Gerenciamento de Logs: Implantação do Graylog como ferramenta de centralização 
de logs de dados na Universidade Federal do Amazonasutiliza um content pack1 específico desenvolvido pela Cisco, que trata o hostname  
e o transforma no endereço IP do host, facilitando o rastreamento do mesmo. 
Este procedimento foi realizado em switches, servidores, câmeras, access point, 
gerenciador de banco de dados, controladora de rede wi-fi, além de aplicações 
específicas em execução.
Para o armazenamento dos logs coletados, foi alocado um espaço em disco 
baseado em testes realizados dentro de uma amostra de hosts específicos da rede, 
de diferentes tipos e atendendo ao Marco Civil de Internet2, que estabelece o tempo 
mínimo de armazenamento de informações por 6 meses, chegando-se ao valor de 
315 GB para o servidor do Elasticsearch. Um script do logrotate é executado para 
limpeza parcial do disco sempre que seu espaço livre estiver abaixo de 20 GB.
Por fim, definiu-se um DNS para o Graylog no domínio ufam.edu.br, com a 
configuração de um proxy reverso para ocultar a porta 9000 no acesso, além da 
implementação da autenticação via LDAP para os administradores da rede da 
Universidade.
3. Resultados
Com o Graylog em funcionamento, a aplicação foi adequada à realidade da 
rede interna com a configuração de inputs3 para receber os logs gerados, streams4 
alerts5 dentro da sua interface web . 
Uma vez que os hosts estejam configurados para enviar seus logs para o 
Graylog e com sua arquitetura preparada e implantada, as mensagens começam a 
ser exibidas na interface web . A tela de buscas mostra os logs coletados e indexados 
pelo Elasticsearch dentro de um determinado espaço de tempo definido pelo 
administrador do sistema.
1 Coleções de entradas pré-construídas por fabricantes de equipamentos para facilitar a formatação das 
mensagens de log geradas por estes equipamentos.
2 Art. 15 da Lei nº 12.965/14.
3 Funcionalidade que determina para onde os logs são encaminhados. É composto por uma porta, um 
protocolo e o IP do servidor syslog.
4 Funcionalidade de encaminhamento dos logs em tempo real segundo regras definidas pelo administrador 
da aplicação.
5 Funcionalidade de configuração de alertas segundo regras definidas pelo administrador da aplicação.página
113
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 110-115, junho de 2019
Capítulo XVIII - Gerenciamento de Logs: Implantação do Graylog como ferramenta de centralização 
de logs de dados na Universidade Federal do Amazonas
Figura 2. Graylog coletando os logs de switches da rede interna da UFAM.
Alguns padrões de mensagens de log foram selecionados para gerar alertas 
por e-mail quando da ocorrência de incidentes na rede ou antecipando problemas 
a ocorrer, tais como: conflito de IP , detecção de roteadores não autorizados na 
rede, IP duplicado, loop na rede, problemas com câmeras, problema no cooler de 
switches ou problema nas portas PoE6 de switches.
Figura 3. E-mail de alerta enviado pelo Graylog sobre um loop de rede 
identificado.
Outra problemática existente e que foi contornada com a implementação 
do Graylog, era relacionada com a ausência de informações acerca de quais 
hosts geravam mais mensagens de log na rede, dado que isto poderia fornecer 
um sintoma sobre o mal funcionamento de algum recurso no momento em que 
ele começa a gerar um grande número de entradas de log, necessitando de uma 
análise da causa para este comportamento. 
6 Power Over Ethernetpágina
114
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 110-115, junho de 2019
Capítulo XVIII - Gerenciamento de Logs: Implantação do Graylog como ferramenta de centralização 
de logs de dados na Universidade Federal do Amazonas
Figura 4. Graylog listando quais recursos geraram mais registros de log em um 
dado intervalo de tempo.
4. Conclusões e Trabalhos Futuros
A conclusão da implantação do Graylog promoveu inúmeras contribuições 
à administração da rede interna da Universidade, destacando-se: a melhoria 
da visibilidade dos problemas existentes na rede, mais rapidez na busca pelos 
dados coletados, maior proatividade na identificação de sintomas que poderiam 
acarretar problemas em recursos da rede e manutenção de dados sensíveis, que 
podem ser utilizados para futuras auditorias na rede, quando necessário.
Apesar de já estar em produção há algum tempo, o Graylog segue em 
processo de melhoria contínua para aperfeiçoar o seu uso por parte da equipe 
de infraestrutura, focando principalmente na otimização e incremento de uso das 
suas funcionalidades, atendendo a uma gama cada vez maior de necessidades 
não relacionadas apenas ao âmbito de falhas na rede, mas atuando também como 
suporte a outras ferramentas de gerenciamento da rede, como o Zabbix.
Como trabalhos futuros sugeridos está o estudo de viabilidade e migração 
do Graylog e das demais aplicações de sua arquitetura para o Docker, com sua 
consequente descrição de execução, pois permitirá maior flexibilidade na 
atualização das ferramentas, com menor esforço e impacto na recuperação de 
informações sempre que a aplicação necessitar de alterações de grande alcance.
Outra linha de estudo futuro seria a configuração do Elasticsearch para o 
envio dos logs indexados diariamente para a nuvem através do Elastic Curator, 
permitindo o armazenamento destas informações por um maior período de 
tempo, otimizando o espaço de disco alocado atualmente para tal fim e ampliando 
o alcance do processo de auditoria interna sobre incidentes mais antigos da rede.página
115
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 110-115, junho de 2019
Capítulo XVIII - Gerenciamento de Logs: Implantação do Graylog como ferramenta de centralização 
de logs de dados na Universidade Federal do AmazonasReferências
Brasil. (2014) Lei nº 12.965, de 23 de abril de 2014. http://www.planalto.gov.br/
ccivil_03/_ato2011-2014/2014/lei/l12965.htm. Fevereiro de 2019. 
Cansian, A. M. (2001) Conceitos para perícia forense computacional. In VI ESCOLA 
REGIONAL DE INFORMÁTICA DA SBC. Anais. Páginas 141-156.
Graylog. (2017) “Welcome to the Graylog documentation” . http://docs.graylog.org/
en/2.2/. Fevereiro de 2019.
Müller, E. J. (2013) “Solução centralizada para logging de aplicações” . Curso de 
Especialização do Programa de Pós-Graduação em Gestão de Tecnologia da 
Informação, UFSM – Universidade Federal de Santa Maria.página
116
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 116-121, junho de 2019
Capítulo XIX - GESCON: Agilidade e transparência na gestão dos órgãos colegiados da universidadeGESCON: Agilidade e transparência na gestão dos 
órgãos colegiados da universidade
Vanderlin Amorim Palmeira Júnior¹ , Alexsandro Cardoso Carvalho¹ 
e Lidiane Cristina Silva¹
¹Superintendência de Tecnologia da Informação (STI) 
Universidade Federal de São Paulo (UNIFESP) – São Paulo – SP – Brasil
{vanderlin,lidiane.cristina}@unifesp.br, alexsandroccarv@ccarvalho.net  
Resumo
As universidades federais são pautadas no princípio da gestão democrática e, portanto, têm garantida a participação 
de seus segmentos (técnicos, docentes e discentes) em órgãos colegiados. Fazer a gestão desses órgãos colegiados 
tornou-se uma função onerosa e complexa devido a suas dimensões e particularidades. Este trabalho tem como 
objetivo apresentar a solução adotada pela Universidade Federal de São Paulo para gestão e transparência de 
seus órgãos colegiados, o GESCON. Como principais resultados tem-se a padronização na confecção das atas; a 
disponibilização em menor tempo dos conteúdos discutidos; a gestão e o controle do mandato e da frequência dos 
membros e a promoção da transparência ativa dos dados institucionais.
Palavras-chave: órgãos colegiados, transparência, controle de mandatos.
1. Introdução
As universidades federais são parte da Administração Pública Federal (ADF) 
e, mesmo gozando de autonomia administrativa, devem seguir os princípios 
gerais inscritos no artigo 37 da Constituição Federal, sendo eles os princípios da 
legalidade, da moralidade, da impessoalidade, da publicidade e da eficiência 
[Brasil 1988]. Além do mais, considerando características específicas, a mesma 
Constituição Federal definiu em seu artigo n. 206 que o ensino público deverá ser 
regido pelo princípio da gestão democrática [Brasil 1988]. 
Uma das formas pelas quais o princípio da gestão democrática consolidou-
se nas universidades foi pela criação e ampliação dos órgãos colegiados. A Lei de 
Diretrizes e Bases da Educação Nacional (LDB) definiu que a gestão democrática do 
ensino se dará por meio dos órgãos colegiados deliberativos, dos quais participarão 
os segmentos da comunidade institucional, local e regional [Brasil 1996]. 
Cada um destes órgãos colegiados demanda uma estrutura organizacional 
que abrange os aspectos da eficiência, da legalidade e da publicidade dos seus 
atos. Em sua forma mais funcional, estes órgãos possuem membros oriundos 
do corpo docente, dos técnicos administrativos, dos discentes e da comunidade 
externa. Geralmente seus membros são eleitos por mandatos diferenciados, 
sendo que discentes possuem mandato menor, e há membros que não são eleitos, 
e sim nomeados pelo reitor. Outro elemento é que muitos regimentos definem página
117
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 116-121, junho de 2019
Capítulo XIX - GESCON: Agilidade e transparência na gestão dos órgãos colegiados da universidadeque o membro do conselho ausente em um número determinado de reuniões 
pode perder seu mandato. As datas de eleições também não são homogêneas, 
assim surge a necessidade de que a instituição acompanhe a eleição, frequência e 
duração dos mandatos. 
Outro aspecto comum a estes órgãos colegiados é a necessidade de registrar 
suas discussões e deliberações que, de forma consultiva ou deliberativa, devem 
ser de conhecimento da comunidade universitária. Este aspecto da publicidade foi 
reforçado com a promulgação da Lei de Acesso à Informação. A Lei nº 12.527/2011 
criou mecanismos que possibilitam, a qualquer pessoa física ou jurídica, sem 
necessidade de apresentar motivo, o recebimento de informações públicas dos 
órgãos e entidades [Brasil [S.d.]]. 
1.1. Sobre a Universidade Federal de São Paulo
A Universidade Federal de São Paulo (UNIFESP), foi fundada em 1933, e 
seu processo de expansão foi iniciado em 2006, quando foram criados campi em 
cidades diferentes de sua sede. Hoje ela possui, além da reitoria, o Campus São 
Paulo, o Campus Zona Leste, o Campus Diadema, o Campus São José dos Campos, 
o Campus Osasco e o Campus Guarulhos. 
A questão da autonomia e descentralização administrativa não foi 
simplesmente pelos recursos financeiros, mas tornou-se uma questão de 
transferência de poder e espaços de decisões. Além de estarem geograficamente 
em locais diferentes, as unidades foram criadas considerando áreas do 
conhecimento diferentes e culturas organizacionais distintas. Entre os diversos 
problemas encontrados elenca-se: a ausência de padrão na confecção das atas; 
atas não publicadas; controle manual de presenças e ausências de membros nas 
reuniões; dificuldade de envio das convocações das reuniões; dificuldade no envio 
de arquivos para os membros; inexistência de um repositório único para atas; 
erros de registros nas atas; dificuldade em pesquisar e encontrar informações em 
atas anteriores; entre outros. Para explicitar o tamanho do problema, é importante 
ressaltar que a UNIFESP possui 165 órgãos colegiados permanentes entre 
Conselhos, Comitês institucionais, Congregações, Colegiados de Cursos, Núcleos 
e Câmaras Técnicas. 
Considerando os problemas descritos acima e visando a transparência, 
bem como a gestão de todos esses conselhos que são descentralizados, surgiu 
a necessidade do desenvolvimento de um sistema informático de Gestão dos 
Conselhos, que realizasse a informatização completa das atividades operacionais. 
Assim foi desenvolvido o Sistema de Apoio à Gestão de Conselhos Centrais, 
Conselhos de Campus, Congregações, Comissões, Comitês e Câmaras da UNIFESP 
(GESCON). página
118
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 116-121, junho de 2019
Capítulo XIX - GESCON: Agilidade e transparência na gestão dos órgãos colegiados da universidade2. Métodos
A proposta foi amadurecida no interior da Superintendência de Tecnologia 
da Informação (STI) e com uma equipe inicialmente composta de um analista e 
dois técnicos de TI iniciou-se o desenvolvimento.
Foram realizadas entrevistas com os servidores responsáveis por secretariar 
os conselhos para elencar os requisitos funcionais e não funcionais, e entender 
o fluxo das atividades operacionais que envolviam a organização das reuniões. 
Também foi realizado um estudo de atas antigas para extrair as características 
comuns e as boas práticas de redação e por último, o mapeamento dos processos 
realizados (as is) utilizando a metodologia Business Process Management (BPM). 
O GESCON foi desenvolvido em linguagem PHP e fez uso das bibliotecas: 
Alertify JS, Bootstrap, Mpdf, PhpMailer, Tinymce e JQuery, todas licenciadas sob 
licenças de livre uso. Foi também utilizada a ferramenta tipo RAD1 de mercado 
chamada ScriptCase. O sistema integrado de gestão de banco de dados utilizado foi 
o Oracle. Após o desenvolvimento da primeira versão estável, em 2016, o GESCON 
foi implementado no principal conselho da UNIFESP , o Conselho Universitário 
(CONSU) e posteriormente foi expandindo seu uso para outros conselhos 
superiores. 
E, finalmente, em dezembro de 2018 foi promulgada a portaria n. 4708 que 
instituiu o GESCON e tornou obrigatório seu uso para os 165 colegiados listados 
na portaria. “O GESCON deverá ser utilizado para auxiliar na gestão de todos os 
conselhos Centrais, Conselhos de Campus, Congregações, Comissões, Comitês e 
Câmaras da UNIFESP , para agendamento das reuniões, registro dos membros natos 
e eletivos, frequência e justificativas, elaboração das pautas e atas de forma mais 
rápida, facilitada, eficiente, integrada, segura e transparente, além de permitir o 
armazenamento de arquivos referentes às reuniões de forma organizada, além de 
obter, com eficiência, informações que facilitem o processo de tomada de decisões, 
a partir da publicação desta portaria. ” [UNIFESP , 2018].
3. Funcionalidades
Entre inúmeras funcionalidades do GESCON, destacaremos algumas das 
mais relevantes: 
a) Cadastro de todos os membros participantes das reuniões, consideramos 
membros natos ou indicados e membros efetivos, isto é, eleitos através das 
eleições periódicas da UNIFESP , bem como os seus suplentes;
1 RAD: Rapid Application Development ou Desenvolvimento Rápido de Aplicações.página
119
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 116-121, junho de 2019
Capítulo XIX - GESCON: Agilidade e transparência na gestão dos órgãos colegiados da universidade
Figura 1. Fomulário do cadastro de um membro no CONSU e da reunião
b) Registro de todos os órgãos2 da UNIFESP , bem como as pautas que serão 
discutidas e também o registro de totais de votos a favor, contra e abstenções, 
e permite registrar da presença e ausência dos membros e convidados das 
reuniões; 
c) Permite o acesso rápido dos arquivos pertinentes a cada reunião pelos 
seus membros e também aberto a comunidade da UNIFESP , através da 
intranet, lembramos ainda que as pautas são enviadas automaticamente 
pelos sistema com anexos destes arquivos dentro do corpo do e-mail;
d) A parte introdutória da ata, incluindo a frequência dos membros e gerada 
automaticamente, bastando o técnico em secretariado digitar todo o 
conteúdo do desenrolar das pautas discutidas e aprovadas;
e) O sistema possui módulo de painel de pautas ou dashboard que é utilizado 
para acompanhamento de todas as reuniões cadastradas pelos técnicos em 
secretariado, nele prescreve com detalhes a pauta da reunião, os membros 
participantes, o local e duração além de um indicador de compliance para 
validar se está em conformidades com as regras internas, possui filtro para 
pesquisar os dados da pauta, em destaque a pesquisa por palavra-chave 
dentro da pauta.
4. Resultados
Os resultados alcançados foram numerosos, mas salienta-se a agilidade na 
elaboração de atas, o controle efetivo dos mandatos e frequência dos membros e a 
transparência pública aos cidadãos. Abaixo lista de forma detalhada alguns desses 
resultados:
Cadastro e gerência de todos os membros efetivos e suplentes de cada órgão 
colegiado com integração com a base de dados do Departamento de Recursos 
Humanos da UNIFESP , com o nome, cargo, departamento, e-mail, telefone, data 
de início e final do mandato (série histórica individual), direito de voto, dados 
sobre a nomeação, geração de declaração; 
2 Orgãos pode ser: Conselhos Centrais, Conselhos de Campus, Congregações, Comissões, Conselhos de 
Departamento, Núcleo ou Câmaras da UNIFESP .página
120
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 116-121, junho de 2019
Capítulo XIX - GESCON: Agilidade e transparência na gestão dos órgãos colegiados da universidade
Figura 2. Justificativas de ausências em reuniões e Presença de conselheiros por 
categoria em reuniões no CONSU
Registro das frequências: presenças, ausências e as justificativas de ausências 
de cada membro; A ata pode ser elaborada em menor tempo, com a utilização 
de um editor incorporado ao sistema, a inclusão automática das presenças e 
ausências e pela utilização de padrão anteriormente definido, já em conformidade 
com o Manual de Redação da Presidência da República, veja figura 2. Registro 
documental das reuniões e disponibilização dos arquivos pertinentes na intranet 
para a leitura dos membros antes das reuniões e para pesquisas futuras; 
As atas, após homologadas, recebem um código de segurança (hash-sha1), 
veja figura 3, visando garantir a sua autenticidade e integridade. O GESCON realiza 
a publicação automática dessas atas na página WEB da UNIFESP (transparência 
ativa) e permite a realização de buscas no conteúdo das mesmas; 
Figure 3. Exemplo de ata já formatada pelo Sistema GESCON
Integração das informações, visando relatórios gerenciais mais consistentes 
e confiáveis com a geração de estatísticas, além de permitir a exportação de 
dados para a análise por outros softwares de Bussines Inteligence e Data Science. 
Redução do consumo de papéis, de insumos de impressão e acesso às informações 
atualizadas (online); página
121
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 116-121, junho de 2019
Capítulo XIX - GESCON: Agilidade e transparência na gestão dos órgãos colegiados da universidade5. Conclusões
A participação nos conselhos centrais tem sido monitorada de forma 
sistemática pelo GESCON. Esse monitoramento permite à instituição adotar 
ações de estímulo à presença nessas instâncias deliberativas e de governança 
participativa, bem como ampliar a transparência ativa. Além da disponibilização 
de informações aos cidadãos, o GESCON também funciona como uma ferramenta 
de apoio à gestão estratégica, subsidiando a tomada de decisões, o planejamento 
e a auditoria interna. 
Futuramente pretende-se disponibilizar o GESCON no portal do Software 
Público do Governo Federal de forma que outras Instituições Federais de Ensino 
possam utilizá-lo e auxiliar em seu desenvolvimento. 
Referências
Brasil (1988). Constituição da República Federativa do Brasil. Brasília, DF. 1988. 
Brasil (1996). Lei no 9.394, de 20 de dezembro de 1996. Estabelece as diretrizes e 
bases da educação nacional. Diário Oficial [da] República Federativa do Brasil, , Poder 
Executivo, Brasília, DF, v. 134, n. 248, 23 dez. 1996. Seção 1, p. 27834-27841. 1996. 
Brasil ([S.d.]). Conheça seu direito. http://www.acessoainformacao.gov.br/assuntos/
conheca-seu-direito/conheca-seu-direito , [accessed on Dec 6]página
122
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 122-127, junho de 2019
Capítulo XX - Iftopper - Controle Dinâmico de Largura de BandaIftopper - Controle Dinâmico de Largura de 
Banda
Eduardo Maroñas Monks1, Jeronimô Feijó Noble da Rosa1
¹Universidade Federal de Pelotas (UFPel), Pelotas – RS – Brasil
{emmonks,jeronimo.feijo}@ufpel.edu.br
Resumo
Este artigo descreve a abordagem realizada na Universidade Federal de Pelotas para o gerenciamento dinâmico 
de largura de banda baseado em software livre. A solução criada possui o nome de Iftopper e tem como princípio 
o melhor uso da largura de banda, com uma abordagem dinâmica para aplicação de restrições. Serão descritas as 
metodologias utilizadas e os resultados e conclusões.
1. Introdução
O principal recurso de rede é largura de banda. Entretanto, sem um 
gerenciamento do uso deste recurso é inevitável que aconteçam problemas de 
lentidão, mesmo com links de acesso à Internet de grande capacidade, tal como 
acontece nas Ifes (Instituições Fe-derais de Ensino Superior). Com a popularização 
de smartphones, tablets e  notebooks, o controle de quais aplicações os usuários 
utilizam em rede acaba tornando a rotina dos gerentes de rede mais complicada. 
Um dos maiores consumidores de largura de banda são as aplicações que fazem 
uso do protocolo Bittorrent e sites tais como Facebook e Youtube. Um das soluções 
é a utilização de firewalls que realizam DPI (Deep Packet Inspection) para determinar 
restrições de banda a serviços específicos. Porém, estas soluções para links de 
grande porte, 1 Gbit/s ou maior, possuem alto custo de aquisição e manutenção. 
Outro fator importante é individualizar as restrições de banda, identificando os 
usuários com consumo excessivo, não penalizando os demais usuários. A ideia 
central é tornar transparente para os usuários o controle dos recursos de rede.
Na UFPel (Universidade Federal de Pelotas), o uso de BYOD (Bring Your Own 
Device) na comunidade gerou um descontrole sobre quais aplicaçõe fazem uso da 
rede e podem estar instaladas nos hosts. Esta realidade vem se tornando comum 
nas demais Ifes e instituições de ensino em geral. Desta forma, o consumo de 
largura de banda deve ser gerenciado sem haver o controle de quais aplicações 
estão sendo usadas. Por exemplo, um vídeo do Netflix em HD pode consumir cerca 
de 5 Mbit/s e em ultra HD pode chegar a 25 Mbit/s [Netflix 2018]. Em um ambiente 
onde existem cerca de 20.000 usuários, como é o caso da UFPel, constantemente 
haverá abusos de consumo de largura de banda. A alternativa radical para resolver 
isto é identificar o tráfego e realizar bloqueios. Entretanto, ao aplicar estas ações, 
haverá reclamações dos usuários. Se não for realizado nenhum procedimento, página
123
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 122-127, junho de 2019
Capítulo XX - Iftopper - Controle Dinâmico de Largura de Bandahaverá reclamações dos usuários que a rede está lenta. Portanto, deverá haver 
uma forma de tornar esta situação mais favorável para a administração da rede, 
fazendo com que nenhum dos serviços seja bloqueado, mas que estes mesmos 
serviços não congestionem a rede.
Neste cenário, foi desenvolvida uma solução baseada em software livre 
para gerenciar o consumo de largura de banda nos diversos campi da UFPel. Esta 
solução denominada Iftopper está em funcionamento desde de 2014 e se tornou 
uma ferramenta essencial para a administração de redes da instituição. O objetivo 
deste artigo é apresentar a ferramenta Iftopper e os resultados coletados ao longo 
da implantação no contexto da UFPel.
2. Métodos
O cenário da UFPel é composto por 6 campi e mais de 400 prédios, com 
mais de 20.000 usuários entre discentes, docentes, técnico-administrativos e 
terceirizados, distribuídos pelas cidades de Pelotas e Capão do Leão. Nestes campi 
existem mais de 5.000 computadores e mais cerca de 2.500 usuários simultâneos 
que utilizam a rede sem fios da instituição.
A UFPel possui dois links de acesso à Internet disponibilizados pela RNP 
(Rede Nacional de Ensino e Pesquisa). No campus Anglo existe um link de 1 
Gbit/s que atende 5 campi localizados na cidade de Pelotas. No campus Capão 
do Leão existe um link de 200 Mbit/s que atende o campus nesta cidade. Os campi 
na cidade de Pelotas são interligados por fibra ótica até o campus Anglo, onde 
fica o link de interconexão com a RNP . No campus Capão do Leão, existe um link 
somente para atender esse campus. A comunicação entre os campi localizados 
em Pelotas e Capão do Leão acontece passando pelo POP/RS (Ponto de Presença 
da RNP no Rio Grande do Sul). Antes da interligação dos prédios por fibra e a 
saída centralizada no link da RNP , vários prédios possuíam links contratados 
com velocidades variando de 5 Mbit/s a 50 Mbit/s, o que facilmente se esgotavam 
causando lentidão e reclamações dos usuários. Atualmente, ainda existem alguns 
prédios com restrições nos links.
Atualmente, existem firewalls distribuídos em alguns dos prédios com maior 
quantidade de usuários. O objetivo é balancear a carga entre vários firewalls ao 
invés de usar apenas um centralizado em cada link de acesso à Internet. São 5 
firewalls baseados em Linux distribuídos na cidade de Pelotas e um no campus 
Capão do Leão. Além destes, existem 6 servidores de gerenciamento da rede sem 
fios que também atuam como firewalls e ficam localizados junto aos firewalls de 
cada prédio/campus.
As primeiras tentativas de gerenciar o consumo de banda foram realizadas 
na rede sem fios, por meio de limitações de banda por IP com o uso da ferramenta 
tc [Hemminger 2018] nos servidores de autenticação. Na época, utilizou-se um 
limite de 1 Mbit/s para cada IP . Sem nenhuma limitação, o consumo de largura 
de banda tornava o link congestionado devido ao grande volume de downloads, 
principalmente, para atualizações de sistemas operacionais e aplicações. Com as página
124
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 122-127, junho de 2019
Capítulo XX - Iftopper - Controle Dinâmico de Largura de Bandalimitações impostas, os usuários reclamavam que a rede estava lenta. A partir disto, 
começou-se a busca por uma solução que pudesse tornar as restrições menos 
agressivas, que causasse penalidade somente ao usuário com consumo excessivo 
e que funcionasse de forma mais transparente possível. Destas necessidades, no 
ano de 2014, começou a ser desenvolvida a solução Iftopper.
2.1 Iftopper
Para a criação da solução Iftopper, foi utilizada a ferramenta Iftop [iftop 2018] 
para gerar o relatório da medição de tráfego na rede. Esta ferramenta, na versão 
1.04pre, permite a geração de relatórios em arquivo das conexões com maior 
volume de bytes transferidos. A partir desta funcionalidade foi desenvolvido um 
shell script para aplicar restrições aos IPs detectados acima do limite configurado 
para consumo de banda. Inicialmente, foi criada a detecção para conexões de 
download, posteriormente para upload, e estabelecido que tráfegos acima de 2 
Mbit/s seriam taxados em 512 Kbit/s por 10 minutos. Após os 10 minutos, o IP seria 
liberado da restrição. A restrição foi criada com filas CBQ (Class Based Queue) por 
meio da ferramenta tc. A sequência de funcionamento padrão do Iftopper segue 
com as etapas:
• O script é agendado para ser executado a cada 2 minutos e chama a 
ferramenta Iftop que fica em execução por 10 segundos. O Iftop gera relatório 
com as 10 conexões com maior quantidade de bytes transferidos;
• Será analisado o relatório para detecção de conexões que superaram o 
limite de consumo estabelecido. Caso existam conexões acima do limite, o 
endereço IP da rede interna será identificado e relacionado com a VLAN a 
qual pertence;
• O IP detectado será verificado em uma lista de endereços liberados e na 
lista de endereços já limitados. Caso o endereço esteja em alguma destas 
listas, não será realizada nenhuma ação; 
• Caso o IP detectado não esteja em alguma das listas, será feita a associação 
do endereço a uma classe CBQ previamente criada com restrições de largura 
de banda. Também será feito o agendamento da remoção do endereço para 
os próximos 10 minutos; 
• Será criado um arquivo de log contendo informações sobre os endereços IP 
de origem e destino, largura de banda consumida e sentido do tráfego (down-
load/upload). A cada 1h, os logs são enviados para um banco de dados para 
armazenamento e geração de dados estatísticos.
Os valores de parametrização do sistema foram definidos de forma empírica 
e ajustados ao longo dos anos para obtenção de melhores resultados. Devido a 
criação da ferramenta ser baseada em outras ferramentas conhecidas e comuns 
em sistemas Linux, o consumo de recursos dos servidores é mínimo, causando 
impacto desprezível no desempenho geral dos firewalls e servidores. Um dos página
125
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 122-127, junho de 2019
Capítulo XX - Iftopper - Controle Dinâmico de Largura de Bandafatores para diminuir o impacto nos recursos do sistema é o uso da criação das 
classes CBQ previamente e não sob demanda. Nas primeiras tentativas, vários 
problemas ocorreram no gerenciamento da criação e remoção de classes, até 
mesmo com travamento do sistema operacional. A solução foi o uso de associação 
de filtros as classes já criadas e a remoção apenas dos filtros que apontam para 
os IPs a serem penalizados. A evolução nos ajustes aconteceu nos parâmetros que 
definem o limite de consumo de largura de banda aceitável e o valor da restrição 
de banda. Por exemplo, atualmente, conexões que tenham 8 Mbit/s ou maior são 
consideradas abusivas. O endereço IP envolvido nesta conexão ficara´ taxado 
em 4 Mbit/s por 10 minutos. Espera-se que dentro destes 10 minutos o download  
tenha acabado, mas caso continue, este mesmo IP poderá voltar a ser penalizado 
diversas vezes. Devido ao script Iftopper ser executado a cada 2 minutos, poderá 
acontecer de este mesmo IP ser liberado da penalização e ficar os próximos 2 
minutos usando o máximo de vazão possível. Portanto, os ajustes dos parâmetros 
devem ser adequados de acordo com os recursos de largura de banda disponíveis.
3. Resultados
Na inexistência de gerenciamento de largura de banda, o consumo de 
largura de banda nos campi da UFPel causava transtornos para os usuários e 
administradores de rede. Com o uso de restrições estáticas para o consumo de 
banda, as reclamações sobre lentidão eram frequentes por parte dos usuários. 
Com o uso da ferramenta Iftopper, o consumo de largura de banda tornou-se 
mais justo e com melhor aproveitamento de recursos. Os valores detectados ao 
longo dos anos obtiveram aumentos a partir do ano de 2016, até então haviam 
permanecidos estáveis. Com o aumento dos links nos campi da UFPel a partir 
do final de 2017, devido a implantação da rede COMEP [RNP 2018], os valores 
médios de consumo aumentaram consideravelmente, estes aumentos podem ser 
visualizados nos gráficos da Figura 1.
Na Figura 1a, pode ser visualizado o número de ocorrências de conexões 
que exce-deram o limite configurado na ferramenta Iftopper. O gráfico mostra três 
faixas de largura de banda, sendo para conexões com largura de banda maior que 
20 Mbit/s e menor que 40 Mbit/s, de 40 Mbit/s até 60 Mbit/s e de 60 Mbit/s até 90 
Mbit/s. Percebe-se a evidente evolução do aumento de largura de banda, onde 
no ano de 2017 com a disponibilização de links com maior capacidade houve um 
aumento considerável nas ocorrências de violações detectadas pela ferramenta 
Iftopper.página
126
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 122-127, junho de 2019
Capítulo XX - Iftopper - Controle Dinâmico de Largura de Banda
Figura 1. (a)Número de ocorrências acima do limite definido, por ano, detectadas 
no iftopper;(b)Número de ocorrências acima de 20 Mbit/s, por ano e tipo de 
serviço, detectadas no iftopper
Na Figura 1b, uma comparação do número de ocorrências de acordo com 
três serviços populares: Netflix, Facebook/Whatsapp/Instagram e Google/
Youtube. Estes da-dos foram filtrados a partir dos endereços IPs registrados nas AS 
(Autonomous System) destes serviços e as ocorrências destes endereços na base de 
registros do Iftopper. Foram contabilizadas todas as ocorrências das conexões que 
atingiram 20 Mbit/s ou mais entre os anos de 2014 e 2018. O consumo de largura 
de banda de serviços de streaming de vídeos Netflix e Youtube são os responsáveis 
pelo maior número de ocorrências e possuem uma tendência de aumento. O uso 
do serviço Whatsapp e aplicações móveis, principalmente para o download de 
vídeos, tem aumentado consideravelmente o que deverá continuar acontecendo 
nos próximos anos. 
A Figura 2, mostra o consumo de largura de banda do link do Campus Anglo 
que possui 1 Gbit/s. Este link atende a 5 campi na cidade de Pelotas, onde ficam 
concentrados a maioria dos usuários e como pode ser visto na imagem, em 30 
dias não passou de 300 Mbit/s, mesmo assim em picos de uso. Percebe-se que 
o consumo do link não atingiu o esgotamento. Isto só se tornou possível com o 
auxílio da ferramenta Iftopper no gerenciamento dinâmico da largura de banda.
Figura 2. Consumo do link de 1 Gbit/s do Campus Anglopágina
127
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 122-127, junho de 2019
Capítulo XX - Iftopper - Controle Dinâmico de Largura de Banda4. Conclusões
O aumento do consumo de largura de banda proporcionado pelas aplicações 
em rede vem crescendo a cada ano. Nos registros obtidos por meio da ferramenta 
Iftopper, no contexto da UFPel, em 4 anos a média de vazão detectada nas 
conexões mais que triplicou. Com este cenário, sem um gerenciamento de 
consumo de tráfego a administração da rede torna-se impraticável. Os ajustes dos 
parâmetros, tempo de detecção, tempo de aplicação de restrições e largura de 
banda disponibilizada para o IP detectado devem ser avaliadas e adequadas ao 
longo do tempo. Na UFPel, ainda existem diversos prédios que possuem links de 
baixa capacidade e sem um controle de consumo de banda haveria prejuízo nas 
atividades acadêmicas e administrativas. Entretanto, só aumentar a capacidade 
dos links de acesso à Internet não é a solução e muitas vezes não está ao alcance 
das instituições devido a falta de recursos, burocracia ou viabilidade técnica para 
implantação. Portanto, fazer o melhor uso dos recursos existentes poderá ser a 
única opção. A ferramenta Iftopper se mostrou uma alternativa viável e eficaz para 
gerenciar a largura de banda de uma instituição de grande porte como a UFPel. 
Como trabalhos futuros, além da continuidade da ferramenta Iftopper, pretende-se 
desenvolver o script na linguagem Python para tornar mais simples a configuração 
em outros ambientes de rede diferentes da UFPel.
Referências
Hemminger, S. (2018). acesso em: 17 abr 2018. Iproute2 - collection of utilities for 
controlling TCP/IP networking and traffic control in Linux. Disponível em: <https: //
wiki.linuxfoundation.org/networking/iproute2/>.
iftop (2018). acesso em: 17 abr 2018. iftop: display bandwidth usage on an interface. 
Disponível em: <http://www.ex-parrot.com/pdw/iftop/>.
Netflix (2018).  acesso em: 17 abr 2018.  Recomendações de velocidade da conexão à 
Internet. Disponível em: <https://help.netflix.com/pt/node/306/>.
RNP (2018).  acesso em: 17 abr 2018.  Redes Comunitárias de Educação e Pesquisa. 
Disponível em: <http://www.redecomep.rnp.br/>.página
128
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 128-132, junho de 2019
Capítulo XXI - Implantação automatizada de sítios institucionais com uso de Docker, Git e JenkinsImplantação automatizada de sítios 
institucionais com uso de Docker, Git e Jenkins
João G. A. Martinez¹ , Gérson B. da Silva¹ , Diogo Soares¹
¹Centro de Tecnologia da Informação e Comunicação – Universidade Federal do Amazonas 
(UFAM)
Caixa Postal 69080-900 – Manaus – AM – Brasil
{jgam,gbs,diogosoaresm}@ufam.edu.br
Resumo
A demanda por sítios web oferecidos por instituições de ensino federais têm intensificado nos últimos anos, com 
objetivo de prover visibilidade e serviços para a sociedade e comunidade acadêmica. Em geral, o processo de deploy 
de sítios pode requerer recursos de tempo e pessoal, visto que essa é uma demanda cíclica. De modo a mitigar o 
uso de recursos de pessoal e tempo, este artigo descreve o processo de implantação automatizada de novos sítios 
institucionais a partir do uso das tecnologias Docker, Git e Jenkins. O objetivo é acelerar, padronizar, centralizar e 
simplificar o processo de implantação gestão e manutenção dos sítios, além de economizar recursos computacionais 
e humanos.
Palavras-chave: sítios, infraestrutura, automação, docker, git, jenkins.
1. Introdução
Existe uma grande demanda por sítios institucionais em universidades federais 
brasileiras devido ao número de departamentos, cursos e órgãos suplementares 
Existe uma grande demanda por sítios institucionais em universidades federais 
brasileiras devido ao número de departamentos, cursos e órgãos suplementares 
existentes nessas instituições. Na Universidade Federal do Amazonas (UFAM), o 
Centro de Tecnologia da Informação e Comunicação (CTIC) é o órgão responsável 
pela implantação, manutenção e infraestrutura desses sítios.  Por padrão, a 
plataforma Joomla! de gestão de conteúdos foi escolhida pela gestão do CTIC 
devido a sua facilidade de uso pelos usuários e simplicidade para instalação e 
manutenção.
No formato anterior de entrega, cada sítio era uma instância Joomla!, que 
precisava de uma entrada VirtualHost no Apache. Manualmente era necessária a 
inclusão do sítio no servidor DNS (Domain Name Service). Na máquina hospedeira, 
era preparado um local para receber os arquivos e no servidor de banco de dados 
era feito a criação de um banco exclusivo para o site, com os arquivos no servidor, 
o sítio entrava em produção, porém, a nível de sistema operacional, estes eram 
executados pelo mesmo usuário e com permissões pouco restritivas, portanto, 
caso um destes fosse invadido, todos os demais estariam em risco.
Além disto, algumas dificuldades operacionais no processo de implantação e 
manutenção dos sítios foram encontradas. Entre elas, quando um sítio era invadido, página
129
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 128-132, junho de 2019
Capítulo XXI - Implantação automatizada de sítios institucionais com uso de Docker, Git e Jenkinsseu código fonte muitas vezes era modificado, dificultando sua recuperação, uma 
vez que a política de backup utilizada era a nível de máquina virtual, assim, sendo 
necessário a recuperação do backup completo da máquina hospedeira. 
Um outro problema era a falta de padronização de versão e configuração 
dos servidores Apache instalados, e a gestão descentralizada. Dificuldade, 
também, era gerenciar e garantir que as permissões de arquivos estabelecidos 
como seguras, estivessem aplicadas, devido ao alto número de sítios (acima de 
100). Além do mais, a implantação de novos sítios dependia do seguinte fluxo de 
comunicação: usuário requisita sítio para o setor de desenvolvimento e este solicita 
configurações gerais para o setor de infraestrutura. Tal comunicação ocorre em via 
dupla até a finalização do sítio, aumentando o tempo de espera do usuário para 
disponibilização do serviço.
Por fim, a redução significativa dos recursos humanos no CTIC, com o passar 
do tempo, tornou-se um problema.
Para atender essa demanda de forma rápida, prática e organizada foi 
elaborado um processo automatizado que utiliza as tecnologias Docker, Git e 
Jenkins para instalar e gerenciar os sítios institucionais. No restante deste trabalho 
é detalhado como as tecnologias supracitadas foram integradas e utilizadas. Além 
disso, os principais benefícios alcançados por nosso processo são descritos.
2. Metodologia
O Docker é uma tecnologia de software que permite a criação rápida e fácil 
de ambientes computacionais isolados, chamados de contêineres. Um contêiner é 
uma unidade padrão de software que empacota código-fonte e suas dependências 
e que compartilha o kernel do sistema operacional da máquina onde está em 
execução. Os contêineres são semelhantes às máquinas virtuais, porém, como 
os recursos são compartilhados, é possível executar vários contêineres em uma 
mesma máquina (física ou virtual), de forma a otimizar os recursos computacionais 
disponíveis [Vitalino e Castro 2016].
Uma imagem Docker é um arquivo que define várias instruções que geram 
uma aplicação executável. Quando um usuário executa uma imagem Docker, ele 
instancia um ou mais contêineres daquela imagem. O Docker cria uma subrede 
dentro da máquina hospedeira para fornecer endereços IP locais para os 
contêineres, e faz a interface entre os contêineres e a máquina hospedeira através 
das portas da máquina. Assim, no momento que um contêiner é instanciado, é 
necessário apenas o mapeamento de uma porta disponível da máquina hospedeira, 
para uma porta do contêiner.
O Git é um sistema de controle de versão distribuído, utilizado para 
registrar o histórico de alterações em arquivos e muito utilizado em equipes de 
desenvolvimento de software. Através do Git, é possível reverter um software para 
versões anteriores de forma rápida e fácil [Silverman 2013].página
130
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 128-132, junho de 2019
Capítulo XXI - Implantação automatizada de sítios institucionais com uso de Docker, Git e JenkinsO Jenkins é um servidor de integração contínua de código aberto feito em 
Java que permite automatizar procedimentos computacionais como executar 
comandos em scripts, compilar código-fonte e enviar emails, dentre outros. Cada 
conjunto de procedimentos é chamado de job no Jenkins [Boaglio 2016]. 
Para a integração completa e automatizada que se buscava, foi necessária 
a utilização de um container DNS que automaticamente adicionasse a entrada 
do job no Jenkins como uma entrada DNS. Por exemplo, job: teste teria a 
entrada DNS: teste.sites.ufam.edu.br adicionada aos servidores da Universidade 
automaticamente.
Inicialmente foi instanciada uma máquina virtual com o sistema operacional 
Ubuntu Server e feita a instalação do Docker, sendo esta utilizada como servidor 
de produção. Além disso, foi instanciada uma máquina virtual com o Gitlab, e uma 
máquina virtual com o Jenkins. Afim de padronizar e agilizar o processo de criação, 
foi definido que os sítios institucionais mantidos pelo CTIC fariam uso somente do  
template visual padrão de sítios do governo federal, sendo permitido ao usuário 
mantenedor do sítio escolher somente a cor base.
3. Resultados
Para cada novo sítio requisitado ao CTIC, é criado um repositório no Git  
com o código-fonte do Joomla! em sua versão mais atualizada com os arquivos 
de configuração editados e customizados para o sítio solicitado conforme 
características do órgão requisitante. Esse procedimento inicial é realizado pela 
equipe de desenvolvimento.
Em seguida é criado um job no Jenkins, que executa o fluxograma demonstrado 
na Figura 1. Este job foi criado inicialmente pela equipe de infraestrutura e, 
cabendo a equipe de desenvolvimento posteriormente, a cópia do job template 
para execução em novas instâncias.
Figura 1. Fluxograma de execução de job no Jenkins.
Ao iniciar, o job recupera o código-fonte do sítio no Git e valida as configurações 
do banco de dados colocadas no arquivo configuration.php para garantir que o 
desenvolvedor seguiu o padrão de nomenclaturas estabelecido pela equipe de página
131
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 128-132, junho de 2019
Capítulo XXI - Implantação automatizada de sítios institucionais com uso de Docker, Git e Jenkinsinfraestrutura. Em seguida é cadastrado um registro DNS de maneira dinâmica 
em um servidor DNS1 que responde para o subdomínio sites.ufam.edu.br. Isso 
possibilita que o sítio já fique disponível para o usuário acessar e inserir conteúdo.
Em seguida são criados dois contêineres no servidor Jenkins para teste, um 
com um banco de dados MySQL inicializado a partir da importação do arquivo .sql 
inicial do sítio, e o outro é construído a partir de uma imagem Apache com PHP2 
com o código-fonte do sítio institucional. Com isso, é feito um teste de requisição 
HTTP (HyperText Transfer Protocol) para validar que o sítio está operacional e, em 
caso de sucesso, ambos os contêineres são descartados e a imagem do contêiner 
do sítio institucional é enviada para o servidor de produção e lá o contêiner é 
instanciado de forma definitiva. Este passo serve para validar se o código fonte 
Joomla! utilizado no sítio conseguirá subir corretamente no servidor de produção.
O fluxo da Figura 1 é executado de forma automática toda vez que o 
desenvolvedor faz uma atualização no código-fonte do repositório Git do sítio 
institucional. O Jenkins faz a detecção automática das atualizações através de uma 
monitoração ativa do repositório a cada cinco minutos.
No servidor de produção, todos os contêineres dos sítios institucionais 
utilizam as portas 80 ou 443 (conexão segura), porque utilizam o servidor Apache. 
No entanto, a máquina hospedeira só pode alocar um contêiner por porta. Para 
resolver este problema foi utilizado um único servidor de proxy reverso [de Oliveira 
Mello 2018] nessas portas, que recebe a requisição e a direciona para o contêiner a 
partir da url do sítio. O Traefik foi escolhido para atuar como servidor proxy reverso 
porque é especializado em contêineres Docker. A Figura 2 mostra o funcionamento 
do redirecionamento das requisições para os contêineres.
Figura 2. Funcionamento básico do Traefik com os sítios institucionais.
4. Conclusão
A automatização no processo de implantação dos sítios institucionais para 
o CTIC proporcionou diversos benefícios após sua concretização: recuperação 
rápida de sítios invadidos, padronização de configurações do servidor Apache  
1 https://hub.docker.com/r/davd/docker-ddns
2 https://hub.docker.com/_/phppágina
132
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 128-132, junho de 2019
Capítulo XXI - Implantação automatizada de sítios institucionais com uso de Docker, Git e Jenkinse das permissões dos diretórios dos sítios, disponibilização rápida de novos 
sítios para usuários finais, mais agilidade na comunicação entre as equipes de 
infraestrutura e desenvolvimento, economia do uso de recursos computacionais e 
menor demanda de recursos humanos envolvidos.
Como trabalhos futuros, será feita a ativação do IPv6 (Internet Protocol version 
6) nos contêineres, a geração e configuração automatizada de certificados SSL 
(Secure Sockets Layer) utilizando o Traefik e a automatização no versionamento da 
plataforma Joomla! dos sítios institucionais.
Referências
Apache. https://www.apache.org. Acesso em 16 de abril de 2019.
Gitlab. https://gitlab.com. Acesso em 16 de abril de 2019.
Joomla!. https://www.joomla.org. Acesso em 16 de abril de 2019.
MySQL. https://www.mysql.com. Acesso em 16 de abril de 2019.
Portal padrão. http://portalpadrao.gov.br . Acesso em 11 de março de 2019.
Traefik. https://traefik.io . Acesso em 16 de abril de 2019.
Ubuntu. https://www.ubuntu.com. Acesso em 16 de abril de 2019.
Boaglio, F. (2016) “Jenkins: Automatize tudo sem complicações” , Casa do Código.
de Oliveira Mello, V. (2018) “Proxy reverso: o que é e como usar” , https://king.host/
blog/2018/06/proxy-reverso-o-que-e-e-como-usar/. Acesso em 16 de abril de 2019.
Silverman, R. (2013) “Git – Guia Prático” , Novatec.
Vitalino, J. F. N. e Castro, M. A. N. (2016) “Descomplicando o Docker” , Brasport.página
133
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 133-139, junho de 2019
Capítulo XXII - Implantação da rede sem fio na UFCAImplantação da rede sem fio na UFCA
Taciano P de A Alcântara¹ , Marcos Iury F M da Silva¹, Herbert N Onofre¹
¹Diretoria de Tecnologia da Informação – Universidade Federal do Cariri (UFCA)
63048-080 – Juazeiro do Norte – CE – Brasil
{taciano.pinheiro,iury.fernandes,herbert.novais}@ufca.edu.br
Abstract
This paper describes the experience of wireless network deployment at the Federal University of Cariri. The difficulties 
encountered and the solutions adopted are exposed. Some results, conclusions and future work are also presented. 
Resumo
Este artigo descreve a experiência da implantação da rede sem fio na Universidade Federal do Cariri. São expostos 
as dificuldades encontradas e as soluções adotadas. Também são apresentados alguns resultados, conclusões e 
trabalhos futuros.
1. Introdução
A Universidade Federal do Cariri (UFCA) foi criada em 2013 a partir do 
desmembramento da Universidade Federal do Ceará (UFC). Com 5 campi, 17 
cursos de graduação e 8 de pós-graduação, a sua comunidade acadêmica possui 
aproximadamente 3.500 alunos e 621 servidores.
Até 2015 a instituição enfrentou diversas dificuldades no acesso à rede sem 
fio. Já existia uma grande demanda da comunidade acadêmica para conectar seus 
dispositivos Wi-Fi e desenvolver suas atividades no conceito Bring Your Own Device  
(BYOD). 
Este trabalho tem o objetivo de compartilhar a experiência de implantação 
da rede sem fio na UFCA e são descritos desafios e dificuldades enfrentados, assim 
como as soluções adotadas. Ao final, são apresentados os resultados e apontados 
trabalhos futuros.
2. Métodos
O desenvolvimento ocorreu conforme os seguintes passos: análise do 
cenário anterior da implantação, que detalha o contexto do problema antes do 
projeto; projeto/aquisição, que detalha os requisitos e a solução escolhida; e a 
implantação, que descreve como o projeto foi colocado em prática.página
134
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 133-139, junho de 2019
Capítulo XXII - Implantação da rede sem fio na UFCA2.1. Análise do cenário anterior da implantação
Em 2015 haviam várias soluções improvisadas e desintegradas de rede sem 
fio na UFCA. Cada ponto de acesso (AP) tinha sua própria identificação de rede 
sem fio (SSID), criptografia, chave compartilhada, entre outras, definidas pelo 
setor responsável pelo equipamento. A cobertura não era suficiente e a qualidade 
da conexão era ruim na maior parte do tempo. Interferências, loops na rede e 
indisponibilidades eram recorrentes. Além disso, os equipamentos utilizados 
não eram adequados para o ambiente universitário, que possui alta densidade 
de usuários. Por fim, a gerência era muito dispendiosa e não se alcançaram bons 
resultados.
2.2.  Projeto e aquisição
O projeto foi iniciado com a realização de testes com 4 soluções de 
fabricantes diferentes, com controladoras físicas (appliances) e em nuvem. Foi 
feito um estudo comparativo avaliando as vantagens e desvantagens de cada um 
e, concorrentemente, os requisitos eram refinados. Ao final, foi escolhida uma 
arquitetura com duas controladoras físicas, funcionando em campus diferentes 
para prover alta disponibilidade e, gerenciando de forma centralizada, todos os 
APs dos campi. 
A Figura 1 apresenta a arquitetura da solução idealizada e ilustra seus 
principais componentes. O campus Juazeiro do Norte hospeda a controladora 
primária, enquanto que a secundária se encontra no campus Barbalha. A 
controladora é o elemento central, sendo responsável por fazer a gestão da rede e 
o gerenciamento dos APs. Os APs possuem endereços IP válidos, com exceção os 
do campus Icó, que precisou de uma VPN por dispor apenas de endereços privados 
(RFC 1918). página
135
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 133-139, junho de 2019
Capítulo XXII - Implantação da rede sem fio na UFCA
Figura 1. Arquitetura da rede sem fio na UFCA
 A forma de autenticação escolhida foi da eduroam, que é uma solução de 
mobilidade internacional que provê acesso seguro aos usuários de instituições 
de ensino e pesquisa [Florio & Wierenga 2005]. O protocolo 802.1X [Mishra and 
Arbaugh 2002] com a ferramenta RADIUS e o serviço de diretório LDAP são os 
principais elementos da solução de autenticação, que sincronizam as credenciais 
dos usuários com o sistema acadêmico.
A aquisição da solução aconteceu em 2015, conforme previsto no Plano 
Diretor de Tecnologia da Informação (PDTI) [UFCA 2015], e o investimento foi de 
R$ 383.082,00 (trezentos e oitenta e três mil e oitenta e dois reais). Ao todo, foram 
adquiridas 2 controladoras, 100 APs internos, 8 APs externos, 6 APs ponto a ponto 
e licenças do software da controladora. Os APs possuem tecnologia Dual-Band, 
802.11ac, 3x3 MIMO, PoE e suportam 500 usuários simultâneos. As controladoras 
têm capacidade para 500 APs e 10.000 usuários, suportam cadastro de visitantes, 
integração RADIUS, detecção de interferência, entre outras. Os custos de ativos e 
de cabeamento estruturado não estavam no escopo do projeto.
2.3. Implantação
A implantação foi iniciada em 2016 e seguiu as etapas: configuração da 
controladora primária; instalação dos APs, implantação de alta disponibilidade 
e solução autenticação. Por ser uma instituição relativamente pequena toda a 
implantação foi realizada pela equipe de TI.página
136
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 133-139, junho de 2019
Capítulo XXII - Implantação da rede sem fio na UFCAFoi realizado um mapeamento de área (site survey) para determinar a 
localização dos APs e em seguida a instalação foi iniciada pelos servidores de TI e 
bolsistas. No geral não houve dificuldades com a instalação dos APs, pois já existia 
cabeamento estruturado disponível. Algumas dificuldades foram observadas 
por deficiência da infraestrutura dos locais, tais como ausência de ponto de 
rede, problemas de execução do cabeamento, tomadas elétricas inadequadas e 
vazamento de água no teto. No início da ativação da nova rede sem fio alguns 
usuários tiveram resistência para desativar as redes legadas. Porém com o bom 
desempenho da nova solução, essa resistência foi superada.
A medida que os APs eram instalados, a controladora era configurada e 
eram criados segmentos de rede (VLAN) específicos para a rede sem fio. Para o 
funcionamento inicial, foi definida uma rede com SSID UFCA sem autenticação, 
pois não se tinha acesso à base de usuários do sistema acadêmico, que ainda 
encontrava-se na instituição tutora (UFC).
Tabela 1. Redes sem fio na UFCA
SSID Visibilidade Autenticação Criptografia Público alvo
eduroam Visível 802.1X EAP WPA2Servidores a alunos da UFCA e 
comunidades acadêmicas da 
rede internacional eduroam.
UFCA_Visitantes VisívelBase interna 
de usuários.Não Visitantes.
UFCA_D Oculta Não WPA2Equipamentos Wi-Fi do tipo: 
Smart TV, impressora, projetor 
multimídia, câmera, etc.
Em 2017, em negociação com a tutora, a área de TI conseguiu acesso read-
only à base de dados do sistema acadêmico da UFC e possibilitou a autenticação 
de usuários. Com isso a rede UFCA passou a ser autenticada utilizando o padrão 
802.1X com RADIUS/LDAP . Neste momento foi observado que alguns dispositivos 
não suportavam este protocolo, porém estes se conectaram à rede UFCA_Visitantes, 
que não utiliza este tipo de autenticação e não possui criptografia. Esta rede utiliza 
funcionalidade de cadastro de usuários visitantes (guest access) da controladora. 
Existem planos de melhorar a segurança dessa rede no futuro através de utilização 
de criptografia.
No mesmo ano a UFCA fez adesão da rede eduroam, através de parceria com 
a Rede Nacional de Ensino e Pesquisa (RNP), e passou a oferecer uma nova forma 
de autenticação. Atualmente existem 3 redes sem fio, conforme Tabela 1. 
Sendo um serviço de bastante visibilidade, houve a necessidade de disciplinar 
o seu uso. Para isso foi aprovada as normas de uso da rede sem fio na instituição, 
disponível na Portaria N.º 79/2019 [UFCA 2019]. O documento foi construído por 
um grupo de trabalho, instituído pela Diretoria de Tecnologia da Informação 
(DTI), com participação da comunidade acadêmica. Por fim, o grupo propôs o 
fortalecimento da rede eduroam e a tornou como rede oficial, desativando a rede 
com SSID UFCA.página
137
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 133-139, junho de 2019
Capítulo XXII - Implantação da rede sem fio na UFCA3. Resultados
Ao final da implantação e, até o momento, foram instalados 83 APs e todos os 
espaços da UFCA estão com cobertura Wi-Fi. O equipamentos que restaram serão 
instalados nos prédios que estão em obra.
 Os benefícios percebidos com a implantação são destacados nos dados 
apresentados da quantidade de usuários conectados, o tráfego na rede sem fio e 
a pesquisa de satisfação do serviço. Os dados coletados são de todos os campi do 
dia 12/03/2019, das 06:00h às 22:00h, escolhido por ser um dia de rotina normal 
sem nenhum evento atípico.
Figura 2. Quantidade de usuários autenticados
Os dados demonstram pico de 1.350 usuários autenticados simultaneamente, 
conforme ilustrado na Figura 2. Essa quantidade representa uma taxa de cerca 39% 
da população acadêmica da instituição. Apesar de cada AP suportar no máximo 500 
conexões simultâneas, se for considerada uma taxa de ocupação de 25% de cada 
equipamento para que a rede mantenha um bom desempenho, a solução pode 
suportar até 10.375 dispositivos conectados sem degradar a qualidade do serviço, 
oferecendo escalabilidade e resiliência para se adaptar a grandes demandas de 
conectividade Wi-Fi.
Figura 3. Tráfego na rede sem fio
Diariamente são trafegados em média 1,3TB na rede sem fio. Na Figura 3 
percebe-se um pico de tráfego de 313,59Mbps para download (linha vermelha) e 
58,49Mbps para upload (linha verde). Esses dados definem o throughput da rede. página
138
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 133-139, junho de 2019
Capítulo XXII - Implantação da rede sem fio na UFCA Por fim, foi realizada, entre 22 e 28/03/2018, uma pesquisa de satisfação 
entre os usuários, onde 224 participaram. A Figura 4 mostra a distribuição das 
respostas quanto ao grau de satisfação em relação à rede sem fio. Havia uma 
escala de 1 a 5 para resposta, onde 1 significava muito insatisfeito e 5 significava 
muito satisfeito.
Figura 4. Grau de satisfação do usuário da sem sem fio
 Analisando o gráfico, percebe-se que as respostas 1 e 2 somadas (26,4%) 
representam muita ou alguma insatisfação. Já as respostas 4 e 5 (40,6%) 
representam os usuários que estão satisfeitos ou muito satisfeitos. Já 33,0% 
responderam a opção 3, que representa satisfação média. Para diminuir a taxa 
de avaliação negativa, foram realizados ajustes de desempenho, que incluíram: 
reposicionamento de APs, controle automático de interferência e potência, 
isolação de cliente por AP e por VLAN, whitelist, proxy ARP , switches com portas 
10/100/1000, entre outras.
4. Conclusão
Pelo exposto, a implantação conseguiu atingir o objetivo esperado, 
ampliando a área de cobertura e garantindo uma boa taxa de tráfego. Ferramentas 
de medição de tráfego foram testadas nos dispositivos clientes e foram atingidas 
taxas superiores à 100Mbps.
Os incidentes relacionados a desempenho e disponibilidade foram reduzidos. 
A pesquisa de satisfação com os usuários apresentou dados positivos, já que foi 
uma pesquisa voluntária. Cabe destacar que alguns dados negativos da avaliação 
serão considerados para melhorias da operação e da administração do serviço.
Como trabalhos futuros, estão planejadas as seguintes ações: implantação 
do IPv6, certificação da rede cabeada, segmentação da rede por perfil do usuário, 
implantação de QoS para cobertura (video streaming) de grandes eventos e elevar 
os índices de satisfação da comunidade acadêmica.página
139
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 133-139, junho de 2019
Capítulo XXII - Implantação da rede sem fio na UFCAReferências
Florio, L., & Wierenga, K. (2005, June). Eduroam, providing mobility for roaming users. 
In Proceedings of the EUNIS 2005 Conference, Manchester.
Mishra, A., & Arbaugh, W. A. (2002). An initial security analysis of the IEEE 802.1 X 
standard.
UFCA, Gabinete do Reitor. Portaria N.º 79, de 19 de fevereiro de 2019. Dispõe sobre a 
Rede Sem Fio da Universidade Federal do Cariri.
UFCA. Plano Diretor de Tecnologia da Informação 2015/2016 - PDTI, janeiro de 2015.página
140
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 140-144, junho de 2019
Capítulo XXIII - Implantação do Serviço Eduroam na UFAMImplantação do Serviço Eduroam na UFAM
Marckson M. da Silva¹ , Vanderson da S. Rocha¹ , Crisley P. Linhares¹ , Gerson B. da 
Silva¹ , João B. L. Carneiro¹ , João G. A. Martinez¹
¹Centro de Tecnologia de Informação e Comunicação Universidade Federal do Amazonas (UFAM) 
– Manaus, AM – Brasil
{marcksonms,vanderson,crisleylinhares,gbs,jcarneiro,jgam}@ufam.edu.br
Resumo
O Marco Civil da Internet, Lei promulgada em 2014, tange à importância do monitoramento e a guarda de registros 
de conexão dos usuários da rede de computadores. A Universidade Federal do Amazonas - UFAM, buscando atender 
o Marco Civil e após o processo de adesão à CAFe (Comunidade Acadêmica Federada), implantou o Eduroam 
(education roaming). O acesso sem fio seguro para a comunidade acadêmica, que inclui alunos professores e 
servidores, é oferecido pelo serviço Eduroam.  Este artigo apresenta os trabalhos desenvolvidos para  implantação do 
serviço Eduroam na rede sem fio da Universidade Federal do Amazonas.
Palavras-chave: rede sem fio, CAFe, Eduroam, identidade única.
1. Introdução
A Lei No 12.965/14, em seu décimo terceiro artigo, determina que: “Na provisão 
de conexão à internet, cabe ao administrador de sistema autônomo respectivo, o 
dever de manter os registros de conexão, sob sigilo, em ambiente controlado e de 
segurança, pelo prazo de 1 (um) ano, nos termos do regulamento” [Presidência 
da República, 2014]. Buscando atender a Lei e dando continuidade natural após 
implantação do CAFe, foi disponibilizado o serviço Eduroam.
A rede sem fio da Universidade Federal do Amazonas oferece para a 
comunidade acadêmica o serviço Eduroam que tornou-se um pré-requisito em 
eventos com grande concentração de pesquisadores e estudantes. O Eduroam tem 
as seguintes vantagens [RNP , 2014]:
• Eliminar a necessidade de fornecer contas temporárias a usuários visitantes 
que têm o Eduroam em sua instituição de origem;
• Disponibilizar Wi-Fi no campus para os pesquisadores acessarem a rede 
de maneira rápida, fácil e segura sem a necessidade de recorrer à equipe de 
suporte;
• Fornecer solução única para todos os dispositivos móveis de uma instituição;
• Propiciar um acesso seguro, uma vez que a comunicação é encriptada.
A rede sem fio da UFAM é composta de 313 pontos de acesso das marcas 
Motorola, Zebra e Extreme, com gerenciamento centralizada na controladora página
141
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 140-144, junho de 2019
Capítulo XXIII - Implantação do Serviço Eduroam na UFAMMotorola RFS7000 (RFS7000, 2019), executando WiNG e servidor DHCP (Dynamic 
Host Configuration Protocol) em um servidor PFSense (PFSENSE, 2019), fazendo 
atribuição automática dos endereçamentos IPv4 (Protocolo de Internet versão 4) 
e IPv6 (Protocolo de Internet versão 4) simultaneamente, com a técnica de pilha 
dupla (IPv6 dual stack, 2019). A quantidade  máxima  de  atribuição  de  IPs  do  
servidor  DHCP ,  já  registrada  pelo monitoramento através do aplicativo Zabbix 
(ZABBIX, 2019), foi superior a 7 mil clientes.
Os 313 rádios foram divididos em 3 perfis: Setor norte, sul e metropolitana. 
Alguns pontos de acesso podem ser encontrados no mapa simplificado Eduroam 
da figura 1.
Neste artigo, primeiro foi apresentada a estrutura física e lógica da rede sem 
fio da UFAM. A seção 2 é feita uma introdução do serviço CAFe e CAT Eduroam, 
e a seção 3 é apresentado o monitoramento do serviço usando Zabbix e graylog 
(GRAYLOG, 2019). E por último, é feita a conclusão e a indicação dos possíveis 
trabalhos futuros à serem realizados.
2. Métodos
O Eduroam é um serviço de acesso sem fio seguro desenvolvido para 
a comunidade internacional de educação e pesquisa. O serviço permite que 
estudantes, pesquisadores e a equipe de instituições participantes obtenham 
conectividade à Internet, através de conexão sem fio segura, dentro de seus campi 
e quando visitam as instituições que participam da federação Eduroam, de forma 
transparente (Muchaluat-Saade, 2013).
O primeiro passo da implementação do serviço foi atender os seguintes 
requisitos: já fazer parte da CAFe e atender especificações técnicas de hardware 
e software para o Eduroam. Em seguida, foi dada continuidade ao processo de 
adesão com o envio de formulários e homologação técnica junto à operadora da 
federação Eduroam-Br que é a RNP (Rede Nacional de Ensino e Pesquisa).
A adesão à CAFe fez a substituição do serviço de proxy institucional, que 
permite aos usuários da UFAM acessar aos periódicos da CAPES (Coordenação 
de Aperfeiçoamento de Pessoal de Nível Superior) quando se encontram fora do 
domínio institucional. Antes, era necessário a configuração manual do proxy em 
cada dispositivo no qual se quisesse fazer uso deste serviço. Com a implantação 
da federação CAFe finalizada, o acesso aos periódicos da CAPES passou a ser feito 
de forma transparente, apenas necessitando que o usuário esteja cadastrado e 
ativo na base de dados institucional.
Com a existência de diversos sistemas e atividades acadêmicas (pós-
graduação, pesquisa e extensão) sendo acessados/realizadas dentro ou fora do 
domínio institucional, o próximo passo foi tornar a federação CAFe a infraestrutura 
de acesso única dos serviços, começando com a internet sem fio (adesão ao projeto 
Eduroam) e gradualmente aplicando-a aos demais serviços.página
142
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 140-144, junho de 2019
Capítulo XXIII - Implantação do Serviço Eduroam na UFAMA base do serviço Eduroam é o servidor Radius (Remote Authentication Dial-In 
User Service). A RNP fornece um roteiro de instalação do servidor IdP do Eduroam 
com FreeRadius V3 (RNP wiki, 2019). Na Instituição, o processo se deu com 
instalação do FreeRadius v3 em uma máquina virtual Ubuntu e em seguida com 
a execução de um script fornecido pela RNP para configuração do serviço. Com o 
FreeRadius em funcionamento, foi feita a integração do servidor com controladora 
WiFi, para distribuição da configuração para mais de 300 rádios da UFAM.
O serviço Eduroam foi implantado em mais de 300 pontos de acesso que 
podem ser encontrados no mapa simplificado de localização do serviço da figura 
1. Para o usuário configurar seu dispositivo na rede Eduroam ele deve baixar o 
aplicativo CAT (Configuration Assistant Tools)1 e seguir os passos da instalação.
Figura 1. Pontos de acesso Eduroam em Manaus (GÉANT, 2019).
3. Resultados
O monitoramento dos ativos de rede é feito usando o aplicativo Zabbix. Ele 
mostra, por exemplo, quando um equipamento de ponto de acesso Eduroam fica 
offline e a quantidade de usuários por ponto de acesso, como exemplificado no 
gráfico do Zabbix da Figura 2, onde é visualizado o valor máximo de 46 usuários 
e média de 4 usuários em 3 meses de monitoramento, para o ponto de acesso 
“AP_CTIC-1Andar-1” . Ainda através da controladora, pode-se obter quais os tipos 
de aplicações ou serviços são acessados por esses clientes. Com o filtro de logs  
do eduroam é possível rastrear os clientes por MAC (Media Access Control) e os 
pontos de acesso, como no exemplo da figura 3, onde o usuário 8243997@usp.
br com MAC 38:9A:F6:D4:2C:2 acessou com Eduroam pelo ponto de acesso de IP 
10.15.4.124. Esses logs são indexados pelo GrayLog para futura auditoria.
Figura 2. Quantidade de usuários em um radio em 3 meses.
1 https://cat.eduroam.org/página
143
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 140-144, junho de 2019
Capítulo XXIII - Implantação do Serviço Eduroam na UFAMOs logs do freeradius também são indexados na base do GrayLog, como 
mostrado no dashboard na figura 4, onde, por exemplo, pode-se confirmar que há 
usuários fazendo roaming com Eduroam.
Figura 3. GrayLog guardando logs do Eduroam.
Figura 4. Dashboard do GrayLog com logs do Eduroam.
4. Conclusão
Com a implementação do CAFe e Eduroam e o tratamento de logs do servidor 
freeradius guardados na base do GrayLog, foi possível atender a Lei No  12.965/14 - 
Marco Civil da Internet. Assim, a equipe de infraestrutura de TI está apta a responder 
solicitações de identificação de utilizadores da rede wifi Eduroam. Ainda com a 
federação da UFAM nossos usuários podem usar wifi em qualquer lugar que tenha 
rede Eduroam.
A implementação da autenticação federada para rede wifi atende a todos 
os usuários da federação Eduroam, mas não os visitantes que não fazem parte da 
federação. Por esse motivo, ainda existe na Instituição uma rede wifi aberta, não 
sendo possível, nesse caso atender ao Marco Civil da Internet.página
144
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 140-144, junho de 2019
Capítulo XXIII - Implantação do Serviço Eduroam na UFAMUm próximo passo será modelar um processo para os usuários convidados, 
sem cadastro na federação, e estudar uma forma de implementar o padrão IEEE 
802.1x (Institute of Electrical and Electronics Engineers) na rede cabeada, que 
atenda a realidade dos usuários da UFAM.
Referências
GÉANT Association. “Global eduroam map, Supporting services” .   Disponível em:
<https://monitor.eduroam.org/map_service_loc.php>. Acesso em fev. 2019.
GRAYLOG, 2019 – Log Management. Disponível em: <https://www.graylog.org/>. 
Acesso em fev. 2019.
IPv6 dual stack, 2019 – Pilha Dupla Disponível em: <http://ipv6.br/post/transição/>. 
Acesso em fev. 2019.
Muchaluat-Saade, Debora Christina; Carrano, Ricardo ; SILVA, E. F. ; MAGALHÃES, Luiz 
Claudio Schara . “Eduroam: Acesso sem Fio Seguro para Comunidade Acadêmica 
Federada” . 1. ed. Rio de Janeiro: Escola Superior de Redes, 2013. v. 1. 162p.
Presidência da República. LEI No  12.965, de 23 de abril de 2014.   Disponível em:
<http://www.planalto.gov.br/ccivil_03/_ato2011-2014/2014/lei/l12965.htm>.   Acesso 
em fev. 2019.
PFSENSE, 2019 – Open Source Security. Disponível em: <https://www.pfsense.org/>. 
Acesso em fev. 2019.
RFS7000,     2019     –     Motorola     Wing     RFS     7000. Disponível     em:
<https://www.archimedes.gr/files/products/pdfs/wing-rfs-7000-datasheet.pdf>. 
Acesso em fev. 2019.
RNP – Rede Nacional de Ensino e Pesquisa. “Leve o eduroam para sua instituição” . 
Disponível em:<https://www.rnp.br/services/advanced-services/eduroam/leve-
eduroam-  sua-instituição>. Acesso em fev. 2019.
RNP wiki – Rede Nacional de Ensino e Pesquisa. “Roteiro de instalação do servidor 
IdP do eduroam com FreeRadius V3” . https://wiki.rnp.br/pages/viewpage.action?  
pageId=107873261. Acesso em fev. 2019.
ZABBIX, 2019 – open-source monitoring software tool for diverse IT components. 
Disponível em: <https://www.zabbix.com/>. Acesso em fev. 2019.página
145
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 145-150, junho de 2019
Capítulo XXIV - Integração das Bases de Login e Senha dos Sistemas da Universidade de Brasília 
- UnBIntegração das Bases de Login e Senha dos 
Sistemas da Universidade de Brasília - UnB
Renato Carauta Ribeiro¹, Everton de Vargas Agilar²
¹Centro de Processamento de Dados (CPD) – Universidade de Brasília (UNB)
Distrito Federal – DF – Brazil
Abstract
In recent years, the modernization of legacy systems has been a priority for the Universidade de Brasília (UnB). This 
paper presents the challenges encountered and the solutions proposed by the CPD/UnB architecture team to integrate 
three different user databases related to the legacy systems of UnB (which has been in use for more than 20 years), 
the Electronic Information System (SEI) and the new Administrative, Personnel and Academic Management System 
acquired from the Universidade Federal do Rio Grande do Norte (UFRN) through a cooperation agreement.
Resumo
Nos últimos anos, a modernização dos sistemas legados tem sido prioridade para a Universidade de Brasília (UnB). 
Este artigo apresenta os desafios encontrados e as soluções propostas pela equipe de arquitetura do CPD/UnB para 
realizar a integração de 3 bases de dados de usuários distintas referentes aos sistemas legados da UnB (que estão 
em uso a mais de 20 anos), o sistema Eletrônico de Informações (SEI) e o novo Sistema de Gestão Administrativo, 
Pessoal e Acadêmico adquirido da Universidade Federal do Rio Grande do Norte (UFRN) por meio de um acordo de 
cooperação.
1. Introdução
Há mais de 20 anos, o Centro de Informática da Universidade de Brasília 
(UnB) desenvolve aplicações nas linguagens VB e Java que consolidam a maior 
parte das informações corporativas da Instituição. Como são sistemas legados 
com pouca documentação e que já não estão mais alinhados com as necessidades 
da Instituição, a UnB decidiu modernizá-los visando promover maior eficiência em 
sua gestão. 
Devido ao corpo técnico reduzido do CPD/UnB, a estratégia de modernização 
adotada na UnB tem sido implantar sistemas já prontos, quando possível, 
e desenvolver apenas os sistemas com características muito específicas da 
Instituição.
Dentre os sistemas implantados há mais de dois anos, pode-se citar o sistema 
Eletrônico de Informações (SEI) que substituiu o antigo sistema UnBDoc. O SEI 
foi concebido pelo Tribunal Regional Federal da 4ª Região (TRF4) para gerir os 
documentos e os processos eletronicamente. O segundo em fase de implantação 
é o sistema de gestão da Universidade Federal do Rio Grande do Norte (UFRN) 
visando  substituir todos os sistemas legados em VB e Java da UnB que fazem a 
gestão administrativa, pessoal e acadêmica da Universidade. Na UnB, os sistemas 
da UFRN passou a ser denominado de SigUnB.
Com a implantação de sistemas de terceiros desenvolvidos fora da UnB, 
a equipe de arquitetura do CPD/UnB precisou prover uma forma de integrar as página
146
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 145-150, junho de 2019
Capítulo XXIV - Integração das Bases de Login e Senha dos Sistemas da Universidade de Brasília 
- UnBbases de dados de usuários desses sistemas com as bases de usuários dos demais 
sistemas da UnB tendo como objetivo a unificação das contas de logins dos 
usuários para tentar impedir que estes tenham contas de login diferentes em cada 
sistema acessado. 
Nesse sentido, foi proposto e implementado no CPD/UnB uma solução 
para integrar as referidas bases por meio de um barramento de serviço com a 
implementação de dois serviços de autenticação: o serviço de autenticação Proxy 
LDAP e o serviço de autenticação OAuth2.
Este artigo apresenta os trabalhos executados pelo CPD/UnB durante os anos 
de 2017 e 2018 para unificar os logins dos usuários da Universidade de Brasília e 
discute os principais desafios encontrados no decorrer do desenvolvimento dos 
serviços de integração e autenticação dos usuários da UnB.
2. Fundamentação Teórica
A arquitetura do barramento de serviços desenvolvido pelo CPD/UnB para 
integração das bases de dados de logins dos usuários da UnB baseia-se nos conceitos 
de microsserviços. Microsserviços, como visto em [Agilar 2016], é uma abordagem 
para desenvolver aplicativos como um conjunto de pequenos serviços fracamente 
acoplados, cada um executando em seu próprio processo e comunicando-se com 
mecanismos leves, geralmente através de API de recursos HTTP .
Com base nessa arquitetura, os protocolos LDAP e OAuth2 foram escolhidos 
por serem abertos e amplamente utilizados no Brasil. O protocolo LDAP é um 
protocolo de aplicação muito utilizado na indústria para fornecer um login único 
onde uma senha para um usuário é compartilhada entre muitos serviços ou 
sistemas [Kumar 2003, Sucasas et al. 2016]. No caso do protocolo OAuth2, este é 
um padrão de autenticação amplamente utilizado em aplicações que não devem 
manipular diretamente nomes de usuários e senhas, como as aplicações Web 
[Ribeiro 2017].
A principal razão para a escolha de uma abordagem de microsserviços foi  
permitir a implementação dos serviços de integração e autenticação independente 
dos demais sistemas da UnB. Além disso, um fator importante foi a independência 
da linguagem de programação visto os sistemas de terceiros variam muito de 
linguagem de programação tais como PHP , Java entre outros.
3. Solução Proposta
A solução da integração das bases de usuários com login unificado iniciou 
com a prototipação da primeira versão do serviço denominado proxy LDAP no 
barramento de serviços. Esse protótipo foi desenvolvido em pouco mais de três 
semanas sendo apresentado para a equipe de implantação do SEI, que na época 
optou por não utilizá-lo, pois o conceito de um proxy LDAP , ou seja, um processo 
que entende o protocolo LDAP , mas acessa os dados de uma fonte externa como página
147
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 145-150, junho de 2019
Capítulo XXIV - Integração das Bases de Login e Senha dos Sistemas da Universidade de Brasília 
- UnBum banco de dados relacional, ainda não estava madura o suficiente e, também, 
não se sabia ao certo se conseguiria escalonar na quantidade de acessos que o SEI 
teria. 
Por este motivo, o proxy LDAP acabou sendo utilizado somente na versão 2.0 
do software SEI na UnB. Assim, na versão 1.0, o CPD/UnB precisou alterar o código 
fonte PHP do SEI para fazer a autenticação dos usuários diretamente no banco 
de dados dos sistemas legados da UnB. Como consequência, não conseguimos 
mantê-lo atualizado com as releases da primeira versão do SEI (que foram sendo 
lançados pelo TRF4 para correções e inclusão de novas funcionalidades) pois 
sempre que uma nova release era lançada, era necessário revisar o código fonte 
do SEI e incluir as mudanças realizadas pelos desenvolvedores do CPD. 
Durante o uso do proxy LDAP , percebeu-se que o proxy poderia ser bem 
mais escalável se não precisasse ir nos bancos de dados a cada requisição de 
autenticação. Note que inicialmente, a cada requisição de autenticação do 
usuário, o barramento precisava ir até a fonte de dados para autenticar o usuário 
sendo necessários dois acessos em bases separadas, uma na base dos servidores 
da UnB e a outra na base dos alunos, ambos em SQL-Server. Pensando nisso, foi 
proposto um subsistema no barramento denominado data-loaders. Tal subsistema 
deveria executar processos em  background no barramento de serviços para cada 
fonte de dados externa com o objetivo de fazer a extração dos dados dos usuários 
cadastrados e realizar o tratamento desses dados, conforme necessário, para 
unificar os registros e se possível, evitar as duplicações de cadastros.
De modo simplificado, a extração dos dados se dá quando o processo em 
execução se conecta na base de dados via uma conexão ODBC e carrega os dados 
para o barramento de serviço, onde posteriormente é realizado o tratamento e a 
validação dos dados. Após o carregamento ocorre a geração de várias tabelas hash, 
uma para cada fonte de dados com os dados dos usuários bem como os perfis 
e permissões de acesso. Esse processo é feito continuamente em background de 
forma incremental.
Pode se dizer, que o desenvolvimento do subsistema de data-loaders foi 
uma das partes mais complexas da implementação pois para fazer a sincronização 
incremental dos dados de usuários de várias bases de dados (atualmente, está 
sendo utilizado SQL-Server nos sistemas legados e no SEI, e o PostgreSQL para 
o sistema SigUnB), foi necessário identificar quando um usuário era incluído, 
alterado ou mesmo inativado. Para resolver essa questão, implementou-se um 
esquema de checkpoint nos processos data-loaders, onde de tempos em tempos 
(definidos através de parametrizações nos contratos de serviços do data-loader), 
o barramento consulta as base de dados externa para buscar os usuários. Esse 
esquema de checkpoint permitiu ao barramento de serviços realizar o acesso às 
bases de dados de tempos em tempos de forma incremental e em segundo plano, 
em vez de a cada autenticação como foi na primeira versão do proxy LDAP .
Com o proxy LDAP em produção, a equipe do CPD/UnB começou a trabalhar 
na autenticação OAuth2 já que alguns sistemas web estão sendo desenvolvidos 
para suprir necessidades não contempladas no sistema de gestão da UFRN. Uma página
148
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 145-150, junho de 2019
Capítulo XXIV - Integração das Bases de Login e Senha dos Sistemas da Universidade de Brasília 
- UnBversão preliminar do serviço OAuth2 foi implementada e pode ser conferida em 
[Ribeiro 2017]. Com base nessa versão preliminar, o CPD integrou ao núcleo do 
barramento de serviços reutilizando o mesmo código de autenticação do proxy 
LDAP .
O próximo requisito acrescentado no barramento de serviços foi a 
funcionalidade de auto-cadastro para garantir que os dados dos usuários estejam 
presentes nas bases de dados dos sistemas de terceiros que possuem seu próprio 
banco de dados. Por exemplo, quando um usuário é cadastrado no sistema de 
pessoal da UnB, ele precisa ser cadastrado na base de dados do SEI para que o 
sistema funcione corretamente.
Para exemplificar melhor o auto-cadastro, note que, geralmente quando se 
utiliza um serviço de autenticação como o LDAP , o sistema tem sua própria tela de 
login. Nesse caso, o trabalho do serviço de autenticação resume-se a autenticar 
o usuário (verificar se o login e a senha existem), mas o sistema que o usuário vai 
acessar, a exemplo do SEI, também precisa ter informações sobre os usuários em 
sua base de dados. A funcionalidade de auto-cadastro serve justamente para essa 
necessidade e funciona da seguinte forma: quando o usuário é cadastro em uma 
fonte de dados, o barramento pode cadastrá-lo em outra fonte de dados baseado 
em várias parametrizações incluídas no serviço de auto-cadastro implementado. 
Uma dessas parametrizações é somente cadastrar no SEI usuários cadastrados no 
sistema SigUnB e nunca cadastrar um usuário no SigUnB que foi cadastrado em 
um sistema legado uma vez que o SigUnB obtém a lista de usuários a partir do 
processamento da fita espelho do Sigepe todo mês.
Para finalizar a seção, a figura 1 exibe o esquema de autenticação para os 
sistemas da UnB atualmente, sejam eles de terceiros ou desenvolvidos pelo CPD/
UnB. A partir de agora, são três formas de realizar a autenticação dos usuários: via 
LDAP e OAuth2 sendo os dois métodos padronizados e, por fim, o acesso direto ao 
banco de dados para os sistemas que não suportam nenhum dos protocolos, como 
é o caso dos sistemas legados.página
149
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 145-150, junho de 2019
Capítulo XXIV - Integração das Bases de Login e Senha dos Sistemas da Universidade de Brasília 
- UnB
Figure 1. Autenticação dos sistemas da UnB
4. Conclusão
Com a integração das bases de dados para um login unificado é possível 
facilitar a autenticação dos usuários nos diversos sistemas da UnB, sejam aqueles 
desenvolvidos pela próprio CPD bem como os sistemas implantados de terceiros. 
Essa integração possibilita que os usuários possam ter um login único independente 
do sistema que está sendo acessado e de forma transparente, mesmo sabendo 
que os dados de usuários estão replicados em várias bases de dados. 
O proxy LDAP é utilizado em diversos sistemas além do SEI, como por exemplo, 
o Redmine (Sistema de gerenciamento de projetos) e os sistemas desenvolvidos 
pelo departamento de sites do CPD/UnB. O proxy LDAP tem sido visto pela Direção 
do CPD/UnB como uma ferramenta com alto potencial para integrar não somente 
os sistemas de gestão mas o Webmail e a UnB Wireless da Universidade.
Foi possível constatar que a maior vantagem de se utilizar o proxy LDAP e 
não uma instância OpenLDAP convencional é permitir que as bases de usuários 
possam estar em um banco de dados relacional onde tipicamente os sistemas da 
própria Instituição já tem acesso direto. Os demais sistemas, sites institucionais e 
gerenciadores de conteúdo (Joomla, Wordpress, etc.) podem consultar e autenticar 
os usuários através do serviço proxy LDAP desconhecendo o protocolo OpenLDAP .
Os data-loaders são um recurso que provê muito desempenho já que o 
barramento não vai nas fontes de dados externas durante a autenticação dos 
usuários, mas seu uso acrescentou vários problemas de sincronização de usuários 
devido ao uso excessivo de ram. Por vezes, os processos dos data-loaders paravam 
de sincronizar ou demoravam muito para fazer a sincronização sendo necessário página
150
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 145-150, junho de 2019
Capítulo XXIV - Integração das Bases de Login e Senha dos Sistemas da Universidade de Brasília 
- UnBreiniciar a instância do barramento. Nós conseguimos diminuir o uso de ram de 6G 
para pouco mais de 2G e atualmente este subsistema está funcionando bem, mas 
melhorias estão sendo realizadas para minimizar o tempo entre as sincronizações 
que atualmente é, em média, de minuto a minuto. 
 O serviço de autenticação OAuth2 também tem trazido benefícios sendo 
o principal o uso de um protocolo documentado e aberto que permite que novos 
serviços possam ser publicados e facilmente consumidos por qualquer aplicação 
independente da linguagem de programação. Outro benefício constatado é a 
facilidade para disponibilizar webservices para outros clientes com mais segurança 
já que o login e a senha do usuário não é enviada entre as requisições HTTP/REST .
 Por fim, a adoção da abordagem de microsserviços pelo CPD/UnB está 
permitindo a criação de serviços no barramento um poucos mais fáceis de realizar 
manutenção e evoluir se comparado as demais abordagens utilizadas. Um resumo 
dessas experiências foi documentado em [Luiz et al. 2018].
Referências
Agilar, E. d. V. (2016). Uma abordagem orientada a serviços para a modernização de 
sistemas legados.
Kummar, A. (2003). The openldap proxy cache. IBM, India research Lab, at least as 
early as May.
Luiz, W., Agilar, E., de Oliveira, M. C., de Melo, C. E. R., Pinto, G., and Bonifácio, R. (2018). 
An experience report on the adoption of microservices in three brzilian govermment 
institutions. In Proceedings of the XXXII Brazilian Symposium on Software Engineering, 
SBES ’ 18, pages 32-41, New York, NY, USA. ACM.
Ribeiro, A. d. S. (2017). Uma implementação do protocolo oauth 2 em erlang para uma 
arquitetura orientada a serviços.
Sucasas, V., Mantas, G., Radwan, A., and Rodriguez, J. (2016). An oauth2-based protocol 
with strong user privacy preservation for smart city mobile e-health apps. In 2016 IEEE 
International Conference on Communications (ICC), pages 1-6.página
151
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 151-156, junho de 2019
Capítulo XXV - Integração entre LDAP e FreePBX para controle de autorização de chamadasIntegração entre LDAP e FreePBX para controle 
de autorização de chamadas
Eliézer de Siqueira, Weber S. R. Takaki
Divisão de Infraestrutura de Redes e Telefonia
Universidade Federal da Integração Latino-Americana (UNILA)
Caixa Postal 2044 – Foz do Iguaçu – PR – Brasil
eliezer.siqueira@unila.edu.br, weber.takaki@unila.edu.br
Resumo
Devido ao rápido crescimento da Universidade, a área de TI da UNILA identificou a necessidade de aperfeiçoar o 
controle de permissões e a auditoria de chamadas telefônicas. Nesse cenário, foi preciso desenvolver uma solução 
confiável para garantir que apenas pessoas autorizadas pudessem efetuar chamadas para celular e de longa distância. 
Desse modo, a solução envolveu a integração do serviço de diretórios institucional (LDAP1) e a central telefônica 
Asterisk, esta última configurada para somente autorizar chamadas mediante a apresentação do pin, o qual se 
encontra vinculado ao usuário por intermédio do serviço LDAP. A solução desenvolvida apresentou desempenho 
satisfatório, e cumpriu com os requisitos estabelecidos inicialmente. 
Palavras-chave: Telefonia VoIP; LDAP; FreePBX.
1. Introdução
Chamadas telefônicas para celular e de longa distância representam a maior 
parte do consumo mensal de telefonia na Unila. Desde sua inauguração em 2010, 
a telefonia esteve sob responsabilidade da TI devido ao pequeno número de 
servidores no quadro da instituição. Com o crescimento da Universidade e das 
demandas por melhor gestão dos recursos de TI, a equipe de Redes e Telefonia 
planejava aprimorar a solução de autorização dessas chamadas. Atualmente, a 
autorização para realizar chamadas nessas categorias é aplicada ao ramal, porém 
essa forma de administrar apresenta alguns problemas:
1. Pessoas não autorizadas (alunos, visitantes e outros) podem realizar 
chamadas de longa distância se souberem quais ramais possuem permissão;
2. Impossibilidade de identificar o usuário que realiza cada chamada;
3. O responsável pelo ramal não tem como saber se há chamadas indevidas 
sendo realizadas pelo seu ramal.
Com objetivo de dar mais controle ao gestor de área e flexibilizar o ponto 
de origem das chamadas que requerem autorização específica, implantou-se na 
central telefônica IP o sistema de autorização de chamadas mediante o uso de pins  
ou senhas numéricas. Paralelamente a isso foram desenvolvidos (1) um mecanismo 
1 Lightweight Directory Access Protocol.página
152
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 151-156, junho de 2019
Capítulo XXV - Integração entre LDAP e FreePBX para controle de autorização de chamadasde atribuição individual e centralizado de senhas numéricas válido para serviços 
de TI (impressão e telefonia) vinculado ao serviço de diretório institucional (LDAP) 
[Sermersheim 2006], e (2) um mecanismo de notificação por e-mail de uso do pin 
individual para chamadas telefônicas. Desse modo, a nova forma de autorização:
1. Minimiza as chances de pessoas não autorizadas realizarem chamadas que 
exijam autorização específica;
2. Permite que os responsáveis pelas chamadas sejam identificados 
individualmente;
3. Comunica o responsável pelo pin2 sempre que este for utilizado em alguma 
chamada. 
Além desses benefícios, cada servidor devidamente autorizado pode realizar 
chamadas de qualquer ramal, esteja em sua sala de trabalho ou fora dela. Essa 
possibilidade confere um grau de flexibilidade e praticidade maior à execução de 
suas tarefas.
2. Métodos
A configuração de pins para autorização de chamadas na central VoIP3 pode 
ser feita diretamente pela interface de administração do FreePBX [FreePBX 2019]. 
De fato, essa é a única forma de configuração disponibilizada pelo FreePBX. No 
entanto, ela possui duas limitações:
1. Não vincula o pin a um usuário, e consequentemente não restringe o uso 
de pins repetidos;
2. Não permite verificar se o pin cadastrado obedece a certas regras de 
segurança para senhas numéricas.
Além das limitações citadas, os pins atribuídos a cada servidor e registrados 
no LDAP são também utilizados para autorizar e contabilizar o uso do sistema de 
impressão institucional. Portanto, a mesma senha já utilizada pelos usuários para 
“liberar” impressões e cópias em qualquer impressora de rede da universidade, 
também será usada para “liberar” chamadas de longa distância ou para celulares 
a partir de qualquer ramal. Essa integração entre os serviços de impressão e 
telefonia por meio do serviço de diretório melhora a experiência do usuário e 
facilita a administração dos recursos. 
Antes de modelar e implementar a solução, a qual está ilustrada no diagrama 
da Figura 1, verificou-se a disponibilidade de ferramentas ou módulos já existentes. 
Nesse sentido, o servidor Asterisk [Keller 2011][Asterisk 2019] possui o módulo 
res_config_ldap, que permite a integração com um servidor LDAP por meio da 
Asterisk Realtime Architecture (ARA)[Madsen et al. 2011]. Porém, considerando que 
2 Senha numérica.
3 Voice over Internet Protocol.página
153
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 151-156, junho de 2019
Capítulo XXV - Integração entre LDAP e FreePBX para controle de autorização de chamadaso uso de pins para autorização de chamadas não é uma funcionalidade do servidor 
Asterisk, mas um conjunto de regras e macros desenvolvidas e disponibilizadas 
pelo FreePBX, o uso desse módulo não foi possível no nosso cenário. Os pins  
são atribuídos automaticamente no cadastro do usuário no serviço de diretório 
(LDAP). Há uma rotina de sincronização que busca diariamente os dados de novos 
servidores das bases do Sistema Integrado de Gestão (SIG) e insere o cadastro 
desse novo servidor no LDAP para que o usuário passe a ter acesso à rede e aos 
serviços de TI institucionais. Essa mesma rotina de sincronização atribui um pin 
disponível na lista de pins que ainda não foram alocados.
Os pins são atribuídos automaticamente no cadastro do usuário no serviço de 
diretório (LDAP). Há uma rotina de sincronização que busca diariamente os dados 
de novos servidores das bases do Sistema Integrado de Gestão (SIG) e insere o 
cadastro desse novo servidor no LDAP para que o usuário passe a ter acesso à rede 
e aos serviços de TI institucionais. Essa mesma rotina de sincronização atribui um 
pin disponível na lista de pins que ainda não foram alocados.
Figura 1. Diagrama da solução
Para a integração da central telefônica com o servidor LDAP , foi escrito um 
programa em linguagem C ANSI, que consulta a base de usuários TAEs4 e docentes, 
e gera três listas de pins, sendo uma para cada nível de permissão. As listas são 
então atualizadas na central telefônica por meio da atualização dos registros da 
tabela pintsets. O servidor FreePBX oferece uma interface de configuração de pins  
individuais, porém para a atualização em massa é necessário o acesso direto às 
tabelas de configuração no banco de dados. O programa de atualização é executado 
periodicamente por meio do serviço cron.
4 Técnicos Administrativos em Educação.página
154
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 151-156, junho de 2019
Capítulo XXV - Integração entre LDAP e FreePBX para controle de autorização de chamadasAlém de atualizar a relação de pins da central telefônica, o programa atualiza 
(ou gera caso não exista) uma tabela de correspondências entre pins e endereços de 
e-mail. Essa tabela é necessária para que o proprietário do pin possa ser notificado 
sempre que houver chamadas registradas na central, associadas a seu usuário.
O tempo de execução do programa de atualização foi medido em diferentes 
momentos da manhã e da tarde, a intervalos de aproximadamente uma hora, 
em dia útil. O tempo total foi medido por meio do comando time do sistema 
operacional Linux. O perfil de execução com os tempos parciais foram medidos por 
meio da impressão de timestamps5 a cada passo do processo de atualização. Os 
timestamps foram obtidos por meio de função disponível na biblioteca sys/time.h, 
com precisão de microssegundos, e somente são registrados se o programa for 
executado com opção de depuração ativa.
Para completar o ciclo idealizado inicialmente, informando os usuários 
quanto ao uso de seu pin para autorização de chamadas, um script de notificação 
foi desenvolvido em linguagem PERL [Perl 2019]. Esse script consulta a base de 
dados de registros de chamadas (CDR - Call Detail Record) mantida pela central 
Asterisk, e obtém todas as chamadas efetuadas que foram completadas no último 
período e que não foram ainda notificadas. A partir do resultado da busca, o script 
elabora um “extrato” das chamadas agrupadas por pin informando data e hora, 
ramal de onde foi originada a chamada, número discado e duração, além do total 
de chamadas realizadas e o período considerado (Figura 2).
Figura 2. Exemplo de e-mail de notificação
5 Carimbos de tempo.página
155
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 151-156, junho de 2019
Capítulo XXV - Integração entre LDAP e FreePBX para controle de autorização de chamadas3. Resultados
O tempo médio de execução do programa de atualização dos pins para a base 
de dados da central telefônica foi de 1,82 segundos, com desvio padrão de 1,23 
segundos. A maior parte do tempo de execução corresponde às ações de consulta 
ao serviço LDAP . Somadas, as operações de bind no serviço (14,35%), consulta aos 
usuários TAEs (48,01%) e consulta aos usuários docentes (35,23%) correspondem 
a 97,06% do tempo de execução do programa. O total de pins lidos foi de 1.030 (mil 
e trinta), sendo 580 de usuários TAE, e 450 de usuários docentes.
O envio do e-mail com as chamadas efetuadas para cada usuário é feito 
diariamente ao final do dia. Mesmo chamadas que não geraram custo, ou seja, 
chamadas com duração igual a zero, são incluídas no relatório pois podem ser 
úteis como alerta ao responsável em caso de uso do pin por outra pessoa.
As credenciais de acesso ao banco de dados utilizadas pelo programa de 
atualização de pins e pelo script de notificação, possuem permissões para realizar 
apenas as tarefas a que se destinam. Desse modo a exposição dos registros de 
chamadas, e das configurações de operação da central telefônica, ficam limitadas 
ao mínimo necessário, favorecendo a segurança do ambiente operacional do 
sistema de telefonia.
A solicitação do pin após discagem para um número, bem como a verificação 
das permissões a ele atribuídas, ocorrem de modo instantâneo.
Atualmente a integração encontra-se em testes nos ramais da Coordenadoria 
de Tecnologia da Informação, englobando um total de 33 ramais configurados para 
somente efetuar chamadas mediante a informação do pin particular.
A solução vem se mostrando totalmente estável e de fácil utilização por parte 
dos usuários. O e-mail diário com o relatório auxilia no controle pessoal e facilita 
a identificação da utilização indevida do pin por terceiros, assim como também 
viabiliza a auditoria de chamadas quando necessário.
Tendo em vista a impossibilidade no sistema atual de se levantar o 
quantitativo de chamadas efetuadas por usuários sem autorização em ramais 
autorizados, não há como prever os gastos que são gerados e consecutivamente 
calcular uma previsão na redução de despesas através da utilização do novo 
sistema. Para obter um possível valor de reduções, será necessário implantar o 
sistema em toda Universidade e comparar os custos ao longo do tempo com as 
faturas anteriores. Ainda assim, espera-se alcançar alguma economia devido ao 
refinamento da segurança.página
156
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 151-156, junho de 2019
Capítulo XXV - Integração entre LDAP e FreePBX para controle de autorização de chamadas4. Conclusão
A equipe de Redes e Telefonia estima para o final do primeiro semestre de 
2019 a implantação do sistema para toda a Universidade. Os resultados mostraram 
estabilidade e desempenho satisfatório da solução, que atendeu aos requisitos 
previstos inicialmente. Trabalhos futuros incluem a integração entre o Sistema 
Integrado de Gestão de Recursos Humanos - SIGRH e o serviço de diretórios no 
sentido de executar a revogação automática das permissões (caso possua) sempre 
que o servidor for realocado para outro setor. Dessa forma evitar-se-á que o usuário 
mantenha autorizações em setores nos quais elas não sejam necessárias.
Referências 
Asterisk (2019). Open source communications software. Acesso em: 15 abr 2019. 
Disponível em: https://www.asterisk.org/. FreePBX (2019). 
FreePBX: let freedom ring. Acesso em: 15 abr 2019. Disponível em: https://www.
freepbx.org/. 
Keller, A. (2011). Asterisk na prática. Novatec, 1th edition.
Madsen, L., Meggelen, J. V., and Bryant, R. (2011). Asterisk: The Definitive Guide. 
O’Reilly, 3 edition.
Perl (2019). The perl programming language. Acesso em: 15 abr 2019. Disponível em: 
https://www.perl.org/.
Sermersheim, J. (2006). Lighweight directory access protocol (LDAP): The protocol. 
Request for Comments: 4511. Acesso em: 15 abr 2019. Disponível em: https://tools.
ietf.org/html/rfc4511.página
157
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 157-163, junho de 2019
Capítulo XXVI - Melhoria Organizacional com Base no Grau de Felicidade dos ColaboradoresMelhoria Organizacional com Base no Grau de 
Felicidade dos Colaboradores
Teresa M M Maciel¹,², Suzanna Sandes¹
¹Núcleo de Tecnologia da Informação – Universidade Federal de Pernambuco (NTI-UFPE)
Recife – PE– Brasil
²Universidade Federal Rural de Pernambuco (UFRPE)
Recife –PE– Brasil
{tmmaciel@gmail.com, suzanna.sandes@ufpe.br} 
Resumo
Este artigo relata uma iniciativa que está em andamento desde dezembro de 2018 no Núcleo de Tecnologia da 
Informação da UFPE, que consiste na implantação de uma técnica conhecida como Radar da Felicidade, a qual 
tem se mostrado uma boa ferramenta para se obter visibilidade sobre o grau de satisfação dos funcionários, o que 
reflete diretamente no engajamento e produtividade . O artigo apresenta uma visão geral da técnica e relata como 
está sendo a sua adoção pelo NTI-UFPE, incluindo a implantação, condução e análise dos resultados, ressaltando 
os desafios e benefícios alcançados.
Palavras-chave: melhoria organizacional, medição da satisfação
1. Introdução
O grau de engajamento dos indivíduos vem sendo cada dia mais considerado 
como um fator crítico de sucesso para o alcance de resultados organizacionais. 
Tanto no seguimento privado quanto no setor público, observa-se uma adesão 
crescente a métodos que promovem a satisfação dos colaboradores como item 
fundamental para maximização da produtividade. Métodos ágeis explicitam que 
projetos devem ser desenvolvidos em torno de indivíduos motivados [Fowler, 
2001]. Alinhados com esta visão, modelos de gestão ágil, como o Management 3.0 
[Appelo, 2010], priorizam práticas de gestão que dão ênfase na maneira como as 
pessoas se comportam.
O Núcleo de Tecnologia da Informação da UFPE (NTI-UFPE) vem inserido 
práticas ágeis em sua área de sistemas há alguns anos, tendo ampliado seu 
escopo de uso para o âmbito da gestão organizacional desde 2016, especialmente 
com o uso sistemático da técnica OKR (do inglês Objective & Key-results) [Grove, 
1983]. Em 2018 houve uma ação estratégica para o desenvolvimento da agilidade 
organizacional com a criação de uma área especificamente preocupada com o 
desenvolvimento da cultura ágil e inovadora. Neste sentido, a adoção de práticas 
como gestão à vista, gerenciamento participativo, feedback entre outras vem 
sendo gradativamente colocadas em prática no Núcleo.
Com foco em obter visibilidade do sentimento dos colaboradores com 
relação ao seu grau de satisfação no NTI e usar esta percepção para a melhoria página
158
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 157-163, junho de 2019
Capítulo XXVI - Melhoria Organizacional com Base no Grau de Felicidade dos Colaboradoresorganizacional, o NTI-UFPE priorizou a adoção de uma dinâmica baseada em 
uma das ferramentas recomendadas pelo modelo Management 3.0, a “Happiness 
Door” [Appelo, 2016], a qual se mostrou um mecanismo simples de feedback para 
um trabalho de melhoria contínua, atualmente em curso no Núcleo. Este artigo 
narra esta experiência, incluindo sua aplicação e resultados parciais obtidos até 
o momento. A Seção 2 apresenta as etapas realizadas; a Seção 3 compreende os 
resultados obtidos. Finalmente, a Seção 4 expõe as conclusões do trabalho.
2. Métodos
Esta seção apresenta a metodologia seguida para a aplicação da ferramenta 
denominada “Felizômetro” , a qual teve o objetivo de obter visibilidade sobre 
o grau de satisfação dos colaboradores do NTI-UFPE com vistas na melhoria 
organizacional.
A felicidade das pessoas implica em engajamento e impacta diretamente 
nos resultados da organização. Com base nesta constatação, a preocupação com 
a satisfação dos funcionários está crescente a cada dia. Neste sentido, métodos 
como pesquisas, índices e avaliações de desempenho são comumente usados, 
mas nem sempre são efetivos. 
Dentre técnicas que valorizam a satisfação do funcionário, a Porta da 
Felicidade (do inglês, Happiness Door) foi proposta por Jungen Appelo parte 
do seu modelo de gestão Management 3.0 [Apelo, 2011], focado em pessoas 
e que propõe uma mudança de mentalidade combinada com uma coleção de 
ferramentas para ajudar qualquer funcionário a gerenciar a organização. A tese é 
de que a gerência não é apenas responsabilidade do gerente, mas de todos o que 
fazem a organização. 
A técnica Happiness Door foi criada com base em duas propostas anteriores: 
a prática ágil Mural de Feedback e o Índice de Felicidade. Basicamente consiste 
de pedir ao seu público para considerar o quanto eles ficaram satisfeitos sobre 
um determinado tópico ou projeto, ou mesmo a organização como um todo. 
Este sentimento é registrado através de post-its em um mural previamente 
disponibilizado. De fato, não há necessidade de se usar uma porta, mas uma parede, 
janela ou coluna, o que for acessível e visível. O que vem sendo um ponto positivo 
nesta técnica é a simplicidade de aplicação e o fato de você receber feedback mais 
rápido, sem precisar aguardar por reuniões ou pesquisas.
A técnica foi selecionada pelo NTI por critérios como a facilidade de condução, 
o alinhamento à estratégia de intensificar a cultura ágil e a boa aceitação pela 
comunidade de TI em geral.
2.1. Etapas
O ciclo de aplicação do Felizômetro no NTI foi iniciado no final do ano de 
2018 e seguiu etapas descritas a seguir.página
159
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 157-163, junho de 2019
Capítulo XXVI - Melhoria Organizacional com Base no Grau de Felicidade dos Colaboradoresi. Preparação. Durante a preparação, a alta gestão do Núcleo, junto à 
Assessoria de Inovação e a Coordenação de Governança e Processos, definiu 
categorias organizacionais a serem avaliados durante a aplicação, conforme 
relacionadas a seguir:
• Pessoas, para verificar o grau de satisfação em relação aos colaboradores e 
integração social entre colegas do Núcleo;
• Comunicação, para avaliar o grau de satisfação sobre a comunicação 
institucional interna e externa;
• Gestão e Processos, para avaliar o grau de satisfação dos colaboradores 
em relação aos processos internos, tais como formas de planejamento e 
acompanhamento dos projetos e/ou serviços;
• Infraestrutura, para avaliar a satisfação dos colaboradores com seus 
equipamentos e ferramentas de trabalho;
• Espaço de Trabalho, para avaliar questões relativas ao ambiente físico 
de trabalho, como organização e manutenção das salas, climatização e 
mobiliário.
ii. Coleta. A coleta das percepções das pessoas foi realizada no dia 20 de 
dezembro de 2018 durante a confraternização de final de ano do Núclo, tendo uma 
participação substancial dos seus colaboradores, incluindo servidores públicos e 
bolsistas. Um facilitador explicou a dinâmica ao público geral e distribuiu a todos os 
participantes post-its para que estes pudessem registrar comentários e expressar 
seu nível de satisfação, entre alto (“Tá massa!”), médio (“Tá mais ou menos”) ou 
baixo (“Aff... ”), conforme ilustrado na Figura 1. Durante toda a confraternização e 
mais 15 dias o Felizômetro ficou exposto em uma área de acesso geral e de fácil 
visibilidade para quem quisesse contribuir. Ao final deste período, a Assessoria de 
Inovação recolhei os postits para consolidação, categorização e posterior análise.página
160
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 157-163, junho de 2019
Capítulo XXVI - Melhoria Organizacional com Base no Grau de Felicidade dos Colaboradores
Figura 1: Coleta do Felizômetro
iii. Análise e Ação. Os dados coletados foram consolidados em uma planilha, 
onde cada comentário foi analisado e categorizado de acordo com similaridades. 
Ao todo, foram 206 votos distribuídos de acordo com a Tabela 1 e gráficos 1, 2 
e 3. Os dados foram analisados pelo Comitê Gestor do NTI em uma dinâmica 
facilitada pela Assessoria de Inovação. Com foco nos níveis “Tá mais ou menos” 
e “Aff... ” , inicialmente foram identificadas ações que já estavam em andamento e 
posteriormente definidas novas ações. página
161
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 157-163, junho de 2019
Capítulo XXVI - Melhoria Organizacional com Base no Grau de Felicidade dos Colaboradores
Gráfico 1: Perspectivas que geram um alto grau de satisfação 
Gráfico 2: Aspectos que geram um grau de satisfação médiopágina
162
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 157-163, junho de 2019
Capítulo XXVI - Melhoria Organizacional com Base no Grau de Felicidade dos Colaboradores
Gráfico 3: Aspectos que geram um grau de satisfação baixo
PERSPECTIVA VOTOS VOTOS COMENTADOS
PessoasSatisfação Alta 24 8
Satisfação Média 10 7
Satisfação Baixa 10 10
ComunicaçãoSatisfação Alta 20 10
Satisfação Média 14 6
Satisfação Baixa 7 7
Gestão e ProcessosSatisfação Alta 20 7
Satisfação Média 7 4
Satisfação Baixa 0 0
InfraestruturaSatisfação Alta 12 5
Satisfação Média 10 7
Satisfação Baixa 7 4
Espaço de TrabalhoSatisfação Alta 10 5
Satisfação Média 16 15
Satisfação Baixa 39 37
Tabela 1: Grau de satisfação dos colaboradores do NTI-UFPE
iv. Ações: Nesta etapa, os comentários negativos foram transformados 
em ações de melhoria a serem trabalhadas durante o ano de 2019. Cada ação 
foi priorizada levando-se em conta a gravidade, impacto, esforço e tempo de 
implementação. Atualmente, as ações se encontram em andamento e estão sendo 
monitoradas pela Diretoria.página
163
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 157-163, junho de 2019
Capítulo XXVI - Melhoria Organizacional com Base no Grau de Felicidade dos Colaboradores3. Resultados
A experiência da aplicação do Felizômetro gerou feedback rápido e 
espontâneo, base para ações concretas e relevantes de melhoria. No momento, as 
ações estão sendo realizada e comunicadas ao NTI também em forma de gestão à 
vista. A previsão é que na próxima confraternização geral, durante a comemoração 
do São João, seja fechado o primeiro e iniciado um novo ciclo de melhoria com 
uma nova coleta.
4. Conclusões
Este trabalho apresentou a experiência do NTI na aplicação de um 
mecaniscmo para medir o grau de satisfação de seus colaboradores. A técnica 
se mostrou efetiva e gerou insumos para melhoria contínua, além de promover a 
valorização das pessoas.
Referências
Appelo, J.; “Management 3.0: Leading Agile Developers, Developing Agile Leaders” , 
Addison-Wesley Professional, 2010.
Appelo, J.; “Managing for Happiness: Games, Tools, and Practices to Motivate Any 
Team” , Wiley, 2016.
Grove, A. S.; “High Output Management” , Harvard, Vintage Books, 1983.
Fowler, M. et al; “Agile Manifesto” , www.agilemanifesto.org, 2001. página
164
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 164-169, junho de 2019
Capítulo XXVII - Método Push de atribuição de tarefas utilizado no desenvolvimento de software 
por um time autogerenciávelMétodo Push de atribuição de tarefas utilizado 
no desenvolvimento de software por um time 
autogerenciável
Danniel Rocha, Euclydes Melo, Luiz Fernando, Marcos Raniere, Matheus Souza, Taison 
Almeida
Superintendência de Tecnologia da Informação  – Universidade Federal do Piauí (UFPI) – Teresina 
– PI – Brasil
{danniel, euclydesmelo, fernando.mendes, marcosraniere, matheuscampanha, 
taison.almeida}@ufpi.edu.br
Resumo
O processo de desenvolvimento de software nas IFEs é repleto de demandas dos mais diversos setores destas 
instituições. A divisão do time por área de atuação acaba criando ilhas de conhecimento e desfavorece a integração. 
Além disso, como disciplina a Engenharia de Software, desenvolver um time autogerenciável impacta positivamente 
na produtividade. Todavia, desenvolver essas competências não é uma tarefa trivial. Este trabalho apresenta um 
método de atribuição automática de tarefas aplicável nesse cenário, a fim de estimular a autogestão.
Palavras-chave: Autogestão, Engenharia de Software, Atribuição de Tarefas.
1. Introdução
A adoção de metodologias e boas práticas de gestão de projetos é uma 
condição para se obter êxito na condução de qualquer empreendimento de 
software, porém existe uma lacuna muito grande entre conhecer essas diretrizes e 
aplicá-las, sobretudo em relação aos processos que tratam do desenvolvimento e 
manutenção de times de projetos.
Segundo Blanchard (2000), a melhor maneira de manter o controle de um 
projeto é ter cada membro da equipe no controle do seu próprio trabalho e esse nível 
de autogestão é satisfeito quando se tem alta competência e uma boa moral, o que 
implica em alta produtividade. Porém, para se chegar nesse estágio ou para manter 
um membro da equipe nesse nível de comprometimento, eficiência e autogestão, 
objetivos claros e bem definidos devem ser traçados, além de mecanismos 
de controle e aferição desses objetivos devem ser implementados dentro da 
equipe, bem como para permitir o macrogerenciamento do desenvolvimento na 
perspectiva do líder do projeto.
Na engenharia de software, existem várias técnicas de medição de software, 
que são utilizadas, dentre outras aplicações, para estimar esforço, tempo e 
custo das iniciativas de desenvolvimento de software. Segundo Vazquez (2010) 
para acompanhar a produtividade da equipe, no mínimo, duas medidas são 
necessárias: o esforço utilizado (horas) e o resultado obtido. Todavia, em um página
165
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 164-169, junho de 2019
Capítulo XXVII - Método Push de atribuição de tarefas utilizado no desenvolvimento de software 
por um time autogerenciávelambiente de negócio híbrido no qual existem vários nichos de atuação (ou áreas 
temáticas), como é o caso das Instituições Federais de Ensino Superior (IFES), criar 
ou manter um time autogerenciável revela outra dificuldade, que são as ilhas de 
conhecimento formadas em tornos de cada nicho, como graduação, pós-graduação, 
pesquisa, extensão, dentre outros. Esse fato dificulta a aferição da produtividade 
por métrica única, além de prejudicar a percepção de time único, visto que cada 
célula de conhecimento se mantém dentro de sua membrana, pouco interagindo 
com as demais, desfavorecendo, assim, a integração e inovação.
Nesse sentido, a fim de mitigar esses problemas e estimular a autogestão 
e interação entre as pessoas, apresenta-se, neste trabalho, uma abordagem de 
atribuição automática de tarefas, cujo esforço de implementação é dimensionado, 
no final, em horas e, a cada atribuição, o tempo total gasto por cada integrante 
do time em suas tarefas é ponderado em relação aos demais, a partir do tempo 
estimado de cada tarefa, independentemente da sua área temática.
2. Solução Proposta
Este trabalho foi desenvolvido com o time da Coordenação de Sistemas 
(CS) da Superintendência de Tecnologia da Informação (STI) da Universidade 
Federal do Piauí (UFPI), composto por 13 servidores (entre Analistas e Técnicos de 
Tecnologia da Informação). Inicialmente, o time era dividido em células por área 
temática. Nesse cenário, observa-se pouca interação e troca de conhecimento e 
experiência entre os integrantes de células distintas, bem como uma desproporção 
nas entregas de algumas células em detrimento a outras.
Diante disso, as células foram extintas e todos passaram a compor um único 
time, no qual todos passaram a atender as demandas dos diversos nichos de 
atuação, a fim de promover a interação e integração, bem como a pulverização 
do conhecimento. Além disso, a autogestão começou a ser estimulada. Todavia, 
os graus das competências de autogestão entre os indivíduos permaneciam 
irregulares. Então, aplicou-se um método de atribuição de tarefas descrito a seguir 
para mitigar essas incongruência 
2.1. Método Push 
A fim de identificar as métricas a serem utilizadas no dimensionamento 
do esforço técnico necessário para realização de uma tarefa, categorizou-se as 
demandas oriundas das áreas de negócio em dois tipos principais: Customização, 
quando se trata de alteração, melhoria e/ou criação de funcionalidades; e 
Sustentação, para aglutinar os demais tipo de tarefas.
As sustentações são associadas aos serviços expostos no catálogo de serviços 
da STI por meio do sistema SINAPSE. Cada serviço possui um Acordo de Nível de 
Serviço (ANS) que estabelece a quantidade de horas, em dias úteis, necessárias 
para realização da demanda, contadas a partir da atribuição automática da 
tarefa (criação do chamado pelo usuário final). No caso das customizações, como página
166
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 164-169, junho de 2019
Capítulo XXVII - Método Push de atribuição de tarefas utilizado no desenvolvimento de software 
por um time autogerenciávelenvolve engenharia de requisitos, elas não são atribuídas automaticamente. 
Ao chegar uma demanda ela é registrada no backlog e estimado o esforço 
técnico por meio da técnica Planning Poker, a fim de se encontrar o tamanho de 
cada customização em Story Point (SP). A partir dessa estimativa as tarefas de 
customização ficam automaticamente disponíveis para serem proativamente 
implementadas. A quantidade de horas necessárias para o desenvolvimento de 
uma customização é estimada a partir do seu tamanho em SP , da Tabela 1, que foi 
proposta empiricamente pela equipe, passando a serem contabilizadas a partir do 
momento em que um desenvolvedor inicia o desenvolvimento.
Tabela 1. Conversão de Story Points em horas.
Conhecendo o esforço em hora para realizar uma sustentação e implementar 
uma customização, a cada nova demanda do catálogo de serviços que chega via 
SINAPSE o escore de todos os integrantes do time de desenvolvimento é calculado 
e a tarefa atribuída automaticamente a quem tiver o menor escore no momento, 
este ficando imediatamente fora da próxima atribuição, a fim de evitar que um 
desenvolvedor receba duas ou mais tarefas consecutivamente. Segue a fórmula de 
cálculo do escore bruto (eb):
Onde: tp é o tempo previsto para realização de uma tarefa em horas; e tu  é o 
tempo transcorrido do início da tarefa ao seu fechamento ou ao momento atual.  
E o escore padronizado (ep) para cada desenvolvedor é obtido simplificadamente 
pela fórmula:
O estímulo à proatividade nas customizações é conquistado pela possibilidade 
de aumentar o escore não recebendo, assim, tarefas push, “empurradas” 
automaticamente pelo algoritmo. Além disso, a transparência nas atribuições 
das tarefas e a possibilidade de identificação fácil de quais tarefas podem estar 
prejudicando o escore, favorecem a própria manutenção e evolução do método, 
vide ferramenta apresentada na Figura 1.página
167
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 164-169, junho de 2019
Capítulo XXVII - Método Push de atribuição de tarefas utilizado no desenvolvimento de software 
por um time autogerenciável
Figura 1. Atribuições e escores por rodada de atribuição
Esse recurso permite que cada desenvolvedor acompanhe graficamente a 
evolução do seu escore em relação ao demais, bem como fornece mecanismos 
de autogestão, possibilitando que cada um acompanhe e esteja no controle do 
próprio trabalho. É possível, ainda, filtrar e obter detalhes dos escores e das tarefas 
que impactaram no seu cálculo para cada desenvolvedor, fornecendo, assim, mais 
um instrumento de aferição do trabalho. 
2.2. Macrogenciamento    
Para se construir de um ambiente autogerenciável, além de disponibilizar 
recursos que estimulem o desenvolvimento das competências e do moral 
necessários, é salutar que o líder de projeto delegue responsabilidades e 
poder de decisão aos membros da equipe. Para isso, torna-se imprescindível o 
desenvolvimento de mecanismos de acompanhamento e controle que permitam a 
aferição da produtividade do time sem a necessidade de intervir no seu trabalho. A 
Figura 2, juntamente com a Figura 1, exemplificam instrumentos de gestão utilizado 
no macrogenciamento das atividades, bem como a aderência do time ao processo.
Figura 2. Painel de indicadores de tarefas
3. Resultados
O método proposto apresentou resultado positivos acerca da aplicabilidade 
no desenvolvimento de um time autogerenciável. A Figura 3 demonstra a 
distribuição da atuação dos desenvolvedores em projetos de nichos distintos.página
168
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 164-169, junho de 2019
Capítulo XXVII - Método Push de atribuição de tarefas utilizado no desenvolvimento de software 
por um time autogerenciável
Figura 3. Distribuição da atuação dos desenvolvedores em projetos distintos
Foi observado uma maior e melhor distribuição da atuação dos 
desenvolvedores nos projetos. Cada desenvolvedor contribui em pelo menos 30% 
das áreas temáticas, quando antes a maioria dos programadores colaboravam 
em menos de 20% dos projetos, tendo casos de participação inferior a 10% o 
que evidenciava as ilhas de conhecimento. Em relação a melhoria no tempo de 
atendimento das demandas (ANS), o método também mostrou-se eficiente, como 
aponta a Figura 4.
Figura 4. Distribuição das tarefas por quantidade de dia utilizado no seu 
atendimentopágina
169
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 164-169, junho de 2019
Capítulo XXVII - Método Push de atribuição de tarefas utilizado no desenvolvimento de software 
por um time autogerenciávelA quantidade de chamados/tarefas atendidas no primeiro dia aumentou 
6%. Tomando as demandas atendidas em até 10 dias (2 semanas) o ganho é de 
mais de 35%, o que evidencia os ganhos desta abordagem no desenvolvimento da 
autogestão.
4. Conclusão
Adoção de práticas que estimulem e mantenham o time no controle 
de suas próprias atividades tem efeito direto na produtividade. Desenvolver 
essas competências não é um exercício fácil. O uso de mecanismos e recursos 
que favoreçam essa evolução são, sem dúvidas, bem vindas. Neste trabalho, 
apresentou-se uma abordagem, denominada Método Push de atribuição de tarefas, 
que demonstrou ser eficiente para esse propósito, estimulando a diversificação de 
competências e autogestão do time.
Pretende-se aprimorar esse método para considerar aspectos de Inteligência 
Artificial na atribuição de tarefas, como a análise da afinidade de cada 
desenvolvedor, bem como implantar recursos de gamification, para aumentar, 
ainda mais, o engajamento, além de realizar uma avaliação sobre melhoria na 
interação da equipe.
Referências
BLANCHARD, K. H. (2000) “O Gerente Minuto. Desenvolve Equipes de Alto Desempenho. ” 
Rio de Janeiro, RJ: Editora Record, 5. ed.
HIDELMAN, K. (2009) “Gerência de Projetos. ” Rio de Janeiro, Editora Elsevier, 5. ed.
MARTINS, J. C. C. (2007) “Gerenciando projetos de desenvolvimento de software com 
PMI, RUP e UML. ” Rio de Janeiro: Brasport, 4. ed.
VAZQUEZ, C. E. (2010) “Análise de Pontos de Função: Medição, Estimativa e 
Gerenciamento de Projetos de Software. ” São Paulo, Editora Érica, 9. Ed.página
170
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 170-175, junho de 2019
Capítulo XXVII - Moodle: Arquitetura redundante e escalável para alta demanda de acessoMoodle: Arquitetura redundante e escalável para 
alta demanda de acesso
André Vinícius Farias Sousa¹, Edmilson Alves do Nascimento¹, Gustavo Pinho Gomes 
dos Santos¹, Michel Peterson Andrade¹
¹Superintendência de Tecnologia da Informação – Universidade Federal da Bahia (UFBA)
Av. Ademar de Barros, s/n – Campus de Ondina – Salvador – BA – Brasil
{andre.sousa,edmilson.nascimento,gustavos,michel.peterson}@ufba.br
Resumo
A Universidade Federal da Bahia (UFBA) possui o Moodle como plataforma de ensino a distância. Este artigo 
apresenta a arquitetura redundante, escalável e de fácil configuração dos recursos computacionais, implantada na 
UFBA e que se aplica a ambientes com um elevado número de usuários, permitindo um grande número de conexões 
simultâneas. A criação desse artigo tem como objetivo auxiliar outras Instituições de Ensino que adotam o Moodle, 
frente ao crescente número de alunos no ensino a distância e a escassa documentação existente para configuração.
Palavras-chave: PHP, Moodle, Apache, MySQL, EAD.
1. Introdução
A Universidade Federal da Bahia (UFBA) adota o Moodle (Modular Object-
Oriented Dynamic Learning Environment) como Ambiente Ensino a Distância 
(EAD) desde 2005, “como forma de dinamizar a educação a distância e ao mesmo 
tempo oferecer recurso poderoso para o ensino pedagógico, entre outros motivos, 
também pela sua capacidade de difundir conhecimento. ” (MARTINS, 2009).
O Moodle1 é uma das plataformas de aprendizado mais populares de EAD, 
flexível e escalável, projetada para fornecer aos seus usuários um sistema robusto, 
seguro e integrado que possibilita criar ambientes virtuais de aprendizado 
personalizados, e que é adotado por instituições de grande e pequeno porte ao 
redor do mundo. 
O ambiente Moodle da UFBA, atualmente abriga mais de 4.000 (quatro 
mil) cursos e possui uma base acima de 54.800 (cinquenta e quatro mil e 
oitocentos) usuários. Esse ambiente é resultado de diversas transformações e 
aperfeiçoamentos, determinados por demandas, ao longo de sua existência. 
Sendo a última, um convênio com a Secretaria de Educação do Estado da Bahia 
(SEC-BA).
O Convênio em questão foi firmado para abrigar cursos de capacitação e 
aperfeiçoamento para mais de 24 (vinte e quatro) mil serventuários do Estado 
da Bahia, com vistas a progressão funcional, em um período muito curto para 
conclusão. Portanto, reverteu-se de extrema criticidade, já que devia ser assegurada 
a integridade, confiabilidade, e principalmente, disponibilidade e desempenho da 
plataforma. 
1 https://docs.moodle.org/34/en/About_Moodlepágina
171
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 170-175, junho de 2019
Capítulo XXVII - Moodle: Arquitetura redundante e escalável para alta demanda de acessoEsses fatores, grande número de usuários e período curto de tempo para sua 
conclusão, determinaram um uso atípico e não esperado, com um grande volume 
de acessos. Somente nos dois primeiros dias foram mais de 50.500 (cinquenta 
mil e quinhentos) acessos concentrados em determinados horários do dia, o que 
gerou lentidão e erros durante o uso nesses horários. O desafio da equipe técnica 
não se limitava a apenas apresentar uma solução aos problemas apresentados, 
mas definir uma nova arquitetura que possibilitasse a expansão da plataforma, 
atendendo as demandas da UFBA.
Esse artigo visa apresentar uma solução redundante, escalável e de fácil 
configuração dos recursos computacionais, adotada pela UFBA, e que se aplica 
a ambientes de alta densidade de acesso, permitindo um grande número de 
conexões simultâneas, por entender se tratar de uma necessidade comum as 
Instituições de Ensino frente ao crescente número de alunos nesse ambiente e a 
escassa documentação existente para configuração.
1.1. Infraestrutura Anterior
O sistema Moodle da UFBA possuía uma arquitetura sem redundância 
(Figura 1) e sem escalabilidade, composto por dois servidores virtuais instalados 
com serviços de Web e de banco de dados. Utilizando os seguintes recursos 
computacionais: Servidor Web (Apache+PHP) - Proc.16 vCPUs, 32GB de RAM e 4TB 
de Disco; Servidor de Banco (MySQL) - Proc.6 vCPUs, 6GB de RAM e 200GB de Disco.
2. Métodos
Foram várias fases para o desenvolvimento da solução. A primeira fase, teve 
como objetivo, viabilizar a realização dos cursos, de forma imediata, sem os erros 
que se apresentavam. Nessa fase foram criados vários ambientes do Moodle, com 
vistas a separar os cursos de maiores demandas de acesso.
Uma vez contornado o problema, passou-se a fase de pesquisa e definição 
da melhor solução em definitivo a ser adotada, tendo como premissa a criação de 
um único ambiente que suportasse todos os cursos e fosse escalável, robusto e de 
fácil configuração.
A pesquisa da melhor solução se deu em duas bases, teórica e empírica. Essa 
última, necessária, dado a escassez de documentação conclusiva sobre a melhor 
arquitetura a ser adotada no Moodle para um alto volume de acessos simultâneos, 
foi realizada em um ambiente experimental e controlado para observação dos 
resultados.página
172
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 170-175, junho de 2019
Capítulo XXVII - Moodle: Arquitetura redundante e escalável para alta demanda de acesso
Figura 1. Arquitetura antiga e Arquitetura adotada
Por conta da limitação de páginas e por não ser o objetivo desse artigo, 
não serão demonstrados os vários cenários, arquiteturas e os resultados dos 
experimentos realizados. Nos concentraremos em demonstrar a arquitetura final 
definida, como a melhor solução e que foi adotada pela UFBA, conforme demonstra 
a figura 1 acima.
O sistema ficou composto por dois servidores para balanceamento de 
carga com os serviços de HAProxy, dois servidores com os serviços de Web, duas 
instâncias do Memchaced (cache + sessões), um serviço NFS para armazenar o 
Moodledata e três servidores em cluster de banco de dados. Com os seguintes 
recursos computacionais: Servidor Web (Apache+PHP) - Proc.16 vCPUs; 12GB de 
RAM; Servidor Balanceador (Haproxy) - Proc. 2 vCPUs; 2GB de RAM; Servidor de 
Banco (MySQL) - Proc.8 vCPUs; 8GB de RAM. 
Nos tópicos seguintes apresentaremos de forma sucinta os componentes da 
arquitetura apresentada, a ferramenta de testes e a motivação que levou as suas 
escolhas.
2.1. Servidores WEB
As escolhas do Apache e Haproxy, respetivamente como servidor web e 
balanceador, estão relacionadas a casos de sucesso de suas implementações 
em outros ambientes dentro da Universidade. A utilização de um balanceador 
foi definida para facilitar o escalonamento horizontal do ambiente, permitindo a 
adição de um número irrestrito de servidores, bem como para facilitar a mitigação 
de alguns tipos de ataques cibernéticos. 
O Apache foi configurado com o processador de requisições “mpm_event” , 
em substituição do “mpm_prefork” anteriormente configurado e que não permitia 
extrair o máximo de desempenho, com o objetivo de otimizar recursos de memória 
e dos processadores. O “mpm_event” é um módulo que utiliza múltiplas “threads” 
em cada processo criado, e cada uma dessas “threads” pode executar mais de uma 
tarefa se utilizando de baixos recursos computacionais.página
173
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 170-175, junho de 2019
Capítulo XXVII - Moodle: Arquitetura redundante e escalável para alta demanda de acessoComo o Moodle é desenvolvido sob a linguagem PHP , fez-se necessário a 
utilização de um manipulador PHP , sendo então escolhido o sistema PHP-FPM que 
se deu por um conjunto importante de características, tais como: baixo uso de 
CPU, maior segurança e melhor detalhamento na execução dos scripts PHP , que 
facilitou o processo de “troubleshooting” na investigação de gargalos e que deram 
direcionamento para a construção da nova arquitetura.
2.2. Armazenamento
O armazenamento do Moodle foi feito utilizando um sistema misto de sistemas 
de arquivos, compostos pelas tecnologias: NFS, TmpFS e Memcached. O NFS foi 
utilizado para armazenar e compartilhar os dados dos usuários (Moodledata) 
entre os servidores web. Dado o tamanho dos scripts PHPs das páginas, a sua 
invariabilidade e buscando mais desempenho para o ambiente, foi criado um 
sistema de arquivos virtual utilizando o TmpFS com o tamanho de 250MB, para 
armazenar esses scripts.
Foram criadas duas instâncias do memcached, uma para armazenar as 
sessões dos usuários e outra para o cache da aplicação, isso se fez necessário 
após identificarmos problemas de lentidão no carregamento das páginas quando 
ambos os diretórios estavam armazenados em sistema de arquivos local.
2.3. Banco de Dados
Foi implantada a arquitetura, amplamente documentada no manual do 
Percona, com 3 (três) servidores de banco de dados, utilizando o SGBD MySQL, 
com o Galera Cluster Synchronous Multi-master, API Percona XtraDB, e mais um 
servidor de balanceamento utilizando o software Haproxy. 
Essa arquitetura, em síntese, funciona da seguinte forma: as requisições 
de leitura e escrita são direcionadas ao balanceador (HAProxy), que distribui a 
requisição para um dos servidores MySQL, impedindo que um único servidor fique 
sobrecarregado. O Galera Cluster, por seu turno, através da API wsrep e usando 
o método SST (State Snapshot Transfer) “xtrabackup-v2” , se responsabiliza por 
registrar as atualizações dos dados e propagar os logs de transação pela rede para 
os servidores do cluster.
2.4. Ferramentas de Teste
Inicialmente foi feito um estudo utilizando uma ferramenta de web analytics 
para identificar, dentro da plataforma Moodle, as páginas mais acessadas pelos 
usuários, bem como as ações mais frequentes dentro dessas páginas.
Após a identificação do perfil de acesso dos usuários foi utilizado o software 
Jmeter para elaborar o plano de testes devido ao seu vasto catálogo de plugins e 
funcionalidades. Junto ao Jmeter, também foi utilizado o Zabbix para monitorar o 
comportamento dos componentes da infraestrutura durante a execução dos testes 
e foi possível identificar o tempo de requisição ao banco quando o teste estava em 
execução.página
174
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 170-175, junho de 2019
Capítulo XXVII - Moodle: Arquitetura redundante e escalável para alta demanda de acesso3. Resultados
Nessa seção descreveremos de forma sucinta as principais observações 
realizadas ao longo da pesquisa e que nortearam a definição da arquitetura 
apresentada.  
Para realização dos testes no ambiente de pesquisa foi definido a execução 
utilizando 500 usuários virtuais simultâneos em um ciclo de 30 vezes. Para 
estabelecer parâmetros iniciais de referência, a arquitetura anterior foi clonada e 
depois executado o plano de teste elaborado no Jmeter. 
Os primeiros resultados demonstraram uma lentidão excessiva. A carga do 
servidor de aplicação ficou extremamente elevada mesmo sem o processamento 
atingir sua capacidade máxima. Foi identificado que o sistema de arquivos NFS não 
estava conseguindo suprir a demanda de criação dos arquivos de sessão e cache 
da aplicação, sendo necessário a implementação do memcached para armazenar 
a sessão dos usuários e os caches dos componentes da aplicação, para melhorar o 
desempenho do ambiente. 
Através do PHP-FPM foi possível observar os scripts de PHP que estavam 
executando de forma lenta, mostrando até mesmo funções específicas do PHP , 
como as relacionadas ao banco de dados. Além disso, foi possível identificar o 
número de processos criados e utilizados pelo próprio PHP_FPM e definir a forma 
como esses processos eram criados, propiciando a realização de ajustes.
Outra variável importante foi o tempo gasto que uma consulta levava para 
retornar ao servidor web durante a execução do plano de teste, o que ensejou 
vários ajustes no servidor de banco de dados MySQL. Além das variáveis no sistema 
operacional que tiveram seus valores ajustados, como o número de segmentos 
SYNCOOKIES para zero e o tempo do TCP_FIN_TIMEWAIT para 5 (cinco). 
O gráfico da figura 2, retirado da ferramenta de testes, traz uma comparação, 
medindo o tempo de resposta ao usuário, entre a nova arquitetura (em vermelho) 
e a arquitetura anteriormente existente (em amarelo), demonstrando o elevado 
ganho de desempenho da nova solução. Vale ressaltar que as grandezas de medição 
adotadas no gráfico são estabelecidas como padrão pela própria ferramenta de 
testes.
Figura 2. Tempo de respostapágina
175
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 170-175, junho de 2019
Capítulo XXVII - Moodle: Arquitetura redundante e escalável para alta demanda de acesso4. Conclusão
As pesquisas demonstraram que os fatores envolvidos no desempenho 
são basicamente os mesmos de qualquer sistema baseado em banco de dados 
e em PHP , sendo que no caso específico do Moodle o “memcached” teve papel 
importante para o bom desempenho da solução.
Após a implantação da nova arquitetura, consultas que antes expiravam ou 
tinham tempos extremamente elevados passaram a responder em tempos iguais 
ou menores a 1seg, com vários usuários simultâneos.
Como já dito, esse artigo procurou demonstrar a arquitetura de uma 
plataforma EAD redundante, escalável e de fácil configuração dos recursos 
computacionais baseada em Moodle, que atualmente é adotada pela UFBA com 
grande sucesso, por entender se tratar de uma necessidade comum as Instituições 
de Ensino. E que se aplica a ambiente de alta densidade de acesso, permitindo 
um grande número de conexões simultâneas. A documentação completa da 
implantação da arquitetura pode ser acessada através do endereço https://sti.
ufba.br/sites/cpd.ufba.br/files/documentação_instalação_moodle.pdf.
Referências
Manual do php-fpm.                         https://secure.php.net/manual/pt_BR/install.fpm.php . 
Acesso em 21 Fev. 2019.
Manual Do Percona Cluster.                 https://www.percona.com/software/documentation. 
Acesso em 20 Fev. 2019.
Martins, Claudio. O ambiente virtual de aprendizagem moodleufba como veículo de 
difusão do conhecimento. http://www.pucrs.br/ciencias/viali/mestrado/literatura/
monografias/Monografia_Claudio_Martins_Moodle.pdf . Acesso em 19 de Fev. 2019
Performance recommendations.  https://docs.moodle.org/34/en/Performance_
recommendations#Obtain_a_baseline_benchmark . Acesso em 20 de Fev. 2019página
176
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 176-181, junho de 2019
Capítulo XXVIII - O uso de containers e máquinas virtuais para a otimização de custos e serviços 
no Governo Federal. Uma análise comparativaO uso de containers e máquinas virtuais para 
a otimização de custos e serviços no Governo 
Federal. Uma análise comparativa 
Felipe C. Costa Alves¹, Jean Caminha¹, Renan Susuki¹, Allan Gonçalves¹, Hernane 
Junior¹, Tierry Lincoln¹, Roberto Benedito¹
¹Ponto de Presença da RNP em Mato Grosso – (POP-MT)
CEP 78060-900 – Cuiabá – MT – Brazil
{felipe,jean,renan,allan,hernane,tierry,roberto}@pop-mt.rnp.br 
Resumo
Este artigo apresenta uma comparação entre sistemas virtualizados e os sistemas em containers através de testes 
em um ambiente de TI no Governo Federal. Os serviços de servidor de páginas web, banco de dados, DNS, e-mail e 
gerenciamento de logs foram analisados em ambos os ambientes e coletados os parâmetros de uso de processamento 
e memória. A arquitetura baseada em contêineres alcançou um desempenho de processamento 68% melhor em 
comparação à virtualização. O uso destas soluções pode colaborar para redução de despesas com infraestrutura de 
TI em instituições públicas e o aumento da disponibilidade dos serviços.  
1. Introdução
O aumento da demanda por recursos computacionais e a necessidade por 
utilização racional destes componentes faz com que estudos sejam realizados 
para mensurar o desempenho, flexibilidade e potencial de otimização de novas 
tecnologias (ZHANG, 2018). Os investimentos em servidores e infraestrutura 
representa uma parcela significativa nos custos totais em TI (Tecnologia da 
Informação). Estes investimentos são originados da necessidade por mais espaços, 
aumento do consumo de energia elétrica, refrigeração, cabeamento, entre outros. 
Soluções tecnológicas como virtualização e os contêineres podem contribuir para 
atenuação de investimentos dentro de uma instituição (ZHANG, 2018). 
 A virtualização é caracterizada por compartilhamento de recursos de hardware 
como os processadores, memória e discos e essas ações podem ser realizadas 
através de um sistema de gerenciamento ou Hypervisor onde são alocados os 
recursos necessários para aplicações em máquinas virtuais (NORONHA, 2018). 
Outra abordagem são os sistemas baseados em Linux containers que 
possuem características que possibilitam um encapsulamento de aplicações para 
isolamento de um sistema operacional compartilhado. Os containers não executam 
um sistema operacional completo em hardware virtual e apenas são necessários as 
bibliotecas e configurações para que a atuação de uma aplicação ocorra fornecendo 
um isolamento extra (FEL TER, 2015). Suas funções reduzem a demanda de uma 
arquitetura completa de sistemas e isso ocorre através do recurso namespaces  
presente no kernel linux, capaz também de criar instâncias isoladas . Neste estudo, 
foi realizada uma comparação de sistemas em containers com uma infraestrutura página
177
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 176-181, junho de 2019
Capítulo XXVIII - O uso de containers e máquinas virtuais para a otimização de custos e serviços 
no Governo Federal. Uma análise comparativavirtualizada para execução de aplicações afim de mensurar a eficiência destas duas 
arquiteturas. Utilizou-se como cenário, um ambiente de TI que atende uma equipe 
de até 50 pessoas, cujo principal objetivo é monitoramento de enlaces e circuitos 
de dados. Testes, simulações e comparações do uso de recursos para implantação 
destas ferramentas foram conduzidas e por fim demonstrados as observações a 
respeito dos parâmetros explorados. 
2. Metodologia 
 A análise comparativa do uso de contêineres e virtualização foi realizada 
através de um experimento utilizando os recursos de computação do Ponto de 
Presença no Mato Grosso1 (POP-MT) da Rede Nacional de Ensino e Pesquisa2 (RNP). 
O servidor disponibilizado possuí as seguintes características: 128GB de memória 
RAM (Random Access Memory), processador 2.60 GHz com 32 núcleos e capacidade 
de armazenamento total de 2.7 TB.  
Os recursos utilizados no estudo foram um processador de 2.6GHz com 8 
núcleos, 17 GB memória RAM e 84 Gb de armazenamento de disco.  
As avaliações foram executadas em um host com a arquitetura de máquinas 
virtuais do fabricante VMWare3, versão ESXi 6. Após a execução dos testes e 
simulações, as configurações foram reiniciadas e foi feito a substituição pelo 
software gestor de container Docker4 na sua versão 1.11. Os testes foram realizados 
sobre um único conjunto de hardware. As aplicações instaladas foram o Mysql 
versão 8.0, Apache versão 2, Php versão 5.6, Bind versão 9, Pop3d versão 2.3.2, 
Sendmail versão 8.14, Wordpres versão 4.9 e Syslog versão 3.8, com a finalidade 
de prover as funcionalidades de um servidor web padrão, servidor de banco de 
dados, serviço de DNS (Domain Name System), serviço de e-mail e gerenciamento 
de logs.  
A primeira etapa da avaliação, feita via protocolo SNMP (Simple Network 
Management Protocol) e utilizando o software de monitoramento Zabbix, foi 
verificado o consumo de memória e processamento nestes dois ambientes. A 
análise de performance das aplicações foi conduzida com o uso destes serviços 
em ambiente de produção.  
Na segunda etapa desta comparação, foi utilizado o software SysBench, 
que é uma ferramenta modular multi-plataformas aplicado para a avaliação de 
parâmetros de sistemas operacionais e análise de desempenho de banco de dados 
em carga intensiva. Seu funcionamento é personalizado e baseia-se na execução 
de um número de threads em paralelo (KOPYTOV, 2012). A ferramenta verifica os 
tempos das solicitações processadas e exibe as estatísticas, tais como: o tempo de 
execução mínimo, médio e máximo de um thread, além do tempo da solicitação. 
No modo cpu do avaliador, cada solicitação consiste no cálculo de números primos. 
1 http://www.pop-mt.rnp.br/
2 https://www.rnp.br/
3 https://www.vmware.com/br.html
4 https://www.docker.com/página
178
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 176-181, junho de 2019
Capítulo XXVIII - O uso de containers e máquinas virtuais para a otimização de custos e serviços 
no Governo Federal. Uma análise comparativa As operações envolvendo desempenho de entrada e saída foram executadas 
utilizando também o software SysBench. O modo de avaliação pode ser usado 
para produzir cargas de entrada e saida, com a criação de arquivos de tamanhos 
especificos. Utilizou-se o comando sysbench --num-threads=16 --test=fileio --file-
total-size=3G -file-test-mode=rndrw run, onde os parâmetros definem as threads, o 
tipo de teste e o modo de escrita aleatória. 
A versão 2.0.9 do software Iperf foi usado para análise de latência de rede 
em download e upload com configurações semelhantes entre servidor e cliente. O 
aplicativo permite o envio de um byte  entre dois hosts e quantifica a velocidade 
de transmissão. No host utilizado neste estudo, a interface de rede é padrão 
10/100/1000 capaz de atingir até um gigabit por segundo. Foi definido o parâmetro 
–t para ampliar os períodos de envio, por padrão o Iperf realiza verificações a cada 
três segundos apenas. 
 3. Resultados 
Os Sistemas virtuais consumiram aproximadamente cinco por cento do 
total da memória disponibilizada, enquanto os containers utilizaram apenas três 
por cento. Na simulação com Sysbench, as cargas de processamento aplicadas 
resultaram os seguintes valores: em virtualização, o uso de CPU apresentou 
valores médios de 1.3% de consumo do processamento. Os containers, por sua 
vez, alcançaram o uso de processamento de 0.875% (68% melhor). Os registros 
de entrada e saída também lograram melhores índices em containers. Em 
contrapartida, os resultados de desempenho em rede demonstraram uma maior 
estabilidade nos sistemas virtualizados.  
 A tabela 1 contém uma síntese dos resultados obtidos nesta avaliação, 
comparando as arquiteturas de virtualização e containers. página
179
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 176-181, junho de 2019
Capítulo XXVIII - O uso de containers e máquinas virtuais para a otimização de custos e serviços 
no Governo Federal. Uma análise comparativa Tabela 1. Síntese dos resultados da avaliação
Item 
analisado Virtualização  Container 
Uso de 
memória A memória utilizada foi 819 MB 
de total de 17GB em sistema 
virtualizado.  A utilização em container 
demonstrou taxas inferiores aos 
sistemas virtualizados, o uso foi de 
aproximadamente 300 MB para execução 
dos mesmos serviços.
Processamento Para demanda de serviços 
aplicada, o uso de CPU apresentou 
valores em média 1.3% de 
consumo do total. Na simulação 
com Sysbench, a injeção de cargas 
de processamento que resultaram 
nos seguintes valores: 2000 
cpu max prime, 1 thread: 35,50 
segundos 
12000 cpu max prime, 1 thread: 
412.88 segundos 
120000 cpu max prime, 32 thread: 
55.91 segundos. Uma eficiência de 68% foi identificada no 
uso de processamento em container, que 
utilizou 0.875% no uso do processador. 
Utilizando Sysbench, as cargas de 
processamento obtiveram um ganho 
de velocidade comparado ao sistema 
virtualizado. 
20000 cpu max prime,1 thread: 26.34 
segundos  
120000 cpu max prime,1 thread: 
313.98  segundos 
120000 cpu max prime, 32 thread: 
43.55 segundos. 
Sobrecarga  de 
Entrada/SaídaOs resultados dos índices obtidos 
em simulação em sistema virtual 
nos testes alcançaram a média de 
1,02 segundos. 
 Com 16 threads, tempo total 
de 1.4577 segundos para 6007 
operações de leitura e 4005 de 
escrita. 
Com 16 threads, tempo total 
de 0,5964 segundos para 6007 
operações leitura e 4003 de escrita. Os resultados apresentados demonstram 
tempo de leitura e escrita reduzido, em 
especial no teste com 16 threads, que 
registrou maior estabilidade.
A Média das operações foi de 0,93 
segundos. 
 
Com 16 threads, tempo total de 0,9348 
segundos para 6066 operações leitura e 
4048 de escrita;  
Com 16 threads, tempo total de 
0,9343s segundos para 6063 operações 
leitura e 4041 de escrita.
Teste de rede Os registros de tráfego de rede 
nesse ambiente atingiram taxas de 
testes entre 
80% e 95% nos dois parâmetros 
principais conforme dados a baixo. 
Download 942 Mbits/sec 120s 
Upload 928 Mbits/sec 120s 
Dowload 886 Mbits/sec 600s 
Upload 873 Mbits/sec 600s Em ambiente com container, os testes 
apresentaram uma diferença significativa 
quando utilizado o parâmetro de 
ampliação do tempo para 600 segundos 
para download. 
Download 853 Mbits/sec 120s 
Upload 884 Mbits/sec 120s 
Download 747 Mbits/sec 600s 
Upload servidor 898 Mbits/sec 600s página
180
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 176-181, junho de 2019
Capítulo XXVIII - O uso de containers e máquinas virtuais para a otimização de custos e serviços 
no Governo Federal. Uma análise comparativaItem 
analisado Virtualização  Container 
Documentação 
e dificuldades 
técnicas O acesso e início das configurações 
não representa dificuldades. 
Identificou-se este como o 
principal benefício dos sistemas 
virtualizados, após instalação 
do hypervisor VmWare ESXi em 
servidor e no vSphere em cliente Uma dificuldade encontrada na gestão 
de containers refere-se ao uso gestão 
de recursos de hardware. De acordo 
com o manual do sistema , é possível 
realizar o controle de memória, CPU, 
entrada e saída, entre outros. Também é 
necessário que seja verificado se o kernel 
suporta os recursos do Linux. Estes são 
exemplos de dificuldades encontradas na 
implementação dos containers.
O custo na implementação destas tecnologias também é um fator relevante 
e essencial para que o gestor da instituição selecione qual sistema utilizar. O uso 
de containers através do Docker, para os serviços especificados neste experimento 
pode ser disponibilizado sem custos e nem limitações de hardware. Entretanto, a 
versão básica de virtualização do cliente VMWare vSphere Essentials possui custos 
iniciais de R$2507.31  com atualizações limitadas ao período de um ano, estas 
licenças são restritas a três servidores com dois processadores cada.   
4. Conclusão 
O governo federal possui um orçamento destinado a TI de cerca de 5 bilhões 
de Reais (2018), para a aquisição de serviços e materiais. Servidores de rede 
representam um alto valor destas aquisições. Essa eficiência colabora diretamente 
na redução dos custos de TI para o serviço público, postergando a aquisição de 
atualizações de hardware e mantendo a infraestrutura otimizada.   
Os resultados desta avaliação demonstram otimização de recursos quando 
equipes de TI das instituições públicas optarem pelo uso de containers. O uso 
de memória, velocidade de escrita e leitura em disco e processamento validam a 
recomendação desta tecnologia em locais que priorizem um maior aproveitamento 
dos recursos de hardware e não disponibilizam de grande orçamento para 
investimentos. 
Como trabalhos futuros, serão investigados os benefícios da virtualização e 
contêineres para os serviços de rede e armazenamento. 
Referências 
ZHANG, Qi et al. A comparative study of containers and virtual machines in big data 
environment. arXiv preprint arXiv:1807.01842, 2018. 
FEL TER, Wes et al. An updated performance comparison of virtual machines and linux 
containers. In: 2015 IEEE international symposium on performance analysis of systems 
and software (ISPASS). IEEE, 2015. p. 171-172. página
181
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 176-181, junho de 2019
Capítulo XXVIII - O uso de containers e máquinas virtuais para a otimização de custos e serviços 
no Governo Federal. Uma análise comparativaPAINEL DE GASTOS DE TI. Acesso em 14 de março de 2019. http://paineis.cgu.gov.br/
gastosti/index.htm. <Acessado em 15 de Março de 2019> KOPYTOV, Alexey. Sysbench 
manual. MySQL AB, p. 2-3, 2012. 
NORONHA, Vivian et al. Performance Evaluation of Container Based Virtualization on 
Embedded Microprocessors. In: 2018 30th International Teletraffic Congress (ITC 30). 
IEEE, 2018. p. 79-84.  
Oracle MY SQL. Disponível em: < https://www.mysql.com/downloads/>. Acesso em: 21 
de abril de 2019. 
Apache HTTP Server Project. Disponível em: < https://httpd.apache.org/>. Acesso em: 
21 de abril de 2019. 
PHP . Disponível em: < https://www.php.net/downloads.php/>. Acesso em: 21 de abril 
de 2019. 
Bind Server. Disponível em: < https://www.isc.org/downloads/bind/>. Acesso em: 21 
de abril de 2019. 
POP3D. Disponível em: < https://www.courier-mta.org/status.html/>. Acesso em: 21 
de abril de 2019. 
Sendmail Server. Disponível em: 
<http://www.linuxfromscratch.org/blfs/view/8.1/server/sendmail.htmlg/>. Acesso 
em: 21 de abril de 2019. 
Iperf. Disponível em: <https://iperf.fr//>. Acesso em: 21 de abril de 2019. 
Wordpress. Disponível em: <https://br.wordpress.org/>. Acesso em: 21 de abril de 
2019. página
182
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 182-187, junho de 2019
Capítulo XXIX - O Uso de Micro Serviços no desenvolvimento de sistemas nas Universidades e seus 
benefícios:um estudo de caso.O Uso de Micro Serviços no desenvolvimento de 
sistemas nas Universidades e seus benefícios: 
um estudo de caso.
Tales Mota Machado¹, Frederico Augusto de Cezar Almeida Gonçalves¹, Abelard 
Ramos Fernandes¹, Tiago Rodrigues Chaves¹
¹Núcleo de Tecnologia da Informação (NTI) – Universidade Federal de Ouro Preto (UFOP)
35.400-000   – Ouro Preto – MG – Brazil
{talesmachado,fred,abelard,tiago.chaves}@ufop.edu.br 
Resumo
Este trabalho apresenta os primeiros resultados proporcionados pela adoção no Micro Serviços no desenvolvimento 
de sistemas na Universidade Federal de Ouro Preto. A adoção dessa arquitetura permitiu a divisão dos sistemas 
em partes menores, independentes, isoladas, escaláveis e com responsabilidades únicas. Ainda, por serem partes 
isoladas, foi possível tratar a segurança dos Micro Serviços de forma a disponibilizar interfaces de programação 
de aplicações para terceiros. Esta arquitetura possibilitou novas formas de desenvolver aplicações focadas na 
experiência do usuário e uma parceria com um professor da Universidade resultando em um aplicativo de controle 
de frequência para os professores.
1. Introdução
Desde a crise de Software na década de 70, novas formas de desenvolver 
Software vem sendo discutidas e estudadas. Temas como Qualidade do Software 
são recorrentes nesse ambiente de estudos assim como Testes e Segurança. Toda 
a evolução na Engenharia de Software possibilitou o avanço nas Metodologias de 
Desenvolvimentos e nas Arquiteturas que são utilizadas pelo Mercado.
A arquitetura de Micro Serviços são um exemplo dessas tendências de 
Mercado. Essa arquitetura se mostrou uma tendência a partir do momento em 
que os cenários de computação em nuvem se tornaram populares [Thönes 2015, 
Alshuqayran et al. 2016, Savchenko et al. 2015]. Essa arquitetura foca na divisão do 
Software em partes pequenas e independentes de forma a terem responsabilidades 
únicas [Savchenko2015]. Dessa forma, esse tipo de construção se torna leve, 
isolada e escalável. Por serem independentes e isoladas, qualquer manutenção 
necessária em um micro serviço não compromete o funcionamento do todo, 
somente dessa pequena parte. O que vale também para escalabilidade, pois é 
possível escalar somente as partes que precisam de mais recursos, otimizando a 
distribuição de recursos entre as aplicações.
No serviço público, especificamente na UFOP , a adoção dessa arquitetura 
possibilitou a aproximação da instituição com a comunidade acadêmica por 
meio da disponibilização de serviços. Atualmente é possível, que algum servidor, 
por exemplo um professor solicite acesso a uma API (Application Programming 
Interface) que forneça os dados do seu diário de classe. Isso garante uma listagem página
183
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 182-187, junho de 2019
Capítulo XXIX - O Uso de Micro Serviços no desenvolvimento de sistemas nas Universidades e seus 
benefícios:um estudo de caso.atualizada de alunos matriculados, evitando a necessidade da impressão da 
listagem de alunos periodicamente para garantir dados consistentes.
Ainda, uma grande preocupação com a segurança dos dados sempre esteve 
presente em todas as etapas da implantação dessa arquitetura. Dessa forma, 
é necessário controlar o acesso a essas API por meio de um cadastro que por 
consequência gera uma chave de integração. Nesse cadastro é necessário que o 
demandante responsável informe quais serviços serão usados, por exemplo: envio 
de e-mail, diário de classe, disciplinas ministradas, etc.; e qual o público que irá 
acessar esse serviço: alunos, professores, técnicos, visitantes, etc. Dessa forma, 
através dessa chave de integração, toda a segurança dos serviços é feita por meio 
de Tokens de acesso JWT . Através desse Token, é verificado se determinado usuário 
tem acesso a determinado serviço.
O restante deste artigo está organizado da seguinte maneira. A Seção 2 
apresenta como foi a implantação da arquitetura de Micro Serviços no Núcleo 
de Tecnologia da Informação, na Universidade Federal de Ouro Preto. A Seção 3 
apresenta os primeiros resultados que esse tipo de arquitetura proporcionou. A 
Seção 4 apresenta as conclusões deste trabalho e os trabalhos futuros.
2. Métodos
Nesta Seção são apresentados os detalhes da arquitetura de Micro Serviços 
implantada no Núcleo de Tecnologia da Informação na Universidade Federal 
de Ouro Preto. Na Subseção 2.1 são apresentados os detalhes de segurança da 
arquitetura. Na Subseção 2.2 são apresentadas as tecnologias utilizadas para o 
desenvolvimento.
2.1. Segurança
A segurança dos Micro Serviços é feita por meio de Tokens JWT . JWT significa 
JSON Web Token [M. Jones], que é um padrão aberto (RFC 75191) que permite a 
transmissão de informações entre as partes usando um JSON. Existem informações 
padronizadas (public claims) que compõem o Token como: nome completo, email, 
aniversário, sexo, etc. Existe a possibilidade de definir informações personalizadas 
(private claims) que atendem a cenários específicos das organizações. O Token  
pode ser verificado e considerado seguro por conta da sua assinatura digital. No 
momento em que o Token é gerado ele é assinado digitalmente de forma que 
qualquer modificação no Token invalida essa assinatura.
Nesse sentido, nossa arquitetura possui dois módulos para tratar a segurança 
da aplicação. Um módulo de Autenticação e um módulo de Autorização. A Figura 
1 apresenta o fluxo de geração do Token, o cliente envia as credenciais para um 
serviço de login. Este serviço, por meio do módulo de Autenticação, verifica no 
banco de dados as credenciais e as permissões que essa credencial possui. Por fim 
esse módulo preenche as claims e entrega o Token para o cliente, ou retorna um 
1 https://tools.ietf.org/html/rfc7519página
184
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 182-187, junho de 2019
Capítulo XXIX - O Uso de Micro Serviços no desenvolvimento de sistemas nas Universidades e seus 
benefícios:um estudo de caso.erro de permissão caso a credencial seja inválida. A credencial, além de usuário 
e senha, possui a informação referente a chave de serviço. Essa chave de serviço 
é um Identificador Universal Único (UUID) gerado quando o demandante faz um 
pedido formal para a utilização de um serviço. Nesse pedido ele deve informar a 
finalidade, quais os serviços que ele quer acesso e quais os usuários terão acesso 
a esse serviço. 
Figura 1. O fluxo de autenticação do serviço.
Esse pedido é analisado e caso seja deferido, uma chave de serviço é gerada, 
sob responsabilidade do demandante. Essa chave de serviço, além de ser usada 
para Autenticar e Autorizar as requisições, é usada para mapear as atividades das 
aplicações. Isso permitirá, futuramente, a emissão de relatórios e a identificação 
no padrão de comportamento das aplicações possibilitando prever picos de 
consumo e assim dimensionar recursos para responder a esses picos específicos. 
A Figura 2 apresenta o fluxo de autorização do Micro Serviço. O Cliente requisita 
uma informação a um Micro Serviço passando o Token. O Micro Serviço por meio 
do módulo de Autorização verifica se aquele Token é válido e se tem autorização 
para acessar esse Micro Serviço. Essa verificação de segurança é feita única e 
exclusivamente analisando o próprio Token, evitando consultas desnecessárias ao 
Banco de dados. Por fim, se o Token está apto a acessar o Micro Serviço a regra 
de negócio é executada e a resposta é retornada para o Cliente, ou um erro de 
permissão é retornado.
Figura 2. O fluxo de autorização do serviço.página
185
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 182-187, junho de 2019
Capítulo XXIX - O Uso de Micro Serviços no desenvolvimento de sistemas nas Universidades e seus 
benefícios:um estudo de caso.Cada Micro Serviço possui um identificador único e uma lista de métodos do 
protocolo HTTP no qual esse pode ser acessado. Por exemplo um Micro Serviço que 
gerencia alunos pode cadastrar um novo aluno por meio do método POST, editar 
o registro de aluno por meio de um método PUT, visualizar as informações de um 
aluno por meio do método GET e deletar um registro de aluno por meio de um 
método DELETE. Afim de evitar uma consulta ao Banco de Dados, para autorizar 
cada requisição deste Micro Serviço, a Figura 3 mostra como a arquitetura foi 
planejada para realizar a autorização baseada em uma  private claim evitando 
assim um overhead no Banco de Dados. 
Para cada Identificador de um Micro Serviço existe uma associação de quais 
métodos este serviço está autorizado a responder. Mais ainda, cada método 
relacionado ao Micro Serviço é representado por um número binário: GET(0001), 
POST(0010), PUT(0100) e DELETE(1000). Dessa forma, combinando esses métodos é 
possível representar como um único valor quais métodos determinado Identificador 
pode ser acessado. Retomando ao exemplo anteriormente mencionado, referente 
ao Micro Serviço para gerenciar alunos, seria possível representar a permissão 
deste serviço em um par de chave-valor, “ID_SERVICO”: “1111”.  
Figura 3. Regra de representação de acesso ao Micro Serviço baseado em métodos 
HTTP.
Por fim, nosso módulo de Autorização está baseado em um private claim 
que define uma lista de chave-valor que especificam quais Identificadores e quais 
métodos de Serviços, determinado usuário tem autorização para acessar. 
2.2. Tecnologias
Antes da adoção da arquitetura de Micro Serviços, todos os sistemas eram 
desenvolvidos em Java, de forma monolítica, usando o framework JSF (JavaServer 
Faces). Por este motivo, a decidiu-se por continuar a utilizar a linguagem Java para 
minimizar o impacto na forma de trabalho dos desenvolvedores do NTI.
O Spring Boot foi o framework escolhido para se trabalhar com Micro 
Serviços. Essa escolha se baseou no tamanho da comunidade de desenvolvedores, 
qualidade de documentação e preocupação com segurança.
E por fim, é utilizado o Swagger para a geração de documentação dos 
Micro Serviços. O Swagger é uma ferramenta para geração de documentação 
open source, que é capaz de gerar uma interface gráfica de documentação para 
os serviços baseados nos comentários escritos no código. Essa interface permite página
186
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 182-187, junho de 2019
Capítulo XXIX - O Uso de Micro Serviços no desenvolvimento de sistemas nas Universidades e seus 
benefícios:um estudo de caso.explorar e testar os Micro Serviços. A Figura 4 mostra como é a interface gerada 
pelo Swagger para uma aplicação demo disponível no site da ferramenta.
Com a adoção dessa arquitetura conseguimos resolver alguns dos problemas 
recorrentes em ambientes de desenvolvimento de Software: retrabalho, 
manutenção, documentação. O retrabalho tem diminuído com o passar do tempo. 
Isso se dá pela criação de novos serviços. Com isso os desenvolvedores passaram 
a utilizar serviços já existentes ao invés de reimplementar tais funcionalidades. 
Consequentemente, um problema de manutenção foi resolvido, pois como as 
regras de negócio estão centralizadas em serviços, a manutenção se restringe a 
um ponto específico garantindo a consistência para todos que utilizam aquele 
determinado serviço. E o problema de documentação foi resolvido adotando a 
ferramenta Swagger, que gera essa documentação de maneira automática através 
dos comentários no código, facilitando a consulta pelos desenvolvedores. 
Figura 4. Exemplo de uma interface de documentação gerada pelo Swagger.
3. Resultados
Após a implantação dessa arquitetura de Micro Serviços, o desenvolvimento 
de novas aplicações puderam ser dividos entre desenvolvimento front-end e back-
end. No desenvolvimento front-end o objetivo é codificar interfaces como o foco na 
interação do usuário. No desenvolvimento back-end o foco é na implementação da 
regra de negócio, sem se preocupar em como isso será apresentado para o usuário.
Essa divisão permitiu um aumento na agilidade das equipes pois com 
o passar do tempo, o reuso de Micro Serviços passou a ser recorrente evitando 
reimplementação de funcionalidades.
Outro resultado que a adoção desta arquitetura possibilitou foi uma parceria 
com um professor da Universidade. Essa parceria resultou na geração de um 
aplicativo na qual os professores podem controlar a chamada dos alunos pelo 
aplicativo evitando a necessidade impressão de chamadas, dados desatualizados 
e que o professor tenha que ficar gerenciando planilhas.página
187
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 182-187, junho de 2019
Capítulo XXIX - O Uso de Micro Serviços no desenvolvimento de sistemas nas Universidades e seus 
benefícios:um estudo de caso.4. Conclusões
Micro Serviços são uma tendência no mercado e trazem um série de vantagens 
no seu uso: simplicidade, isolamento, segurança, escalabilidade, reuso, etc. Além 
disso permitem que o desenvolvimento de interfaces sejam desacoplados da 
regra do negócio possibilitando que para o mesmo Micro Serviço, possam haver 
diferentes interfaces de usuário, web e aplicativo.
Como trabalho futuro, estamos estudando formas de automatizar a 
escalabilidade dos Micro Serviços por meio de container’s Docker e orquestradores. 
Referências
Alshuqayran, N., Ali, N., and Evans, R. (2016). A systematic mapping study in 
microservice architecture. In 2016 IEEE 9th International Conference on Service-
Oriented Computing and Applications (SOCA), pages 44–51.
M. Jones, J. Bradley, N. S. Json web token (jwt). https://tools.ietf.org/html/rfc7519. 
[Online; consulté Mai-2016].
Holton, M. and Alexander, S. (1995) “Soft Cellular Modeling: A Technique for the 
Simulation of Non-rigid Materials” , Computer Graphics: Developments in Virtual 
Environments, R. A. Earnshaw and J. A. Vince, England, Academic Press Ltd., p. 449-
460.
Savchenko, D. I., Radchenko, G. I., and Taipale, O. (2015). Microservices validation: 
Mjolnirr platform case study. In 2015 38th International Convention on Information 
and Communication Technology, Electronics and Microelectronics (MIPRO), pages 
235–240. 
Thones, J. (2015). Microservices. ¨ IEEE Software, 32(1):116–116.página
188
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 188-193, junho de 2019
Capítulo XXX - O uso prático do traffic control (tc) para evitar o colapso de links de Internet em
regiões próximas a áreas residenciaisO uso prático do traffic control (tc) para 
evitar o colapso de links de Internet em 
regiões próximas a áreas residenciais
Rômulo N. de Oliveira¹ , Deive F. V. Gomes¹, Carlos R. A. da Silva¹, Icaro dos S. Silva¹
¹Campus Arapiraca – Universidade Federal de Alagoas (UFAL)
Caixa Postal 61 – 57309-005 – Arapiraca – AL 
{romulo,deivefabian,carlos.araujo}@nti.ufal.br,icaro@arapiraca.ufal.br
Resumo
Em redes lógicas corporativas é comum encontrar usuários insatisfeitos com a conexão para Internet. O problema 
ainda se agrava quando usuários usam este recurso para fins secundários ou particulares. Na Universidade Federal 
de Alagoas, Campus Arapiraca, Unidade de Penedo, os usuários da rede lógica ainda compartilham e competem pelo 
recurso com a vizinhança local. Este trabalho apresenta uma solução prática para o problema, utilizando recursos 
acessíveis e gratuitos do próprio Linux. Para garantir uma distribuição justa e equilibrada da banda de Internet, 
um conjunto de cotas de tamanho dinâmico foi implementado com a ferramenta traffic control. O resultado se 
mostrou bastante eficiente na percepção dos usuários e da equipe técnica.
Palavras-chave: gerenciamento de tráfego de rede, traffic control.
1. Introdução
Um dos papéis que as universidades assumem é o de produzir e difundir 
conhecimento, que é feito através da troca de experiências, diálogos com a 
sociedade e da pesquisa científica. Um dos requisitos para isso é o acesso à Internet.  
Além disso, a maioria das instituições públicas brasileiras vivem uma fase de terem 
como requisito que seus sistemas computacionais sejam acessíveis pela Web. O 
governo também incentiva a transparência da informação, a desburocratização 
na realização de serviços públicos e a acessibilidade para todos. Esse contexto nos 
leva a um cenário que demanda:
• uma maior banda no link da dados para as instituições;
• a melhoria na cobertura de rede sem fio, que implica na instalação de mais 
equipamentos access points;
• um melhor controle no gerenciamento dos recursos de rede lógica.
Entretanto, o controle efetivo do acesso à rede lógica, através de login 
e senha, ou mesmo pelo cadastro de dispositivos, não oferece bons resultados 
práticos em instituições onde os recursos humanos são limitados e o número de 
usuários é grande. Isso faz com que os profissionais de TI normalmente adotem a 
solução de criar uma rede sem fio pública isolada da rede lógica administrativa, o 
que funciona na maioria das vezes. O efeito colateral trazido por essa prática é o 
uso indiscriminado do link de Internet pela comunidade, como numa lan house. página
189
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 188-193, junho de 2019
Capítulo XXX - O uso prático do traffic control (tc) para evitar o colapso de links de Internet em
regiões próximas a áreas residenciaisQue é agravado em instituições públicas muito próximas a residências, que fazem 
uso do sinal de rede sem fio aberto como doméstico, até mesmo replicando o sinal 
da rede sem fio. Este trabalho apresenta uma solução alternativa para esta última 
situação.
Na Universidade Federal de Alagoas (UFAL), numa Unidade Educacional na 
cidade de Penedo-AL, existem alguns prédios de um ou mais pavimentos localizados 
muito próximos a residências. Para atender a demanda da comunidade acadêmica 
nesses pavimentos, alguns access points foram instalados em diversos pontos 
nos prédios, de forma que o sinal wireless propagado alcança com facilidade as 
residências mais próximas. O resultado era o seguinte: a comunidade acadêmica, 
que de fato precisava de um link de qualidade para Internet, estava prejudicada ao 
ponto do trabalho administrativo ser impraticável pela rede sem fio.
Dentre as soluções possı ́ veis, foi preferida uma que fosse compatı ́ vel com a 
força de trabalho disponı ́ vel, para que também fosse possı ́ vel manter a solução 
no local. Assim, decidiu-se por implantar um servidor de rede para trabalhar 
um sistema de cotas dinâmicas via tc (traffic control), ferramenta nativa do 
Linux [Jamhour 1984][Debian.Org 2018]. Os usuários cadastrados usam cotas de 
grupos trabalho, que compartilham um fluxo com valor mı ́ nimo assegurado e 
valor máximo estipulado. Esse valor máximo estipulado poderá ser atingido por 
uma classe de acordo com a disponibilidade do link, ou seja, conforme existam 
as sobras não utilizadas pelas outras classes. Dessa forma, foi possı ́ vel garantir 
cotas mı ́ nimas para os diversos grupos de trabalho e outra para o público geral, 
também garantindo que a banda não utilizada em um grupo de trabalho não 
ficasse subutilizada na instituição. Vale destacar que essa solução é compatı ́ vel 
com pacotes QoS, priorizando e tratando adequadamente os pacotes de voz da 
telefonia VoIP .
Métodos
A concepção da solução foi precedida de um conjunto de ações coordenadas:
• estudar a ferramenta e configurar um servidor para testes em laboratório, 
tomando como base uma experiência anterior semelhante;
• instalar o servidor1 definitivo na Unidade Acadêmica de Penedo-AL.
• definir um cronograma de implantação de forma a não interromper o 
serviço atual por longos perı ́ odos.
• acompanhar o comportamento da solução, adequando-o para atingir o 
resultado esperado.
A solução empregada neste trabalho vem sendo estudada e usada na UFAL 
desde 2010, em uma situação semelhante no Campus Sertão. Nessa oportunidade, 
1 O serviço foi instalado em um servidor HP modelo EliteDesk 800 G1 SFF, com 4 núcleos físicos de 
processamento e 16 GB de memória, placa de rede gigabit PCI Express modelo TG-3468 da TP-Link. O roteador 
e controle de fluxo ocupa em média menos que 5% dos recursos de processamento da máquina disponíveis.página
190
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 188-193, junho de 2019
Capítulo XXX - O uso prático do traffic control (tc) para evitar o colapso de links de Internet em
regiões próximas a áreas residenciaisverificou-se a existência de um conjunto de algoritmos e técnicas para se fazer 
controle de fluxo através da manipulação de tráfego, com foco em QoS (Quality 
of Service) [Jamhour 1984] [Livre 2015]. Após estudo e testes com muitas 
implementações e abordagens diferentes, empiricamente foi selecionado um 
modelo que mais se adequava como solução para o problema.
O modelo lida com um conjunto de algoritmos para a disciplina de 
enfileiramento e filtragem de pacotes, a saber: SFQ (Stochastic Fair Queuing), 
PRIO(Priority Queue) e DS MARK (Diff-Serv Marker) (para QoS) [Kurose and Ross 
2006] [Tanenbaum 2003] compondo um modelo HTB (Hierarquical Token Bucket), 
cujos parâmetros serão alterados pela ferramenta tc, que atua junto ao kernel 
linux e trata essas configurações de manipulação de pacotes. Segundo Jamhour 
[Jamhour 1984], o HTB permite estruturar uma hierarquia de divisão de bandas 
pela concatenação de classes. Além disso, o HTB é considerado substituto do CBQ 
(Class Base Queuing), cuja implementação ineficiente não é considerada mais 
recomendada.
Para implementar a solução, foi criada uma qdisc (queuing discipline) raiz do 
tipo HTB, que é associada a interface fı ́ sica [Livre 2015] [Filho 2007]. Em seguida, 
à classe raiz foram atreladas 2 classes: uma delas representando uma espécie 
de “bypass” do link de Internet (para conteúdos advindos da própria máquina 
servidora) e outra classe que representa o tamanho da largura de banda do link 
de Internet, a qual por sua vez lhe foram vinculadas 6 classes filhas. Cada classe 
filha representa um tratamento diferenciado para cada necessidade particular da 
organização de rede (Figura 1). A partir desse esquema hierárquico já montado, 
foram experimentados alguns parâmetros e ao final foi possı ́ vel alcançar a 
modelagem desejada: melhor utilização do link de Internet e possibilidade de 
tratamento de QoS na rede. É importante destacar que, no contexto deste trabalho, 
a banda de upload não sofria com o excesso de tráfego. Isso porque o mau uso 
da banda se dava via access points, que já apresentam uma limitação natural 
de banda de upload. Entretanto, em outros contextos é possı ́ vel usar a mesma 
ferramenta para disciplinar o upload de forma independente.
Figura 1: Mapeamento das classes de tráfego de rede para solução de cotas.
O código para o modelo criado é o seguinte:página
191
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 188-193, junho de 2019
Capítulo XXX - O uso prático do traffic control (tc) para evitar o colapso de links de Internet em
regiões próximas a áreas residenciaistc qdisc add dev  \$IFOUT root handle 1: htb default 70
tc class add dev  \$IFOUT parent 1: classid 1:1 htb rate 100Mbit
tc class add dev  \$IFOUT parent 1:1 classid 1:10 htb rate 25Mbit ceil 90Mbit prio 1
tc qdisc add dev  \$IFOUT parent 1:10 handle 10: sfq perturb 5
tc class add dev  \$IFOUT parent 1:1 classid 1:20 htb rate 25Mbit ceil 90Mbit prio 1
tc qdisc add dev  \$IFOUT parent 1:20 handle 20: sfq perturb 5
tc class add dev  \$IFOUT parent 1:1 classid 1:30 htb rate 22Mbit ceil 90Mbit prio 1
tc qdisc add dev  \$IFOUT parent 1:30 handle 30: sfq perturb 5
tc class add dev  \$IFOUT parent 1: classid 1:2 htb rate 1gbit ceil 1gbit prio 1
tc qdisc add dev  \$IFOUT parent 1:2 handle 2: pfifo
tc class add dev  \$IFOUT parent 1:1 classid 1:50 htb rate 8Mbit ceil 90Mbit prio 1
tc qdisc add dev  \$IFOUT parent 1:50 handle 50: sfq perturb 5
tc class add dev  \$IFOUT parent 1:1 classid 1:60 htb rate 5Mbit ceil 90Mbit prio 1
tc qdisc add dev  \$IFOUT parent 1:60 handle 60: sfq perturb 10
tc class add dev  \$IFOUT parent 1:1 classid 1:70 htb rate 5Mbit ceil 90Mbit prio 1
tc qdisc add dev  \$IFOUT parent 1:70 handle 70: sfq perturb 5
onde $IFOUT é a interface de saı ́da do router.
Para garantir a implantação do serviço sem interromper o fornecimento de 
acesso à Internet para os usuários, inicialmente configuramos uma cota default para 
tratar todos os dispositivos não cadastrados na rede (que nesse primeiro momento 
era 100% dos dispositivos, por isso, a cota tinha um valor mı ́ nimo assegurado 
de 25 Mbps). Criamos grupos de trabalho para o administrativo, docentes, para 
os serviços do NTI (monitoramento de tráfego, câmeras de segurança, VoIP etc), 
laboratórios de ensino e pesquisa, e para o Centro de Extensão. Depois disso, o 
técnico de TI lotado na unidade cadastrou todos os dispositivos dos usuários, em 
seus respectivos grupos, retirando-os assim do grupo default. Após o perı ́ odo de 
cadastro dos dispositivos, que durou cerca de duas semanas, ajustamos o valor 
mı ́ nimo do grupo default para 5Mbps. Na Tabela 1 estão descritos os grupos 
criados e seus respectivos valores mı ́ nimos e máximos para o link de 100Mbps. Vale 
destacar que o valor máximo não pode ser igual ao valor máximo da banda, porque 
isso causaria o ceifamento do enlace, com o esgotamento da banda necessária aos 
controles de rede externos ao servidor de controle de fluxo
Tabela 1: Grupos de trabalhos definitivos
Grupo de trabalho Banda mínima garantida Banda máxima dinâmica
Administrativo 25Mbps 90Mbps
Docentes 25Mbps 90Mbps
Laboratórios de Ensino e Pesquisa 22Mbps 90Mbps
Centro de Extensão e Cultura 8Mbps 90Mbps
Serviços do NTI 5Mbps 90Mbps
Default (Público não cadastrado) 5Mbps 90Mbpspágina
192
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 188-193, junho de 2019
Capítulo XXX - O uso prático do traffic control (tc) para evitar o colapso de links de Internet em
regiões próximas a áreas residenciaisResultados
Após implantação do controle de tráfego, foi aplicado um questionário para 
os técnicos administrativos da unidade, com o objetivo de avaliar a percepção dos 
usuários quanto a solução adotada. Dentre as perguntas, o questionário pedia para 
os usuários avaliarem o acesso à Internet em 2018, antes da aplicação do controle 
de tráfego, e 2019 após a implantação (A instalação do serviço conincidiu com a 
época de recesso natalino e acadêmico.) A Figura 2 apresenta uma comparação 
desses dois momentos. Observa-se que os usuários perceberam uma melhoria 
significativa na conexão com a Internet após a implantação da solução. 
Apesar do bom resultado, vale destacar que durante o perı ́ odo de implantação 
do serviço de controle de tráfego, o link de Internet esteve offline em alguns 
momentos, devido à problemas com a operadora. Essas quedas no link de Internet 
podem, de alguma forma, ter interferido na percepção dos usuários e no resultado 
da pesquisa.
Do ponto de vista técnico, foi observado que o controle se mostrou eficaz, 
por dividir equalitariamente a banda sem causar maiores prejuı ́ zos aos usuários 
da comuni- dade acadêmica. Além do problema em questão, a solução também 
minimizou o impacto causado por outro problema: o dos downloads via torrents e 
serviços P2P semelhantes dinâmicas se mostrou adequada ao contexto, garantindo 
uma divisão justa e equilibrada do recurso. 
Conclusão
Este trabalho apresentou uma das possı ́ veis soluções para minimizar o 
impacto causado pelo uso indiscriminado da banda de Internet pelos usuários. 
A solução de implantar cotas dinâmicas se mostrou adequada ao contexto, 
garantindo uma divisão justa e equilibrada do recurso. Os setores administrativos, 
laboratórios e docentes, desfrutam de cotas individuais para desenvolverem 
suas atividades, mas também foi mantida uma cota para rede aberta ao público 
acadêmico.
Figura 2: Comparação sobre a satisfação dos usuários com o acesso à Iternet no 
final de 2018 e no inı́cio de 2019página
193
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 188-193, junho de 2019
Capítulo XXX - O uso prático do traffic control (tc) para evitar o colapso de links de Internet em
regiões próximas a áreas residenciaisDestaca-se ainda que esta solução não buscou eliminar os usuários moradores 
da vizinhança dos prédios, porque inevitavelmente estão na mesma condição de 
acesso dos visitantes, terceirizados, dos smartphones de alunos e demais pessoas 
que circulam na universidade. Ao contrário, buscou-se uma solução para garantir 
o trabalho dos usuários devidamente cadastrados, evitando assim colapsos pelo 
mau uso ou pela subtutilização da banda.
Para trabalhos futuros, pretende-se replicar a estratégia para outras unidades 
acadêmicas na UFAL e testar o comportamento da solução em localidades com um 
maior número de usuários.
Referências
Debian.Org (2018). Traffic control. https://wiki.debian.org/TrafficControl. (acessado 
em: 01/02/2019).
Filho, J. E. M. (2007). Controle de tráfego com tc, htb e iptables. https://bit.ly/2C1FFUR. 
(acessado em: 28/02/2019).
Jamhour, E. (1984). Mecanismos de qos em linux: tc - traffic control. https://www.ppgia.
pucpr.br/ jamhour/Pessoal/Mestrado/TARC/QoSIP .pdf . (acessadoe m: 28/02/2019).
Kurose, J. F. and Ross, K. W. (2006). Redes de Computadores e a Internet. Pearson –
Addison Wesley, 3th edition. 
Livre, E. (2015). Controle de tráfego de dados no linux (tc). https://escotilhalivre.
wordpress.com/2015/08/31/controle-de-trefego-de-dados-no-linux-tc-parte-1. 
(acessado em: 15/02/2019).página
194
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 194-199, junho de 2019
Capítulo XXXI - Padronização de tipos de documentos: um passo na direção da desburocratizaçãoPadronização de tipos de documentos: um passo 
na direção da desburocratização1
Cléber M. Tavares¹, Diogo G. Pereira¹, Eliara Maria Tavares¹, Gustavo F. Afonso¹, José 
Roberto P. Ribeiro¹, Michel L. Alves¹
¹Núcleo de Tecnologia da Informação – Universidade Federal de Alfenas – UNIFAL-MG
CEP 37130-001 – Alfenas – MG – Brasil
{cleber.tavares, diogo.pereira, eliara.tavares, gustavo.afonso, jose.
ribeiro, michel.alves}@unifal-mg.edu.br
Resumo
Este artigo descreve a estratégia que está sendo adotada na UNIFAL-MG para atender à Lei da Desburocratização 
e otimizar processos internos relacionados diretamente com a obtenção, armazenamento e reaproveitamento de 
documentos do corpo discente, com o suporte de uma estrutura computacional projetada pelo Núcleo de Tecnologia 
da Informação em conjunto com áreas fim da Instituição.
1. Introdução
O corpo discente atual da UNIFAL-MG é composto por 6760 alunos distribuídos 
em 34 cursos de graduação e 27 de pós-graduação stricto sensu [1]. Todos os 
anos ingressam na Instituição cerca de 1000 novos alunos, considerando tanto a 
modalidade presencial quanto o ensino a distância. A Universidade conta com 4 
campi localizados em 3 cidades da região sul de Minas Gerais e vários polos EAD. 
O processo de ingresso dos calouros compreende o envio de vários documentos 
relativos a admissão (CPF, documento de identidade, histórico escolar do ensino 
médio, etc.) e outros relativos a comprovação de situação socioeconômica para 
aqueles ingressantes por cotas (Extrato bancário, comprovante de rendimentos, 
declaração de imposto de renda, etc.).
Durante a vida acadêmica, tanto os alunos que ingressaram por cotas quanto 
aqueles que ingressaram na ampla concorrência tem à disposição uma série de 
assistências que podem ser concedidas mediante análise e disponibilidade 
orçamentária como alimentação, permanência, creche e material didático. O 
processo de solicitação desses benefícios também passa pelo envio de documentos, 
muitos dos quais já são solicitados no ato da matrícula e outros exclusivos para a 
concessão de algum benefício.
Além desses exemplos, outros com essa mesma dinâmica foram identificados 
e, com isso percebeu-se que o número de sistemas que lidam com o upload de 
documentos de discentes tende a crescer nos próximos anos. Hoje a Instituição 
mantém pouco mais de 21400 documentos digitalizados originados dos uploads de 
arquivos que são feitos por meio de seus sistemas, o que ocupa aproximadamente 
13 gigabytes de espaço em disco, sem contar o espaço necessário para backup. 
Esse volume refere-se a apenas dois períodos de ingresso, 2018/2 e 2019/1.
1 Referência à Lei da Desburocratização e Simplificação, publicada em 08 out. 2018. Disponível em http://
www.planalto.gov.br/ccivil_03/_ato2015-2018/2018/Lei/L13726.htmpágina
195
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 194-199, junho de 2019
Capítulo XXXI - Padronização de tipos de documentos: um passo na direção da desburocratizaçãoA solicitação de um mesmo documento repetidas vezes durante a vida 
acadêmica do aluno vai contra o que preconiza a legislação vigente e pode se 
tornar um problema de ordem jurídica para a Instituição. De acordo com a Portaria 
Interministerial nº 176 [2] e a Lei nº 13726 [3], é proibido exigir do usuário de serviço 
público documentos que já estejam em poder do órgão.
Além disso, a manutenção duplicada de documentos acarreta para a 
instituição  prejuízos: 1) financeiro, uma vez que quanto mais arquivos se mantém, 
mais espaço de armazenamento em disco precisa ser comprado; 2) operacional, 
considerando que esses arquivos precisam ser validados por alguém, a sua 
multiplicidade significa retrabalho de pessoas; 3) social, já que cria para o discente 
uma camada a mais de burocracia, o que leva quase sempre a uma sensação maior 
de ineficiência do serviço público.
Diante desse cenário, e considerando todos os problemas que a manutenção 
isolada dos documentos em diferentes sistemas pode trazer para Instituição, o 
Núcleo de Tecnologia da Informação definiu uma estratégia de padronização de 
tipos de documentos que envolveu a criação de um repositório central de arquivos, 
a convergência de informações entre setores diversos da Instituição e a alteração 
na forma como os diferentes sistemas existentes lidam com a manutenção desses 
documentos.
2. Métodos
A estratégia de padronização dos tipos de documentos envolveu três frentes 
distintas: 1) a definição de uma lista oficial de tipos, elaborada pelas áreas da 
Universidade que realmente fazem uso dos documentos; 2) a criação de um 
repositório central de arquivos integrado a uma estrutura de banco de dados que 
armazena os tipos oficiais definidos pelas áreas responsáveis; 3) a realização de 
ajustes nos sistemas de informação que recebem arquivos para que reconheçam e 
se apropriem do repositório central de arquivos. O desdobramento dessas frentes 
está descrito a seguir.
2.1. A definição dos tipos de documentos
Foram convidados para essa discussão, representantes das três unidades 
administrativas que mais lidam com documentos de alunos, desde o ingresso até 
a conclusão do curso: o Departamento de Registros Gerais e Controle Acadêmico 
– DRGCA, a Diretoria de Processos Seletivos – DIPS e a Pró-Reitoria de Assuntos 
Comunitários e Estudantis – PRACE.
O primeiro contato do discente com a Instituição ocorre por meio da 
DIPS enquanto candidato a uma vaga. A DIPS, por meio do Sistema de Ingresso 
de Calouros, obtém os documentos necessários à matrícula e, no caso de 
candidato optante pelo sistema de cotas, os documentos necessários à análise 
socioeconômica.página
196
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 194-199, junho de 2019
Capítulo XXXI - Padronização de tipos de documentos: um passo na direção da desburocratizaçãoApós o ingresso e durante a sua vida acadêmica, o discente interage com 
o DRGCA, órgão que mantém armazenado todos os documentos do aluno e que 
faz a expedição do seu diploma quando da sua colação de grau. O DRGCA conta 
com o suporte tecnológico do Sistema Acadêmico, o maior sistema de informação 
mantido internamente em operação na Universidade.
Quando necessita de alguma medida assistencial para se manter na 
Instituição, o aluno recorre à PRACE que, mediante a análise de uma vasta lista de 
documentos e de acordo com a disponibilidade orçamentária, faz a liberação de 
benefícios de cunho social. A PRACE se vale do sistema Assistência Discente para 
fazer a gestão das solicitações e a liberação dos auxílios.
Na reunião dessas três áreas foi possível confrontar os tipos de documentos 
atualmente solicitados por elas com a real necessidade documental de cada 
área. Dessa forma conseguiu-se eliminar redundâncias, alinhar as descrições de 
documentos que eram feitas de forma ligeiramente diferente e criar tipos novos, 
dando origem assim à primeira versão de uma lista oficial de tipos, que passou a 
ser reconhecida por todos os sistemas de informação da Universidade.
2.2. A criação do repositório central de arquivos
O repositório central de arquivos é uma estrutura composta de duas tabelas 
criadas dentro do esquema público do Banco de Dados Integrado da Instituição, 
mantido em PostgreSQL [4], e de um espaço em disco alocado em um dos servidores 
do data center institucional. É importante destacar aqui que esse espaço é acessado 
exclusivamente pelos sistemas que fazem a manutenção dos arquivos e não está 
disponível diretamente a partir da web. O esquema dessa estrutura pode ser visto 
na Figura 01.
Figura 01. Esquema do repositório central de arquivospágina
197
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 194-199, junho de 2019
Capítulo XXXI - Padronização de tipos de documentos: um passo na direção da desburocratizaçãoA primeira tabela chamada “arquivodocumento” é responsável por armazenar 
o endereço e outros metadados dos arquivos. Essa estrutura permite que a 
localização real do arquivo seja transparente para o usuário e para os sistemas que 
fazem uso de classes especializadas em manutenção de arquivos. Nessa tabela, 
os atributos “nomepasta” e “nomearquivo” permitem a localização do arquivo no 
servidor e o último atributo “tipodocumento_id” indica o tipo de documento que 
esse arquivo armazena.
A segunda tabela, chamada “tipodocumento” , é responsável por armazenar 
a lista oficial de tipos de documentos elaborada pelas áreas responsáveis, 
conforme descrito anteriormente. O atributo “id” da tabela “tipodocumento” 
serve como chave estrangeira não só para a tabela “arquivodocumento” mas para 
qualquer tabela do Banco de Dados Integrado que registra o upload de arquivos. A 
inclusão ou alteração de tipos nessa tabela é feita exclusivamente pelo Núcleo de 
Tecnologia da Informação como forma de garantir a padronização dos tipos e evitar 
a duplicidade de tipos de arquivos nos sistemas e nos servidores da Instituição. 
2.3. A manutenção nos sistemas de informação
A alteração nos sistemas de informação que lidam com o upload de arquivos 
está ocorrendo basicamente na estrutura de tabelas que armazenam arquivos. 
Normalmente cada sistema faz uso de uma tabela de tipos e de uma tabela de 
arquivos. O que está sendo feito é a adição de um novo atributo na tabela de 
tipos que faz referência ao tipo oficial, armazenado na tabela “tipodocumento” 
do repositório central. Dessa forma, cada sistema pode armazenar uma descrição 
particular de um documento sem perder a relação dele com a lista oficial de tipos. 
A Figura 02 abaixo ilustra essa relação com um esquema. Para os sistemas que não 
utilizam uma estrutura de tipos, a própria tabela de documentos está recebendo 
o atributo adicional, que também garante a relação do documento armazenado 
pelo sistema com um tipo oficial descrito no repositório central.
Figura 02. Esquema da relação entre sistema e repositóriopágina
198
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 194-199, junho de 2019
Capítulo XXXI - Padronização de tipos de documentos: um passo na direção da desburocratização3. Resultados
Embora a estratégia de padronização ainda esteja em fase de implantação, 
os resultados obtidos até o momento se mostram promissores, uma vez que: 1) 
a quantidade de tipos de documentos diferentes diminuiu, passando de 80 para 
58 tipos. Um exemplo é o tipo “extrato bancário” , que dependendo do sistema, 
era descrito como “extrato de conta-corrente dos últimos três meses” , “extrato 
bancário dos meses de novembro e dezembro” ou ainda “extrato bancário do 
candidato” . Após a padronização, apenas o tipo “extrato bancário” passou a 
existir; 2) os discentes que acessaram o sistema da PRACE para solicitar algum 
tipo de assistência estudantil para o semestre 2019/1, já tiveram a experiência 
de ver vários de seus arquivos previamente enviados sendo recuperados pelo 
sistema. Com isso, provavelmente economizaram tempo e dinheiro com a 
obtenção de arquivos que já estavam nos servidores da Instituição. A tendência 
é que o número de arquivos novos que chegam diariamente ao servidor caia 
significativamente; 3) os profissionais que fazem a análise da documentação 
comprobatória da situação socioeconômica do aluno, estão se beneficiando do 
fato de que documentos validados no primeiro envio não precisam ser revalidados 
a cada nova solicitação, já que são reaproveitados, o que otimiza o processo do 
setor evitando o retrabalho e aumentando a agilidade; 4) setores como o DRGCA, 
que precisam de acesso constante aos documentos dos alunos, enxergaram nesse 
movimento a oportunidade de trazer para dentro do Sistema Acadêmico o acesso 
aos documentos enviados e solicitaram ao Núcleo de Tecnologia da Informação 
ajustes no sistema, para que os documentos possam ser acessados pelo usuário a 
partir de um clique na tela de cadastro do aluno.
4. Conclusão
Ainda não há como expressar em números a economia em gigabytes de 
armazenamento, a redução nas horas de trabalho dos servidores, o aumento da 
satisfação dos usuários entre outros aspectos. Essas medições serão possíveis assim 
que o ciclo de implantação estiver completo. Apesar disso é possível concluir que 
a estratégia de padronização é positiva, uma vez que atende à legislação vigente, 
propõe a economia de recursos públicos, racionaliza processos, desburocratiza 
o atendimento público e promove a eficiência dos serviços prestados pela 
Universidade.
O próximo passo é garantir a conformidade desse modelo com a Lei Geral 
de Proteção de Dados [5], de forma que seja possível garantir ao discente 
formas de acesso seguro aos seus próprios documentos e em casos específicos 
obter o seu consentimento em relação aos dados que podem ser extraídos dos 
arquivos enviados. Os papeis do Controlador, do Operador e do Encarregado 
pelo Tratamento de Dados Pessoais na Instituição ainda precisam ser definidos e 
legalmente atribuídos.página
199
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 194-199, junho de 2019
Capítulo XXXI - Padronização de tipos de documentos: um passo na direção da desburocratizaçãoReferências
[1] Portal de Dados Abertos da Universidade Federal de Alfenas. Disponível em http://
sistemas.unifal-mg.edu.br/app/si3/home.php . Último acesso em 17/03/2019.
[2] Portaria Interministerial nº 176 de 25 de junho de 2018. Disponível em: http://www.
in.gov.br/materia/-/asset_publisher/Kujrw0TZC2Mb/content/id/27340041/do1-2018-
06-26-portaria-interministerial-n-176-de-25-de-junho-de-2018-27340030. Último 
acesso em 17/03/2019.
[3] Lei nº 13726/2018. Disponível em http://www.planalto.gov.br/ccivil_03/_ato2015-
2018/2018/Lei/L13726.htm. Último acesso em 17/03/2019.
[4] PostgreSQL Global Development Group. Sistema Gerenciador de Banco de 
Dados PostgreSQL. Disponível em https://www.postgresql.org/. Último acesso em 
22/04/2019. 
[5] Lei nº 13709/2018. Disponível em http://www.planalto.gov.br/ccivil_03/_Ato2015-página
200
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 200-205, junho de 2019
Capítulo XXXII - Planejamento de Projetos Utilizando Uma Ferramenta Baseada no Modelo PMC 
(Project Model Canvas)Planejamento de Projetos Utilizando Uma 
Ferramenta Baseada no Modelo PMC (Project Model 
Canvas)
Daniel Biasoli¹, Ocimar Luiz Zolin²
¹Secretaria Especial de Tecnologia da Informação – Universidade Federal da Fronteira Sul (UFFS)
Rodovia SC 484 Km 02, 609 – 89.812-000 – Chapecó – SC – Brasil
{daniel.biasoli,ocimar.zolin}@uffs.edu.br
Resumo
Este meta-artigo descreve a construção de uma ferramenta, PMC (Project Model Canvas), com a finalidade de 
simplificar a gestão de projetos na Secretaria Especial de Tecnologia da Informação (SETI) da Universidade Federal da 
Fronteira Sul (UFFS). A simplificação, neste caso, é facilitada pela concepção de utilização de um processo de gestão 
de projetos híbrido, que encapsula metodologias de gerenciamento de projetos existentes em setores hierarquizados, 
aprimorando a eficácia dos planejamentos e aumentando o dinamismo e a simultaneidade de projetos, aos quais 
soluções rígidas e engessadas nem sempre podem ser aplicadas. Desse modo, visualizou-se na metodologia do Project 
Model Canvas (PMC) ferramentas eficazes para tornar o processo de gestão de projetos híbrido, encapsulando uma 
estrutura setorizada hierarquizada em um modelo de gestão projetizado, em consonância com a teoria que rege o 
gerenciamento de projetos. Assim, construiu-se uma aplicação Web que auxilia o processo de gestão dos projetos, 
proporcionando mecanismos de análise, monitoramento e interatividade entre os membros de projetos.
Palavras-chave. Project Model Canvas; Gestão de Projetos; Ferramenta Web, Universidade Federal da Fronteira 
Sul.
1. Introdução
Tanto no Serviço Público Federal quanto no mercado privado é fundamental 
se utilizar de ferramentas de gerenciamento de projetos para se modelar, dividir 
e executar as tarefas de maneira mais simples e efetiva, porém, as ferramentas 
existentes no mercado são muito burocráticas e pouco dinâmicas para realizar 
gestão de projetos.
Dentro da SETI, o Departamento de Gestão de Projetos da UFFS (DEPRO) 
exerce um papel mais criativo e habilidoso no que diz respeito ao planejamento, à 
execução e ao monitoramento de projetos.
Kerzner (2006) destaca a importância da criação de um departamento 
que centralize o aprendizado em projetos, como é o caso do DEPRO, bem como 
impulsione e leve a organização à maturidade neste tipo de gestão: O Escritório 
de Gerenciamento de Projetos (EGP), também conhecido pela sigla em inglês PMO 
(Project Management Office). 
No caso do DEPRO, porém, há de se destacar sua falta de autonomia para 
gerenciar tarefas dos setores da SETI, uma vez que não faz parte da árvore 
hierárquica dos demais setores que compõem a Secretaria. página
201
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 200-205, junho de 2019
Capítulo XXXII - Planejamento de Projetos Utilizando Uma Ferramenta Baseada no Modelo PMC 
(Project Model Canvas)Nesse sentido, identificou-se no Project Model Canvas (PMC), criado pelo 
consultor de gerenciamento de projetos José Finocchio Júnior [Finocchio Junior 
2013], ferramentas capazes de encapsular uma estrutura hierarquizada em uma 
realidade pautada na projetização de demandas. Assim, para auxiliar neste 
processo, o DEPRO desenvolveu a aplicação Web chamada PMC, a qual já está 
em uso na SETI e, atualmente, facilita a gestão dos projetos de TI da UFFS, sem 
interferir nas metodologias de gerenciamento das tarefas dos setores internos da 
SETI.
1.1. Estrutura do Artigo
Para uma breve compreensão da criação e uso da aplicação, este artigo 
está dividido em capítulos seccionados. O Capítulo 2 aborda de forma sucinta 
os métodos utilizados para criar e utilizar a aplicação. O Capítulo 3 aborda os 
resultados esperados com a solução e no Capítulo 4 explanam-se as conclusões e 
resultados obtidos com a utilização do PMC.
2. Métodos
Conforme já mencionado, atualmente a SETI possui uma organização 
hierarquizada. Além do DEPRO, da Diretoria de Infraestrutura de TI (DITI) e da 
Diretoria de Sistemas de Informação (DS), a Secretaria conta com um Departamento 
de Gestão Documental (DGDOC), um Setor de Governança (SGTI). Para evitar que 
haja deficiência quanto ao recrutamento de especialistas das áreas funcionais 
para contribuir com seus conhecimentos em projetos, a DS e a DITI apresentam 
setores internos especializados, que proporcionam uma maior facilidade quanto 
à alocação de profissionais necessários em vez de disponíveis, para as demandas 
de TI. Esta hierarquia possui diversos setores especializados (Divisão de Gestão 
de Dados, Suporte, Qualidade e Testes de Software, Redes e Telecomunicações, 
dentre outros), que exigem uma organização adequada para que as demandas 
sejam projetizadas.
De modo a proporcionar uma maior independência dos projetos o 
DEPRO se adequou ao trabalho das diretorias (DS e DITI), tornando os projetos 
mais independentes, diminuindo a distância entre a divisão departamental 
tradicional e o foco nos projetos, proporcionando uma resolução mais rápida das 
lacunas e as quebras de continuidade (gaps) e garantindo maior identificação 
e comprometimento das equipes. Para isso, o DEPRO mantém o portfólio de 
projetos de TI, por meio de monitoramento constante, prestando informações aos 
interessados, elaborando relatórios de acompanhamento, gerando indicadores, 
caminhos críticos e sugerindo correções quando há desvios.
Como as tarefas de projetos são de responsabilidade dos setores, o DEPRO 
utiliza um modelo híbrido de gestão em que o gerenciamento das tarefas está 
dentro do escopo de atividades de cada setor e a gestão e monitoramento dos 
projetos, bem como suas demandas é um intento do próprio DEPRO. Ao definir 
as demandas de um projeto, metas, prazos e entregas são negociados junto aos página
202
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 200-205, junho de 2019
Capítulo XXXII - Planejamento de Projetos Utilizando Uma Ferramenta Baseada no Modelo PMC 
(Project Model Canvas)setores responsáveis. Estas entregas são determinadas por marcos significativos 
nos projetos, os quais são desmembrados em tarefas que são gerenciadas e 
executadas internamente pelos setores. A conclusão de um grupo de tarefas de um 
projeto gera a finalização de um marco, culminando com uma nova atualização 
de status do projeto por parte do DEPRO, que também tem outras incumbências 
em projetos, como elicitação de requisitos, prospecção de alternativas de solução 
existentes no mercado, análise de viabilidade de soluções, negociação de recursos 
e resolução de conflitos.
Para a viabilização do acompanhamento dos projetos, o DEPRO não se deu 
por satisfeito com o acompanhamento realizado via sistema Redmine, já que cada 
setor da SETI mantém uma forma de registrar suas tarefas. Assim, identificou-se 
no Project Model Canvas uma solução para encapsular as informações dos setores 
em marcos de projetos, visualmente compreendidos por uma metodologia que 
propõe uma maneira mais amigável de conceber planos de projetos, trazendo 
rapidamente à tona um modelo mental, permitindo visualizar as dependências 
e ligações de um projeto de forma mais simplificada. Para isso, criou-se uma 
aplicação Web, totalmente adaptada às necessidades da SETI, que gera um canvas 
por meio do registro de informações de cada projeto. Qualquer participante de 
um projeto pode ter acesso à ferramenta, que possui método de autenticação via 
LDAP para usuários da UFFS.
Como cada usuário, em geral, participa de mais de um projeto, a tela inicial do 
sistema, após autenticação, gera um painel com um resumo de todos os projetos 
cadastrados e monitorados via PMC. As estatísticas presentes no painel são geradas 
por meio da ferramenta Power BI e registros de dados mais detalhados sobre os 
projetos, além das entregas, também podem ser inclusos no sistema. Estes dados 
vão desde ocorrências e status reports a anexos e cadastros de lições aprendidas, 
bem como registros de reuniões e cadastros de anexos (documentos de projetos). 
Há de se ressaltar que está sendo desenvolvido um módulo de gerenciamento 
de riscos para cada projeto, o qual deverá prever a ocorrência ou não de riscos 
utilizando a fórmula apontada na Dissertação de Mestrado de Daniel Biasoli 
[Biasoli 2012]. Espera-se concluir este módulo e anexá-lo aos demais, já em uso, 
em uma próxima versão do PMC.
O sistema também traz consigo a possibilidade de vínculos de projetos a 
planos e metas institucionais, como por exemplo, planos diretores de tecnologia 
da informação (PDTICs) ou planos plurianuais (PPAs), além de itens e soluções de 
portfólio cadastrados previamente, conforme pode ser observado na Figura 1, que 
traz uma visão da tela inicial de um projeto cadastrado e em fase de execução, 
sendo monitorado por meio da ferramenta PMC:página
203
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 200-205, junho de 2019
Capítulo XXXII - Planejamento de Projetos Utilizando Uma Ferramenta Baseada no Modelo PMC 
(Project Model Canvas)
Figura 1. Painel do Projeto
3. Resultados
Os resultados esperados pela construção da ferramenta PMC são:
1. Encapsulamento de uma visão hierárquica organizacional em uma estrutura 
projetizada. Por meio da simplificação das informações e otimização da 
rotina, busca-se alinhar os fluxos de trabalho, os quais são monitorados 
continuamente, garantindo a alocação de recursos próprios e cumprimento 
dos prazos.
2. Busca-se facilitar o alinhamento do fluxo de trabalho, utilizando modelos 
de projeto para padronizar os pacotes de gerenciamento ou até mesmo todo 
o projeto.
3. Como existe uma ferramenta de gerenciamento para as tarefas dos setores 
da SETI, os papéis e responsabilidades em projetos limitavam-se às definidas 
por competência e especialidade. Já com a ferramenta PMC, a atribuição dos 
papéis se dá por responsabilidade nos projetos, a fim de evitar dúvidas e 
conflitos entre os membros das equipes de cada projeto.
4. Por meio de inclusão de documentos ou até mesmo da atribuição de papéis 
nos projetos, torna-se facilitada a criação da matriz de responsabilidades, 
permitindo a atribuição e controle das atividades e dos membros das equipes 
responsáveis por cada uma delas.
5. Tornar o processo de gestão e controle de projetos híbrido, independente 
da metodologia de gerenciamento adotada pelos setores da SETI.
6. Organizar as entregas, lembrando os usuários quanto aos marcos de cada 
projeto, priorizando alcançar as metas previamente estabelecidas, por meio 
da ferramenta “Calendário de Entregas.página
204
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 200-205, junho de 2019
Capítulo XXXII - Planejamento de Projetos Utilizando Uma Ferramenta Baseada no Modelo PMC 
(Project Model Canvas)4. Conclusão
Este meta-artigo apresentou, brevemente, a utilização de uma ferramenta 
Web construída pelo DEPRO, a qual foi concebida para tornar híbrido o processo 
de gestão de projetos de TI da UFFS.
Embora cada setor da SETI tenha autonomia para gerenciar as suas próprias 
tarefas, observou-se a necessidade de monitoramento das entregas e marcos dos 
projetos, estabelecendo-se parâmetros confiáveis os quais devem servir de base 
de apoio para previsões e análises estatísticas.
As medições e controles efetuados com o auxílio da aplicação PMC, além 
de servirem para fortalecer o processo decisório, quando as informações são 
devidamente catalogadas, constituem “dados históricos” , alimentando a gestão 
do conhecimento da organização, podendo serem usadas em futuras estimativas 
e, consequentemente, contribuindo para maximizar o retorno de investimento da 
Instituição.
Com o envolvimento de todas as pessoas ligadas ao projeto na criação de 
seu respectivo plano, há um grande acréscimo de conhecimento compartilhado a 
partir de diferentes perspectivas, pois recursos humanos de áreas distintas podem 
interagir entre si com a finalidade de contribuírem para o projeto.
Embora ainda haja paradigmas a serem quebrados, como a construção do 
canvas com toda a equipe de um projeto, bem como sua visualização, a aplicação 
PMC vem minimizar o fato de que uma equipe alocada em um projeto não ocupa 
o mesmo espaço físico. Os membros de projetos tem total acesso à ferramenta, 
possibilitando saber em que estágio do projeto estão e quais os próximos objetivos 
a serem alcançados.
A gestão dos projetos de TI pôde ser padronizada, buscando simplificar e 
otimizar a rotina dos trabalhos. Além disso, os fluxos de trabalho foram alinhados e 
estão sendo monitorados continuamente, sempre que uma demanda é projetizadaA 
gestão dos projetos de TI pôde ser padronizada, buscando simplificar e otimizar 
a rotina dos trabalhos. Além disso, os fluxos de trabalho foram alinhados e estão 
sendo monitorados continuamente, sempre que uma demanda é projetizada.
4.1. Trabalhos Relacionados
Rahimian e Ramsin (2007) elaboraram uma metodologia de desenvolvimento 
híbrida para a criação de softwares para celulares.
Batra et al. (2010) propuseram um framework híbrido para combinar princípios 
tradicionais para prover atividades de planejamento, controle e coordenação e 
utilizar os princípios ágeis como veículo de resposta para o dinamismo e incertezas 
dos requisitos.
Amaral et al. (2011, p.52) elaboraram um modelo referencial para a 
abordagem de gerenciamento ágil para ser adotado em produtos manufaturados, 
porém propõem o uso deste modelo de maneira combinada com um processo de 
planejamento tradicional nos casos em que o produto é complexo.página
205
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 200-205, junho de 2019
Capítulo XXXII - Planejamento de Projetos Utilizando Uma Ferramenta Baseada no Modelo PMC 
(Project Model Canvas)4.2. Perspectivas Futuras
A principal perspectiva futura deste trabalho é monitorar e avaliar o 
desempenho dos projetos institucionais da UFFS. Outra perspectiva é a criação 
de um módulo para gerenciamento dos riscos dos projetos, capaz de calcular a 
previsibilidade de um risco ocorrer ou não em um projeto. Também pretende-
se integrar o PMC à gestão de demandas, inserindo fases anteriores aos projetos 
(surgimento, análise, priorização e autorização).
Referências
Amaral, D. C., Conforto, E.D., Benassi, J. and Araujo, C. (2011) “Gerenciamento ágil de 
projetos: aplicações em projetos de produtos inovadores” . Saraiva, São Paulo.
Batra, D., Xia, W., Vandermeer, D. and Dutta, K. (2010) “Balancing Agile and Structured 
Development Approaches to Successfully Manage Large Distributed Software 
Projects: A Case Study from the Cruise Line Industry” , Communications of the 
Association for Information Systems: Vol. 27, Article 21.
Biasoli, D (2012) “Avaliação de Ações Preventivas de Riscos Utilizando Teoria de Decisão 
e Redes de Petri Coloridas” , Dissertação de Mestrado, UFSM, Brasil.
Finocchio Junior, J. (2013) “Project Model Canvas - Gerenciamento de Projetos Sem 
Burocracia” , Alta Books, Brasil.
Kerzner, H. (2006) “Gestão de projetos: as melhores práticas” , 2. ed., Bookman, Porto 
Alegre, Brasil.
Project Builder (2019) “Guia definitivo do Project Model Canvas” , https://www.
projectbuilder.com.br/.
Rahimian, V. and Ramsin, R. (2008) “Designing an Agile Methodology for Mobile 
Software Development: A Hybrid Method Engineering Approach”,  Second 
International Conference on Research Challenges in Information Science (RCIS), 
Marrakech, Page(s): 337 – 342.página
206
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 206-211, junho de 2019
Capítulo XXXIII - PoC RNP/UFG: Implantação do G Suite por meio da adesão ao serviço da Plataforma 
Nasnuvens/RNPPoC RNP/UFG: Implantação do G Suite por meio da 
adesão ao serviço da Plataforma Nasnuvens/RNP
Igor R. Vieira¹, Jean T. Lima¹, Ricardo H. D. Borges¹
¹Centro de Recursos Computacionais – Universidade Federal de Goiás (UFG)
Goiânia – GO – Brasil
{igor_vieira, jeanteixeira, ricardoborges}@ufg.br
Resumo
Manter o serviço de e-mail de uma IFES é uma tarefa árdua e dispendiosa. O esforço para que o serviço esteja 
sempre funcional, disponível e com qualidade aceitável é desproporcional à força de trabalho e recursos existentes 
para a grande maioria das instituições. Este trabalho tem por objetivo descrever os passos, dificuldades e estratégias 
adotadas no processo de migração do serviço de e-mail anterior (Zimbra Collaboration Server) para o G Suite for 
Education, por meio da adesão ao serviço disponibilizado na Plataforma Nasnuvens, viabilizado por uma PoC 
entre a UFG e a RNP.
Palavras-chave. E-mail, implantação, migração, G Suite, Zimbra, Nasnuvens, RNP.
1. Introdução
O serviço de e-mail institucional da UFG (Zimbra Collaboration Server) 
possuía algumas limitações, especialmente no que se refere aos seus recursos 
e capacidade de armazenamento (caixas limitadas à 2 GB por usuário), sendo 
alvo de constantes reclamações por parte da comunidade usuária e levando-os, 
muitas vezes, a utilizarem um serviço de e-mails não institucional para envio de 
mensagens eletrônicas relacionadas ao contexto de trabalho, em inobservância à 
legislação vigente.
Outra limitação do serviço de e-mail institucional existente é que, por 
restrições técnicas e de infraestrutura, ele era disponibilizado apenas para 
servidores (docentes e técnicos-administrativos) e colaboradores, deixando um 
conjunto de aproximadamente 35 mil estudantes desprovidos do serviço, com 
destaque para estudantes da pós-graduação.
Um levantamento realizado pela equipe técnica do Centro de Recursos 
Computacionais (CERCOMP/UFG), em janeiro de 2019, demonstrou que das 
4.850 contas de e-mail institucional (Zimbra) de servidores, apenas 11% deles 
cadastraram esse e-mail institucional (@ufg.br) como e-mail preferencial junto 
ao Portal UFGNet (portal integrador dos sistemas institucionais da UFG). Outra 
observação é que aproximadamente 64% desses usuários utilizavam um e-mail 
particular (@gmail.com) como e-mail preferencial cadastrado. Esses dados 
demonstram a baixa utilização do serviço de e-mail institucional existente 
considerando as limitações já apresentadas.página
207
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 206-211, junho de 2019
Capítulo XXXIII - PoC RNP/UFG: Implantação do G Suite por meio da adesão ao serviço da Plataforma 
Nasnuvens/RNPDiante desse cenário, considerando as constantes reclamações da 
comunidade usuária, as limitações e falta de confiabilidade no serviço de e-mail 
institucional disponibilizado na infraestrutura do Data Center da UFG, bem como 
a necessidade de otimização da força de trabalho de TI alocada para manutenção 
desse serviço, foram iniciadas algumas tratativas, por meio da Secretaria de 
Tecnologia e Informação (SeTI/UFG), junto à Rede Nacional de Ensino e Pesquisa 
(RNP), para viabilizar a implantação do G Suite For Education [1] na UFG, incluindo 
o serviço de e-mail (Gmail), por meio da adesão ao serviços da Plataforma 
Nasnuvens.
As tratativas negociadas contemplaram tanto a assinatura do Termo 
Associativo RNP , por parte da UFG, como a realização de uma Prova de Conceito 
(PoC - Proof of Concept), com o apoio da RNP e supervisão técnica de uma empresa 
parceira, indicada pela RNP e com experiência nesse processo.
Lançado oficialmente no Fórum RNP 2018, o Nasnuvens tem como objetivo 
ampliar a oferta de serviços em nuvem para as instituições de ensino e pesquisa 
através de um modelo baseado em consumo. Sua interface web permite atender 
às necessidades dos gestores de TIC, pesquisadores, professores e estudantes de 
pós-graduação, para acesso aos serviços disponibilizados em um único ambiente 
de fácil acesso e seguro. Inicialmente serão disponibilizados alguns dos serviços 
federados da própria RNP , como Edudrive, MConf, Compute e FileSender, além de 
ferramentas de grandes provedores, como Office 365 da Microsoft e G Suite for 
Education do Google [2].
2. Métodos
O processo de migração do serviço de e-mail institucional da UFG estruturou-
se com as seguintes etapas, respectivamente: preparação da base LDAP de 
usuários, adequação do método de autenticação, migração das contas de e-mail, 
disponibilização do serviço e migração das mensagens dos usuários, conforme 
representado na Figura 1.
Figura 1. Representação do processo de migração de e-mail na UFG
2.1. Preparação
Na etapa inicial foram realizadas pesquisas sobre alternativas disponíveis 
para migração de um serviço de e-mail já existente para o G Suite, bem como 
experiências de outras instituições de ensino que realizaram a implantação do 
G Suite, seja com equipe própria ou com contratação de consultoria, em acordo 
assinado diretamente com o Google.página
208
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 206-211, junho de 2019
Capítulo XXXIII - PoC RNP/UFG: Implantação do G Suite por meio da adesão ao serviço da Plataforma 
Nasnuvens/RNPApós o levantamento dessas informações preliminares, foram definidas as 
ferramentas, o processo de migração das contas e das caixas de e-mail, e a forma 
de utilização do serviço. Nesse processo foram identificadas diversas alternativas, 
cada uma com vantagens e desvantagens, levando em consideração que a escolha 
poderia impactar diretamente no processo de migração ou no surgimento de 
etapas não previstas.
Uma das primeiras definições foi a segmentação do processo em três 
etapas de implantação, conforme o público usuário a ser atendido: 1.º servidores 
(docentes e técnicos-administrativos) e colaboradores; 2.º estudantes (graduação 
e pós-graduação); 3.º estudantes egressos. Para cada público também foram 
definidos domínios específicos: @ufg.br, @discente.ufg.br e @egresso.ufg.br, 
respectivamente. Posteriormente, foram encaminhadas as seguintes atividades e 
definições, a saber:
• Migração das contas:
Havia duas alternativas disponíveis, a primeira utilizando os Web Services 
disponibilizados pelo Google [3], para os quais seria necessária a criação das 
contas por meio de uma rotina. A segunda por meio de sincronização entre bases 
de diretórios, utilizando a ferramenta Google Cloud Directory Sync. Após avaliação 
da equipe técnica, decidiu-se pela segunda opção para migração de contas, tendo 
a base LDAP como a principal fonte de dados para o serviço de e-mail.
• Migração das mensagens:
Existem diversas ferramentas para esta finalidade, tanto gratuitas quanto 
pagas. Para esta atividade, considerando principalmente limitações orçamentárias 
e financeiras e apoiando-se no relato de experiência da Universidade de São Paulo 
(USP) sobre o uso da ferramenta G Suite Migration For Microsoft® Exchange [4], 
optou-se inicialmente por essa ferramenta gratuita para migração das caixas de 
e-mail (mensagens).
• Adequação da base LDAP:
Com a escolha da ferramenta Google Cloud Directory Sync para migração das 
contas de e-mail foi necessário a adequação da base LDAP para o funcionamento 
da ferramenta. Para tanto, criou-se rotinas para tratamento dos dados necessários 
e a retirada de inconsistências que poderiam surgir com a nova estruturação da 
base. Alguns atributos precisaram ser atualizados como, por exemplo, “mail” (para 
considerar apenas e-mails institucionais “@ufg.br”) e “location” (para considerar 
a lotação atual de cada servidor).
• Método de autenticação:
Havia três alternativas de autenticação disponíveis: a) diretamente no 
GMail/Google, mas que no caso de alteração de senha não seria possível sua 
sincronização com a base de usuários LDAP; b) por meio do serviço de autenticação 
da Federação CAFe; c) por meio do serviço de autenticação do Portal UFGNet, o 
qual necessitava de atualização da versão do CAS, devido a exigências de versões página
209
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 206-211, junho de 2019
Capítulo XXXIII - PoC RNP/UFG: Implantação do G Suite por meio da adesão ao serviço da Plataforma 
Nasnuvens/RNPmais atuais do protocolo SAML. Considerando a única opção que não apresentava 
restrições, optou-se por integrar o G Suite ao Shibboleth, serviço de autenticação 
da Federação CAFe, tendo como principal fonte de dados a base LDAP . 
• Plano de Comunicação:
Com a definição dos procedimentos a serem seguidos no processo de 
migração do serviço de e-mail, passou-se então à elaboração do Plano de 
Comunicação para informar à comunidade usuária sobre o quê, quando e como 
se dariam essas mudanças, bem como sobre a divulgação dos serviços e recursos 
disponíveis no G Suite.
Essas informações foram disponibilizadas aos usuários dez dias antes do 
início do processo de migração e contou com diversas estratégias: notícias em 
destaque no Portal UFG, envio de e-mail marketing e campanhas nas redes sociais 
da Universidade.
2.2. Migração piloto e testes internos
Para reduzir o impacto de possíveis erros no processo de migração, foi 
realizada a execução do processo em um escopo menor, abrangendo inicialmente 
a equipe técnica da área de TI vinculada ao  CERCOMP e à SeTI. Essa estratégia 
teve um papel fundamental na avaliação preliminar de problemas e realização de 
alguns ajustes antes da migração efetiva para toda a Universidade. 
Nesta etapa, após criação das contas e início do processo de migração 
das caixas de e-mail (mensagens), foram disponibilizadas ao “grupo piloto” as 
aplicações do G Suite para o uso e realização de testes. Durante os testes já foram 
configurados e disponibilizados os links permanentes de acesso aos serviços 
(Gmail, Drive, Agenda, Grupos e Sites). Nessa atividade também foram observados 
alguns problemas relacionados à preparação da base LDAP , bem como falhas de 
configurações do método de autenticação. Todos os erros encontrados nesta etapa 
foram solucionados.
2.3. Migração
Após as etapas de preparação e migração piloto, colocou-se em execução o 
processo de migração. A criação das contas de e-mail aconteceu tranquilamente, 
validando o processo de preparação da base LDAP . Contudo, o processo de 
migração das mensagens não aconteceu como esperado. A utilização do G Suite 
Migration For Microsoft® Exchange [4] foi muito trabalhosa devido ao fato que a 
maioria das caixas de e-mail tinham um grande volume de mensagens de tamanho 
pequeno (82% das mensagens menores que 50 KB). Após otimizar o serviço IMAP 
do Zimbra para um grande volume de requisições, tentou-se executar novamente 
o processo, utilizando a mesma ferramenta, mas agora considerando um intervalo 
de tempo de sete dias, decrescente ao dia de início da migração.página
210
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 206-211, junho de 2019
Capítulo XXXIII - PoC RNP/UFG: Implantação do G Suite por meio da adesão ao serviço da Plataforma 
Nasnuvens/RNPApós várias tentativas sem sucesso, tendo-se passado quatro dias da criação 
das contas, optou-se por usar a ferramenta de migração de dados disponível no 
próprio G Suite [5], a qual correspondeu à expectativa e executou o processo 
com sucesso. Para o seu adequado funcionamento, foi necessário dividir as 
caixas de e-mail em arquivos “.csv” de no máximo 1000 entradas e submetê-los 
gradativamente no mesmo processo de migração. Apesar desse método apresentar 
logs de erros com menor nível de detalhes ele possibilitou maior agilidade do 
processo, sendo que em dois dias foi possível a migração de um período de 3 meses 
de mensagens. Em seguida submeteu-se todas as mensagens restantes, levando 
apenas 4 dias para finalizar o processo de migração.
Dessa forma, o processo de migração total das mensagens para as contas 
criadas no G Suite demorou aproximadamente 7 dias. O Zimbra, antigo serviço de 
e-mail, permaneceu disponível para consulta em um link alternativo, uma vez que 
alguns recursos não foram importados no processo de migração (spam, contatos, 
porta-arquivos e agenda), possibilitando ao usuário exportar manualmente o que 
fosse do seu interesse.
3. Resultados
Além dos problemas no processo de migração das mensagens, relatados na 
seção anterior, observou-se a dificuldade de adaptação da comunidade usuária, 
especialmente no que se refere à migração das “listas de distribuição” do Zimbra, 
as quais foram convertidas para “grupos” no G Suite. Nesse processo, além de não 
importar membros externos (que não eram @ufg.br) os grupos não foram criados 
com as configurações padrões do G Suite, gerando desconforto quando do uso de 
e-mails dos grupos.
Apesar dessas dificuldades os resultados foram satisfatórios e as atividades 
do processo aconteceram dentro dos procedimentos e cronograma previstos, 
ainda considerando que o processo de implantação do G Suite na UFG foi realizado 
como uma PoC. Na primeira etapa, foram criadas aproximadamente 4.850 
contas de e-mail (para servidores e colaboradores) e migradas mais de 2.760.000 
mensagens, num total de aproximadamente 1,2 TB de dados, com envolvimento 
de 4 analistas de TI no processo.
3.1. Agradecimentos
Agradecemos à RNP e à empresa Grupo Binário que de maneira muito 
profissional e comprometida apoiaram e supervisionaram tecnicamente as 
atividades planejadas para a execução da PoC, possibilitando o sucesso do projeto 
aqui apresentado. página
211
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 206-211, junho de 2019
Capítulo XXXIII - PoC RNP/UFG: Implantação do G Suite por meio da adesão ao serviço da Plataforma 
Nasnuvens/RNP4. Conclusão
Em geral, o novo serviço de e-mail (Gmail) e os recursos disponibilizados 
pelo G Suite tiveram boa receptividade por parte da comunidade usuária. Com 
a finalização do processo para a 1.ª Etapa, pode-se notar maior confiança na 
utilização de um produto já conhecido pelos usuários e uma maior adesão ao 
serviço, favorecendo a não utilização de contas não institucionais. Na execução da 
2.ª etapa foi possível disponibilizar, via Portal UFGNet, a criação da conta de e-mail 
para mais de 30 mil estudantes da UFG.
Além dos benefícios citados, o painel administrativo da Plataforma 
proporciona maior comodidade e recursos aos administradores do serviço, 
disponibilizando relatórios de controle de uso. Também possui ferramentas de 
auditoria que permitem mitigar dúvidas sobre o funcionamento do serviço, com 
controles ajustáveis de acordo com as regras de negócio da instituição e as normas 
de segurança da informação.
O novo serviço disponibilizado, que possui o e-mail como principal produto, 
amplia as possibilidades de recursos para comunicação, ensino e pesquisa na 
UFG [6], de forma a dinamizar e favorecer a integração entre pessoas, processos e 
sistemas.
Referências
[1] Google. G Suite for Education. Disponível em: <https://edu.google.com/intl/pt-
BR/products/gsuite-for-education>. Acesso em: 11 de março de 2019.
[2] RNP apresenta novo conceito em computação em nuvem no Fórum 2018. 
Disponível em: <https://www.rnp.br/destaques/rnp-apresenta-nasnuvens-
forum-201>. Acesso em: 13 de março de 2019.
[3] G Suite Developer. Disponível em: <https://developers.google.com/gsuite/>. 
Acesso em: 12 de março de 2019.
[4] G Suite. G Suite Migration for Microsoft® Exchange.  Disponível em: <https://tools.
google.com/dlpage/exchangemigration>. Acesso em: 12 de março de 2019.
[5] G Suite. Migrar de outros provedores de webmail para o G Suite. Disponível em: 
<https://support.google.com/a/answer/6351474?hl=pt-BR>. Acesso em: 12 de março 
de 2019.
[6] Novo serviço de e-mail da UFG já está em funcionamento. Disponível em: <https://
www.ufg.br/n/113358-novo-servico-de-e-mail-da-ufg-ja-esta-em-funcionamento >. 
Acesso em: 15 de março de 2019.página
212
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 212-217, junho de 2019
Capítulo XXXIV - Portal de Dados Abertos UFGPortal de Dados Abertos UFG
Bruno N. Machado¹, Jhonny L.Cabral¹, Juarez E. Lima¹, Ricardo H. D. Borges¹
¹Centro de Recursos Computacionais – Universidade Federal de Goiás (UFG)
Avenida Esperança, s/n, 74.690-900 - Goiânia - GO - Brasil 
{bnunes, jhonny, juarez, ricardoborges}@ufg.br 
Resumo
A publicação de dados abertos é uma realidade nos órgãos públicos, incluindo as instituições de ensino superior, 
atendendo aos anseios da Lei de Acesso à Informação. Entretanto, existe o desafio de publicar dados nos meios 
digitais em um formato que, de fato, possa ser manipulado e reutilizado por outros indivíduos ou sistemas. Neste 
contexto, este artigo apresenta o Portal de Dados Abertos UFG que tem por objetivo a publicação espontânea de 
dados dos sistemas institucionais da UFG em formatos manipuláveis. O Portal de Dados Abertos UFG está em fase 
final de desenvolvimento e permitirá a publicação de dados nos formatos .csv e .json. Atualmente, os conjuntos de 
dados estão sob avaliação e classificação para posterior publicação no Portal.
Palavras-chave. Dados Abertos, publicação, manipulação, Web.
1. Introdução
A política de abertura de dados no Brasil veio atender aos anseios da Lei 
12.257/2011 conhecida como Lei de Acesso à Informação (LAI), a qual entrou em 
vigor em 16 de maio de 2012 e criou mecanismos que possibilitam a qualquer 
pessoa, física ou jurídica, sem necessidade de apresentar motivo ou finalidade, 
o recebimento de informações públicas dos órgãos e entidades [Brasil b]. Nesse 
contexto, se encontra a Política de Publicação de Dados Abertos estabelecida pelo 
Decreto n. 8.777, de 11 de maio 2016 [Brasil a].
O termo “dados abertos” tem estado cada vez mais em evidência devido o 
movimento global de transparência e governo aberto, mas há divergências quanto 
a sua compreensão. Segundo a Open  Knowledge  International [Open Knowledge], 
dado aberto é qualquer dado que pode ser livremente utilizado, reutilizado e 
redistribuído por qualquer um. No âmbito deste trabalho, os “dados” aqui referidos 
são todos aqueles que estão sob a guarda da Universidade Federal de Goiás (UFG), 
fruto das atividades de ensino, pesquisa, extensão e administrativa, armazenados 
nas bases dos seus sistemas institucionais.
Em abril de 2017, a UFG desenvolveu seu plano de dados abertos, definindo 
como a política de dados abertos seria implantada na instituição. Em paralelo, o 
Centro de Recursos Computacionais (CERCOMP) desenvolveu mecanismos para a 
publicação desses dados abertos, que serão disponibilizados no  Portal de Dados 
Abertos UFG (https://dados.ufg.br).
Este trabalho apresenta os procedimentos que estão sendo adotados para 
a publicação dos dados da UFG, a fim de que estejam disponíveis e possam ser página
213
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 212-217, junho de 2019
Capítulo XXXIV - Portal de Dados Abertos UFGconsumidos por toda a comunidade, estando organizado da seguinte forma: a 
seção 2 apresenta a metodologia para publicação dos dados no Portal de Dados 
Abertos UFG. Já na seção 3 são discutidos os resultados obtidos até a fase atual do 
trabalho. Por fim, a seção 4 traz as conclusões e trabalhos futuros.
2. Métodos
A abertura e publicação dos dados apresentada neste trabalho segue os 
princípios discutidos por [Lóscio et al. 2018] e está dividida em quatro fases: 
preparação, criação, avaliação e publicação dos dados. Uma breve descrição e os 
resultados dessas fases são apresentados nas seções seguintes.
2.1 Preparação
Nesta fase são identificadas as demandas de dados, por meio de análises de 
solicitações de acesso a informação, demandas internas da instituição, interações 
com potenciais consumidores desses dados e o próprio Plano de Dados Abertos. 
Posteriormente, são identificados conjuntos de dados em potencial, agrupando 
as demandas que dizem respeito a itens de dados similares em um mesmo 
conjunto de dado. Por fim, é definido a prioridade dos conjuntos de dados que 
serão disponibilizados. Vale destacar que a análise das solicitações é realizada à 
luz das diretrizes definidas no Plano de Dados Abertos e sob consulta ao órgão 
responsável pela classificação e publicidade ou não de determinado dado.
2.2 Criação
Nesta fase é realizada a modelagem dos dados utilizando GraphQL [GraphQL a 
query language}, onde são avaliadas as propriedades de cada demanda, agrupando 
as propriedades semelhantes e eliminando as propriedades redundantes. A seguir, 
são identificadas as fontes de dados de origem e realizado o mapeamento da 
estrutura para sua extração.
É importante se atentar a esta fase, pois os dados mapeados podem já existir 
no GraphQL, caso contrário é necessário realizar o mapeamento e estabelecer a 
correspondência entre as propriedades do esquema do conjunto de dados e as 
propriedades das fontes de dados de origem. Além disso, é desenvolvido o artefato 
dicionário de dados, que contemplará a semântica das propriedades envolvidas 
em cada conjunto de dados, para promover o entendimento daquele conjunto de 
dados.
Após o mapeamento e o desenvolvimento do dicionário de dados, os dados 
e o dicionário correspondente são disponibilizados para avaliação do usuário 
responsável pelas informações institucionais. Essa disponibilização é realizada 
por meio de uma ferramenta desenvolvida pelo CERCOMP para manipulação, 
seleção e avaliação dos dados e dicionário correspondente.página
214
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 212-217, junho de 2019
Capítulo XXXIV - Portal de Dados Abertos UFG2.3 Avaliação
Conforme o guia para publicação dos dados [Pires 2015], todo dado que 
é público deve ser aberto, mas nem todo dado é público. A legislação brasileira 
trata como exceção à abertura de dados particulares, capazes de identificar 
indivíduos, ferir seu direito à privacidade, sua honra, dados considerados sigilosos 
ou dados que possam comprometer a segurança nacional. Assim, nesta fase, são 
identificados os dados considerados sensíveis, que necessitarão de consulta ao 
guardião dos dados da instituição e/ou legislação para decidir sobre sua permissão 
para publicação ou não.
Além disso, a depender de como foi demandado esse conjunto de dados, 
é realizado uma avaliação quanto a qualidade dos dados, seguindo os critérios 
previamente estipulados, como por exemplo, completude e a consistência dos 
dados. É avaliado também se o conjunto de dados que está sendo preparado 
para publicação atende as necessidades apontadas na solicitação de acesso 
à informação. Por fim, é avaliado o dicionário de dado correspondente para 
verificação da semântica correta daquele dado.
Atualmente, a avaliação dos dados a serem publicados é realizada de forma 
manual. Entretanto, está em discussão o desenvolvimento uma ferramenta para 
apoiar a etapa de avaliação e posteriormente autorização da publicação dos 
dados. O resultado da avaliação indicará se o dado poderá seguir para publicação 
no Portal de Dados Abertos ou se deverá retornar para a fase de criação para 
realização de correções ou melhorias identificadas.
2.4 Publicação
A fase de publicação dos dados abertos, consiste em publicar no Portal de 
Dados Abertos UFG os conjuntos de dados das bases dos sistemas institucionais 
da Universidade conforme demanda identificada e o dicionário de dados 
correspondente. Tim Berners-Lee propôs um princípio que visa categorizar o nível 
de abertura de um dado conhecido como “5 Estrelas dos Dados Conectados” (5 
Stars Linked Data) [Berners-Lee 2006].
Berners-Lee estabelece que a qualidade de um dado está relacionado 
com a sua capacidade de conexão. Assim, de acordo com o Índice 5 Estrela, um 
dado estruturado e em formato aberto como JSON ou CSV possuem 3 estrelas. 
Para chegar a 4 estrelas deve obedecer padrões estabelecidos pela World Wide 
Web Consortium (W3C) utilizando triplas Resource Description Framework (RDF) e 
SPARQL 4. Já para 5 estrelas, é necessário dados interconectados. Para escopo 
desta fase do projeto de publicação dos dados UFG, o Portal de Dados Abertos 
contemplará a publicação dos dados em formato JSON e CSV , correspondendo 
então ao nível de 3 estrelas definidos por Tim Berners-Lee.
Juntamente com os conjuntos de dados publicados, também serão publicados 
dicionários de dados correspondentes para auxiliar o usuário interessado no 
entendimento e uso daquele conjunto de dados. É importante destacar que página
215
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 212-217, junho de 2019
Capítulo XXXIV - Portal de Dados Abertos UFGesta fase de publicação dos dados estabelece um canal de comunicação com os 
consumidores de dados e promove maior transparência da gestão pública, permite 
que a própria sociedade contribua com serviços inovadores ao cidadão, viabiliza 
novos negócios e por fim, atende a Lei de acesso à informação [Brasil b].
3. Resultados
O Portal de Dados Abertos da UFG foi concebido a partir da necessidade de 
publicar dados abertos de forma a facilitar o seu acesso e manipulação por parte da 
comunidade (interna e externa). Para atender esse objetivo, foi implantando uma 
ferramenta web para catalogação de dados, desenvolvido pela Open Knowledge 
Foundation [Open Knowledge], o CKAN [CKAN Association 2014]. Na implantação 
da ferramenta, foi alterado o tema para adicionar a identidade visual da UFG, bem 
como desenvolvido um sistema para auxílio na seleção e avaliação dos dados 
desejados para publicação.
Conforme definido no plano de dados abertos da instituição, os primeiros 
dados a serem abertos serão os dados referentes a vida acadêmica dos estudantes, 
a vida funcional dos servidores, a extensão, à pesquisa e inovação na instituição, 
às pós-graduações, dados financeiros e por fim dados referentes  às demandas 
da Ouvidoria e Fale Conosco da instituição. Todos esses dados estão dispersos 
em vários sistemas institucionais, para cada conjunto de dados foi necessário 
identificar a fonte de dados de origem para extração. Realizada a extração dos dados 
foi desenvolvido o artefato, dicionário de dados, que contemplará a semântica de 
cada propriedade envolvida no conjunto de dados em questão. Posteriormente, 
conforme descrito na seção 2 é realizada a avaliação dos dados para posterior 
publicação.
Atualmente, os dados a serem publicados estão sob avaliação quanto a sua 
sensibilidade, ou seja, se os conjuntos de dados extraídos podem ser publicados em 
sua totalidade ou não. Conforme definido no respectivo processo, essa atividade 
está sendo realizada pelo Centro de Informação, Documentação e Arquivo (CIDARQ), 
órgão responsável pela gestão da informação. Há também o acompanhamento da 
recém criada Secretaria de Planejamento, Avaliação e Informações Institucionais 
(SECPLAN). A arquitetura atual do sistema é apresentada na Figura 1.página
216
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 212-217, junho de 2019
Capítulo XXXIV - Portal de Dados Abertos UFG 
Figura 1. Processo para publicação dos dados abertos
4. Conclusão
Diante da evolução nacional no âmbito da abertura de dados, é importante 
garantir que estes dados tenham a melhor qualidade possível e sua disponibilização 
não seja apenas para visualização pelo cidadão, mas também que possam ser 
manipulados. Nesse sentido, este trabalho apresentou o Portal de Dados Abertos 
que disponibiliza os dados brutos categorizados da UFG em formatos que poderão 
ser processados.
A publicação de dados abertos contribui para diversas finalidades: 
transparência ativa da instituição na disponibilização de dados que são de 
interesse público; diminuição da demanda de elaboração e envio de relatórios 
recorrentes solicitados à área de negócio e órgão de TI; disponibilização dos dados 
para consulta pública e necessidades de pesquisas; por fim, possibilidades de 
integração de processos, órgãos e sistemas. 
Diante do cenário de nova configuração administrativa da UFG, estão sendo 
redefinidas as estratégias de avaliação e classificação dos dados, conforme 
indicado no respectivo processo incorporado ao PDA. Assim, até o presente página
217
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 212-217, junho de 2019
Capítulo XXXIV - Portal de Dados Abertos UFGtrabalho, conforme apontado na seção 3, os dados a serem publicados estão sob 
avaliação para posterior disponibilização no Portal de Dados Abertos.
Para trabalhos futuros, destaca-se o desenvolvimento de uma ferramenta 
para apoiar na avaliação e autorização da publicação dos dados; automatização da 
extração dos dados por meio do uso de serviços e continuar evoluindo a qualidade 
dos dados a serem publicados aumentando o nível de abertura dos dados da 
instituição.
Referências
Berners-Lee, T . (2006). Linked data. https://www.w3.org/DesignIssues/LinkedData.
html . Acesso em: 01 de Março de 2019.
Brasil a. Decreto nº 8.777 de 11, de maio de 2016. http://www.planalto.gov.br/
ccivil_03/_ato2015-2018/2016/decreto/d8777.htm . Acesso em: 05 de Março de 2019.
Brasil b. Lei nº 12.527, de 18 de novembro de 2011. http://www.planalto.gov.br/
ccivil_03/_ato2011-2014/2011/lei/112527.html. Acesso em: 05 de Março de 2019.
CKAN Association (2014). Ckan - comprehensive knowledge archive network. https://
ckan.org/. Acesso em: 04 de Março de 2019.
GraphQL a query language. Graphql (a query language for you api). https://graphql.
org/ . Acesso em: 05 de Março de 2019.
Lóscio, B. F., Burle, C., Oliveira, S., M. I., and Calegari, N. (2018). Fundamentos para 
publicação de dados na web. Comitê Gestor da Internet no Brasil.
Open Knowledge. Open data handbook. https://okfn.org/. Acesso em: 04 de Março de 
2019
Pires, M. T . (2015). Guia de Dados Abertos. Comitê Gestor da Internet no Brasil.página
218
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 218-223, junho de 2019
Capítulo XXXV - Portal de Integração UFG - APIs de serviços para integração de dados e sistemasPortal de Integração UFG - APIs de serviços 
para integração de dados e sistemas
Andrey E. da Silva¹, Bruno N. Machado¹,  Jhonny L. Cabral¹, Juarez E. Lima¹, Lauro R. 
Gomides¹, Ricardo H. D. Borges1
¹Centro de Recursos Computacionais – Universidade Federal de Goiás (UFG)
Avenida Esperança, s/n, 74.690-900 - Goiânia - GO - Brasil 
{andreyestevao, bnunes, jhonny, juarez, laurorg, ricardoborges}@ufg.br 
Resumo
A necessidade de transformação digital em todos os setores da economia, destacando aqui o serviço público 
brasileiro, é uma realidade. Entretanto, existe o desafio da disponibilização de dados nos meios digitais em um 
formato que de fato possa ser manipulado e reutilizado por outros indivíduos ou sistemas. Neste contexto, este artigo 
apresenta o Portal de Integração UFG que tem por objetivo a disponibilização de dados dos sistemas institucionais 
da UFG por meio de APIs, aqui definido como um conjuntos de Web Services. O Portal de Integração UFG está em 
desenvolvimento e já possui uma API com um conjunto de serviços em operação. A expectativa do Portal é atender 
demandas de integração de dados e sistemas tanto internas como da comunidade externa.
Palavras-chave. Integração, serviços, dados, API, web service.
1. Introdução
A evolução tecnológica cria, de forma crescente, a necessidade de 
transformação digital em todos os setores da economia, sobretudo naqueles 
que demandam cuidado extra com a gestão e administração de informações. Em 
paralelo, cada vez mais indivíduos, negócios e governos dependem de soluções 
de software para decisões estratégicas e táticas, assim como para o controle de 
operações cotidianas [Pressman and Maxim 2016]. Nesse contexto, pode-se dizer 
que essa necessidade é também uma realidade das instituições federais de ensino 
superior.
Em sua maioria, essas instituições já compreendem a importância dessa 
evolução e muitas já percebem claramente o papel fundamental das APIs no 
seu modelo de negócio. O termo API é um acrônimo em inglês de Application 
Programming Interface — interface de programação de aplicação, em uma tradução 
literal, que tem como objetivo promover integrações, facilitando e fortalecendo 
parcerias entre as instituições, além de oferecer ao interessado uma série de 
chamadas padrões para extração de dados de determinada base, por meio de 
requisições na Web  [Pires 2015].
Hoje as APIs não são vistas apenas como um termo técnico da Tecnologia 
da Informação (TI), mas como uma ferramenta vital para estratégias de negócios 
e para a transformação digital. Uma API, no escopo deste trabalho, é um conjunto 
de Web Services que de forma técnica, representa uma camada de interação entre página
219
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 218-223, junho de 2019
Capítulo XXXV - Portal de Integração UFG - APIs de serviços para integração de dados e sistemasuma base de dados e um aplicativo que consome esses dados. Nesse contexto, a 
Universidade Federal de Goiás (UFG) vem discutindo e estabelecendo políticas para 
adoção de estratégias e padrões que possibilitem integração com a comunidade 
interna e externa para o consumo destas APIs. Considerando a disponibilização 
de Web Services e, consequentemente, a abertura das bases de dados da UFG, 
este trabalho apresenta a proposta de uma plataforma que visa, não somente, 
mas também, a publicação desses Web Services, agrupados em APIs de serviços, 
denominada Portal de Integração UFG. 
O Portal de Integração UFG corresponde a um dos produtos do Projeto 
de Desenvolvimento Institucional da UFG denominado  “UFG Aberta - Projeto 
de desenvolvimento e implantação de plataforma de dados abertos da UFG” , 
cujo desenvolvimento está sob a responsabilidade do Centro de Recursos 
Computacionais (CERCOMP/UFG). Tal proposta almeja promover o acesso facilitado 
e rápido às bases de dados dos sistemas institucionais da UFG, em que o usuário 
(desenvolvedor/empreendedor) não necessite de acesso ou uma cópia da base de 
dados, ao contrário, bastará realizar uma requisição ao Portal de Integração UFG 
para extrair os dados que o interessa naquele momento. O Portal de Integração UFG 
também facilitará o acesso em tempo real a partes específicas da base, permitindo 
a criação de aplicações que possuem esta necessidade. 
Na seção 2 é apresentada a metodologia para publicação das APIs no Portal 
de Integração UFG. Já na seção 3 são discutidos os resultados obtidos até a fase 
atual do trabalho. Por fim, a seção 4 traz as conclusões e propostas de trabalhos 
futuros.
2. Métodos
A metodologia utilizada para a publicação de APIs apresentada neste trabalho 
segue os princípios discutidos por [Lóscio et al. 2018] e está dividida em quatro 
fases: preparação, criação, avaliação e publicação das APIs. Uma breve descrição 
e os resultados dessas fases são apresentados nas seções seguintes.
2.1 Preparação
Nesta fase são identificadas as demandas de acessos aos dados, por meio 
de solicitações de acesso a informação, solicitações de desenvolvimento de Web 
Services e interações com os consumidores dos serviços por meio de entrevistas. 
Posteriormente, são identificados APIs em potencial, caso ainda não existam, e 
classificados os Web Services com o mesmo domínio em uma mesma API. Por fim 
é definida a prioridade dos Web Services que serão disponibilizados. Vale destacar 
que a análise das solicitações é realizada à luz das diretrizes definidas no Plano 
de Dados Abertos (PDA)1 e sob consulta ao órgão responsável pela publicidade ou 
não de determinado dado ou serviço.
1 O PDA é o documento orientador para as ações de implementação e promoção de abertura de dados, 
obedecendo a padrões mínimos de qualidade para facilitar o entendimento e a reutilização das informações.página
220
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 218-223, junho de 2019
Capítulo XXXV - Portal de Integração UFG - APIs de serviços para integração de dados e sistemasPara entendermos um pouco mais sobre a preparação de uma API, a palavra 
chave desta etapa é a classificação. Basicamente esta etapa se reduz em identificar 
a necessidade da demanda e, caso seja atendida, deverá ser classificada em uma 
API. Assim, um Web Service para consumir dados de estudantes de graduação 
matriculados no ano de 2018 deverá situar-se na API de Estudantes de Graduação, 
por exemplo.
Outro detalhe importante nesta etapa está na granularidade das APIs em 
que se deve classificar os Web Services. É um termo muito utilizado no design 
de software, sendo dividida em granularidade fina e granularidade grossa, de 
acordo com o nível de abstração. Na granularidade fina temos APIs com poucas 
responsabilidades, ou seja, poucas operações, já na granularidade grossa temos 
maiores responsabilidades e, consequentemente, mais operações na mesma API. 
A granularidade depende do escopo de abrangência na disponibilização das APIs e 
também do modelo de negócio adotado. O ideal é encontrar o equilíbrio.
Normalmente é nessa fase que podem ser identificados dados considerados 
sensíveis, os quais necessitarão consulta a um responsável pela confidencialidade, 
integridade e disponibilidade dos dados, podendo a demanda não ser atendida 
pela criticidade destas informações. A este papel dá-se o nome de guardião dos 
dados.
2.2 Criação
Nesta fase é realizada a modelagem das APIs, onde são avaliadas as 
propriedades de cada demanda, agrupadas em propriedades semelhantes e 
eliminadas as propriedades redundantes. Em seguida, são identificadas as fontes 
de dados necessárias para a construção e realizada a modelagem e desenvolvimento 
de acordo com o design de APIs definido pela instituição. Torna-se importante 
ressaltar que o consumo dos dados se dará na maioria dos casos pelo público 
externo, sendo necessário uma padronização no design de APIs, atendendo desde 
requisitos de segurança até a latência de dados requeridos para consumo de cada 
Web Service.
Quando se fala no desenvolvimento de Web Services para o público externo o 
maior desafio é promover a reutilização de rotinas e reduzir o impacto de mudanças 
necessárias no ambiente corporativo interno. Para este cenário, é recomendado a 
criação de uma camada extra que reduza o acoplamento do cliente com o modelo 
de dados da instituição. Desta forma, fica garantido o consumo de dados do 
público externo caso ocorram mudanças no modelo de dados.
2.3 Avaliação
Nesta fase, é realizada a avaliação da API conforme os critérios definidos 
quanto a completude e atualidade dos dados. Aqui é verificado se a API desenvolvida 
atende às necessidades do usuário solicitante. O resultado da avaliação da API 
indicará se a API pode seguir para publicação no Portal de Integração UFG ou se 
deverá retornar para a fase de criação para realização de correções ou melhorias 
identificadas.página
221
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 218-223, junho de 2019
Capítulo XXXV - Portal de Integração UFG - APIs de serviços para integração de dados e sistemas2.4 Publicação
A fase de publicação da API consiste em publicar no Portal de Integração 
UFG o serviço criado e validado conforme a demanda identificada. Para publicar, é 
realizado a classificação no Portal de Integração UFG nas categorias pré-definidas 
(Ensino, Pesquisa, Extensão, Patrimônio, etc) ou, caso seja necessário, cria-se novas 
categorias. A publicação é realizada em dois ambientes, sendo um o ambiente de 
testes e o outro de produção.
A primeira publicação é realizada no ambiente de testes para validação da 
API pelo usuário solicitante com dados não necessariamente atualizados. Após as 
validações do cliente e com sua liberação, é realizada a publicação no ambiente 
de produção com os dados atuais e liberado o acesso para o cliente consumi-los. 
Juntamente com a API é também publicada uma documentação para orientar o 
usuário interessado no uso do referido serviço. É importante destacar que esta 
fase de atualização do catálogo de serviços, seja com o incremento ou atualização 
de APIs, estabelece um canal de comunicação com os consumidores de dados e 
promove melhorias no seu uso.
3. Resultados
O Portal de Integração UFG está sendo desenvolvido a partir da necessidade 
de publicar dados por meio de serviços para realização de integração com outros 
sistemas, tanto institucionais quanto externos. 
Como ferramenta, a equipe técnica optou pelo WSO2 API Manager para o 
gerenciamento de suas APIs, fornecendo vantagens como o controle do ciclo de 
vida, filtro de requisições, versionamento, estatísticas de acesso, limitações de 
requisições e de uso, e também a possibilidade de monetização [WSO2]. Além 
disso, o WSO2 também gerencia o acesso às APIs, possui suporte a autenticação 
e autorização que faz com que cada usuário tenha sua credencial individual 
de acesso, proporcionando assim maior segurança aos dados institucionais 
publicados por meio de serviços. Sua arquitetura atual é apresentada na Figura 1.
Figura 1. Arquitetura do Portal de Integração UFGpágina
222
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 218-223, junho de 2019
Capítulo XXXV - Portal de Integração UFG - APIs de serviços para integração de dados e sistemasAtualmente foi desenvolvida e disponibilizada uma API no Portal de Integração 
UFG cujo serviço apresenta informações relacionadas a estudantes egressos. Tal 
API, denominada “Discentes” , na sua versão 1.0.0, contempla o serviço que reúne 
um conjunto de dados sobre os estudantes egressos da UFG, que até então estavam 
dispersos em vários sistemas de informação da UFG.
A API Discentes foi desenvolvida a partir da solicitação de dados referentes aos 
estudantes egressos da UFG para apoiar o desenvolvimento do sistema “Sempre 
UFG” , um projeto sob a responsabilidade da Fábrica de Software do Instituto de 
Informática (INF/UFG). A partir dos dados fornecidos pela API Discentes, esse 
sistema poderá conferir automaticamente se a identificação informada por um 
usuário corresponde a um egresso da UFG, confirmando se a pessoa em questão 
é ou não um ex-aluno da instituição. Outras possibilidades ainda podem ser 
exploradas no sentido de se estabelecer uma estratégia de comunicação com 
os egressos, bem como realizar manipulação e cruzamento de dados para obter 
informações sobre a inserção e atuação desses egressos no meio acadêmico e no 
mercado de trabalho.
4. Conclusão
Diante da evolução brasileira no âmbito do acesso à informação e da abertura 
das bases de dados dos órgãos públicos, incluindo as instituições federais de ensino 
superior, conforme exigido pela Lei de Acesso à Informação [Brasil], é importante 
promover condições para a interoperabilidade, ou seja, o trabalho em conjunto 
de diferentes bases de dados, por diferentes atores da sociedade. A construção 
de sistemas e soluções cada vez mais robustos, sejam aqueles desenvolvidos na 
esfera governamental, na privada, na acadêmica ou na sociedade civil, dependem 
da interoperabilidade de seus sistemas e bases de dados.
Nesse sentido, este trabalho apresenta o Portal de Integração UFG: uma 
plataforma que disponibiliza os dados das bases dos sistemas institucionais da 
UFG, por meio de APIs de serviços para integração de sistemas. Além de possibilitar 
a integração com outras aplicações, o Portal de Integração UFG contribui também 
para o avanço da ciência permitindo que pesquisas possam ser realizadas com 
dados reais e constantemente atualizados, agregando maior valor aos sistemas 
que consomem tais informações e permitindo o desenvolvimento de ações 
estratégicas.
Atualmente o Portal de Integração UFG disponibiliza uma API, “Discentes” , 
cujo serviço apresenta informações relacionadas a estudantes egressos da 
instituição e está conectada ao sistema “Sempre UFG” . Como trabalhos futuros 
está previsto o desenvolvimento de novas APIs para uso externo e interno da 
instituição classificadas em APIs de Ensino, Pesquisa, Extensão, Patrimônio, dentre 
outras categorias a serem definidas.
Para uso interno da instituição está em fase de preparação, o desenvolvimento 
APIs para apoiar a geração de Guias de Recolhimento da União - GRU - pelos 
sistemas institucionais. Também para uso interno e em fase de preparação, está página
223
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 218-223, junho de 2019
Capítulo XXXV - Portal de Integração UFG - APIs de serviços para integração de dados e sistemaso desenvolvimento de APIs voltadas para integração com o Sistema Eletrônico de 
Informações - SEI -  para apoiar transferência de dados e documentos entre os 
sistemas institucionais internos e o SEI . 
Todavia, a proposta do Portal de Integração UFG não se restringe a apenas 
estes cenários. Sendo assim, destaca-se a extensão do Portal para atender não 
apenas demandas internas da instituição como também demandas externas 
da sociedade, que poderão solicitar o desenvolvimento de APIs para atender a 
necessidade do seu negócio, como por exemplo, empresas de transporte goiano 
que de tempos em tempos solicitam a instituição informações referente aos 
estudantes que estão oficialmente matriculados na instituição, para concessão do 
passe livre estudantil.
Referências
Brasil. Lei nº 12.527, de 18 de novembro de 2011. http://www.planalto.gov.br/
ccivil_03/_ato2011-2014/2011/lei/112527.html. Acesso em: 05 de Março de 2019.
Lóscio, B. F., Burle, C., Oliveira, S., M. I., and Calegari, N. (2018). Fundamentos para 
publicação de dados na web. Comitê Gestor da Internet no Brasil.
Pires, M. T . (2015). Guia de Dados Abertos. Comitê Gestor da Internet no Brasil.
Pressman, R. and Maxim, B. (2016). Engenharia de Software. McGraw Hill Brasil, 8th 
edition.
WSO2. WSO2 Integration Agile Platform. https://wso2.com. Acesso em: 05 de Março de 
2019.página
224
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 224-229, junho de 2019
Capítulo XXXVI - Processo de gerenciamento de riscos: um relato sobre a experiência da STI/UFFProcesso de gerenciamento de riscos: um relato 
sobre a experiência da STI/UFF 
Núbia dos S. R. Santana dos Santos¹, Henrique Oswaldo Uzêda Pereira de Souza²
Superintendência de Tecnologia da Informação – Universidade Federal Fluminense (STI/UFF)
Niterói – RJ – Brasil
{nubiarosa, henriqueuzeda}@id.uff.br
Abstract
Risk management in projects requires the definition of a process to identification, register, monitoring and control 
risks from planning until to conclusion. Risk management is necessary to avoid impediments and failures in projects, 
also allowing the definition of contingency actions, should the event occur. In this context, the article presents a 
risk control process introduced in the project management process of STI / UFF. The study presents the process, the 
support tools used and some metrics and analysis performed.
Resumo
O gerenciamento de riscos em projetos requer a definição de um processo que permita identificar, registrar, 
acompanhar e controlar os riscos desde o planejamento até o encerramento do projeto. O gerenciamento de riscos é 
necessário para evitar impedimentos e falhas em projetos, permitindo também a definição de ações de contingência, 
caso o evento ocorra. Nesse contexto, o artigo apresenta um processo de controle de riscos introduzido no processo 
de gerenciamento de projetos da STI/UFF. O estudo apresenta o processo, as ferramentas de suporte utilizadas e 
algumas métricas e análises realizadas.
1. Introdução
No cenário atual de grande transformação digital, em que aumentam as 
demandas de softwares e serviços de TI, há uma preocupação e exigência pela 
qualidade, segurança e ao mesmo tempo agilidade na entrega. Dessa forma, 
a execução de projetos de TI requer um processo de gerenciamento de riscos, 
que permita a entrega no prazo previsto no cronograma dos projetos, o escopo 
acordado e a qualidade dos produtos a serem entregues. Nisso, o gerenciamento 
de riscos é fundamental no processo de gerenciamento de projetos de TI, que 
engloba vários produtos, além de software. A definição de risco no COBIT 5, 
baseada na ISO/IEC 73, é a combinação da probabilidade de um evento e suas 
consequências. Já o PMI (2017), no PMBOK, define risco como um evento ou 
condição incerta que, se ocorrer, provocará um efeito positivo ou negativo em 
um ou mais objetivos do projeto. A ISO 27005 também aborda que os riscos 
podem ser negativos ou positivos, estes últimos proporcionando oportunidades. 
Já na definição do MPS.br, a gerência de projetos abrange que os riscos de 
projetos devam ser identificados e o seu impacto, probabilidade de ocorrência e 
prioridade de tratamentos sejam determinados e documentados (MPS.br, 2013). O 
monitoramento de riscos é crucial no entendimento do cenário da instituição, de 
oportunidades e limitações, e no auxílio na tomada de decisões importantes que página
225
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 224-229, junho de 2019
Capítulo XXXVI - Processo de gerenciamento de riscos: um relato sobre a experiência da STI/UFFevitem os eventos e/ou os impactos, caso ocorram. Além disso, a análise dos casos 
de sucesso e insucesso é importante para uma orientação e revisão dos processos 
que envolvem a gestão de riscos em projetos. Conforme aborda Pressman (2011), o 
risco é um problema potencial, podendo ocorrer ou não, mas independentemente 
do resultado é importante identificá-lo, avaliar sua probabilidade de ocorrência 
e impacto, estabelecendo um plano de contingência. O Escritório de projetos da 
STI monitora os projetos oriundos de demandas da Universidade, e priorizados 
através do processo de análise de demandas (Damasceno et al, 2018) executado 
pela Governança de TI, usando método ágil (SCRUM). O Scrum é um framework  
Ágil1 com foco em construir software que realmente atenda às necessidades do 
cliente. Na STI-UFF, o Scrum é o modus operandi das equipes, sendo amplamente 
utilizado, em especial, na fase de execução dos projetos. 
Este artigo apresenta um processo de controle de riscos introduzido no 
processo de gerenciamento de projetos da STI/UFF, considerando boas práticas 
e recomendações tais como, PMBOK e MPS.BR. O estudo apresenta o processo, 
as ferramentas de suporte utilizadas, algumas métricas e análises realizadas. A 
próxima seção apresenta o processo de gerenciamento de riscos em projetos 
da STI/UFF e a seção 3 destaca os resultados percebidos com a implantação do 
processo. Por fim, são apresentadas as conclusões. 
2. Métodos
O processo elaborado teve como base parte das recomendações de 
gerenciamento de riscos do MPS-SW(MPS.br, 2013) e do PMBOK (PMI, 2013). O MPS.
br é um programa que objetiva a melhoria de processo de software e de serviços 
no Brasil, e com isso melhorar a capacidade de desenvolvimento de software e 
serviços nas empresas brasileiras, baseando-se na premissa de que processos com 
qualidade tendem a produzir produtos com qualidade. O processo gerenciamento 
de projetos de TI da STI adicionou o gerenciamento de riscos a partir de 2014. 
O Redmine, a ferramenta utilizada no gerenciamento de projetos também 
é utilizada para registro e controle dos riscos associados a projetos. No início do 
projeto os riscos são identificados, cadastrados no Redmine com o preenchimento 
dos campos: título, descrição, probabilidade, impacto, exposição, ações de 
contenção, ações de contingência, categoria e subcategoria (Figura 1). Após o 
cadastro, o relatório de riscos gerado pelo Redmine segue como anexo no plano de 
projeto. Durante a execução do projeto os riscos são acompanhados a cada Sprint 
(ciclo de desenvolvimento Scrum) e as informações atualizadas no Redmine. Caso 
surja algum risco novo, este deve ser cadastrado no Redmine e comunicado nas 
reuniões de acompanhamento de projeto. 
1 http://agilemanifesto.org/history.htmlpágina
226
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 224-229, junho de 2019
Capítulo XXXVI - Processo de gerenciamento de riscos: um relato sobre a experiência da STI/UFF
Figura 1. Tela de cadastro de riscos - Redmine 
Os riscos são categorizados em cinco tipos, sendo eles:
•T
écnico - riscos associados às tecnologias e conhecimentos necessários
para o desenvolvimento, execução e as metas de desempenho.
Subcategorias: tecnologia, complexidade / interface, requisitos, performancee confiabilidade, e qualidade.
•
E
xterno - riscos associados aos fatores e influências externas à equipe do
projeto e à organização. Subcategorias: externo à instituição, terceirização e
fornecedores, priorização, cultural, legislação, cliente e climáticos.
•
 Or
ganizacional – relacionados à própria estrutura da organização
(processos), à política e à gestão dos projetos. Subcategorias: dependências
de projeto, recursos, financiamento e priorização de portfólio.
•Ger
enciamento - riscos associados ao gerenciamento de projetos, desde
o planejamento até a fase de encerramento. Subcategorias: estimativa,
controle, planejamento e comunicação.
•P
essoal - riscos associados aos fatores humanos, competências técnicas e/
ou comportamentais e qualificações. Subcategorias: rotatividade, período
crítico e treinamento.
Os riscos de cada projeto devem estar relacionados aos riscos gerais, que são 
registrados na área específica de riscos no Redmine (Base de Conhecimento - BC). 
A partir disso, pode-se gerar métricas sobre os eventos de riscos registrados em todos os projetos, riscos críticos, entre outras. 
A Figura 2 apresenta o processo de registro de riscos de projetos no Redmine. 
O processo completo, integrado ao processo de gerenciamento de projetos, está disponível no Portal
2 de processos da STI. Quando não contidos, os riscos podem 
ocasionar impedimentos. Os impedimentos de projetos também são cadastrados no Redmine, podendo ter ou não relação com os riscos identificados.  
2 http://www.sti.uff.br/processos-novo/projetos/index.html#diagram/731135fc-9fdb-42dd-a2e9-dd5fb08c9923página
227
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 224-229, junho de 2019
Capítulo XXXVI - Processo de gerenciamento de riscos: um relato sobre a experiência da STI/UFF
Figura 2. Processo de registro de riscos de projetos no Redmine
Utilizando a metodologia Scrum, os riscos de projetos e as respectivas 
mudanças ocorridas na Sprint, são informados a Governança de TI e aos Gestores, 
através de resumo de acompanhamento de projetos (por Sprint) e dos resumos 
mensais. Além disso, a situação e mudanças são comunicadas nas reuniões com 
os gestores. Ações de contenção para mitigar os riscos e de contingência, quando 
o evento ocorre, são realizadas assim que a exposição ou registro de ocorrência 
são identificados e/ou comunicados ao Escritório de Projetos. Em casos urgentes, 
as ações são comunicadas e realizadas, quando possível, rapidamente. 
3. Resultados
Desde a implantação do controle de riscos de projetos no Redmine, os 
projetos de TI (desenvolvimento de software, de infraestrutura e implantação) têm 
os riscos registrados e controlados. Foram registrados e acompanhados cerca de 
312 riscos de projetos. De acordo com os dados obtidos a partir do gerenciamento 
de riscos foi identificado que os riscos mais frequentes estão relacionados à 
questão de tecnologia (Técnico) e recursos (Organizacional) de projetos, seguidos 
de rotatividade da equipe e período crítico, ambos da categoria Pessoal, conforme 
demonstra o gráfico da Figura 3. Verificou-se também que grande parte dos 
impedimentos, oriundos de riscos, estão relacionados aos riscos dessas categorias.página
228
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 224-229, junho de 2019
Capítulo XXXVI - Processo de gerenciamento de riscos: um relato sobre a experiência da STI/UFF
Figura 3. Riscos categorizados e registrados no Redmine
Dessa forma, a análise foi importante para corroborar a necessidade de novas 
ações de mitigação, de contenção e de contingência para esses riscos recorrentes 
em projetos e principalmente para aqueles que vêm causando impedimentos. 
Em alguns casos, como por exemplo, os riscos relacionados à falta de material, 
principalmente em projetos de infraestrutura, as ações são restritas levando a 
aceitação do risco. Nisso, o processo de análise de demandas, executado pela 
Governança de TI, aliado ao processo de gerenciamento de projetos tem sido 
importante para evidenciar a situação dos projetos, os riscos e impedimentos 
associados de modo que, considerando o alinhamento estratégico de TI na 
Universidade, seja realizada a priorização das demandas contemplando também 
a análise de riscos. 
4. Conclusões 
A partir do monitoramento de riscos de projetos de TI pôde-se identificar 
quais são os riscos recorrentes, os que causam impedimentos em projetos 
- necessitando de ações mais eficazes para a mitigação e eliminação - e a 
necessidade de replanejamento de projetos quando a única resposta possível é a 
aceitação. A partir da análise de riscos e impedimentos, pode-se verificar também a 
relevância do registro e comunicação sobre lições aprendidas em projetos. Devido 
a necessidade de realizar uma análise mais qualitativa dos riscos registrados nos 
projetos, compreende-se que os resultados da análise contribuirão para o registro 
de lições aprendidas e consequentemente, da melhoria do processo. Ainda, como 
melhoria do processo, verifica-se a necessidade de adequação da ferramenta para 
a geração de métricas de forma mais otimizada, além da revisão do processo com 
ações mais concretas para compartilhamento de riscos identificados em projetos, 
auxiliando no tratamento dos mesmos. Alguns campos também serão adicionados 
no Redmine para o controle de riscos. Um dos pontos positivos identificados ao 
longo da implantação do processo é percepção da relevância do controle por parte 
dos gerentes de projetos. página
229
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 224-229, junho de 2019
Capítulo XXXVI - Processo de gerenciamento de riscos: um relato sobre a experiência da STI/UFFReferências
ABNT , ABNT ISO/IEC 27005 -Tecnologia da informação – Técnicas de segurança, Gestão 
de riscos de segurança da informação. 2011.
Damasceno, G. M. P .O. Santos, N. S. R. S., Araujo, V. L. N., Governança de TI e a gestão de 
demandas, WTICIFES 2018, Disponível em: https://eventos.unila.edu.br/wticifes2018/
wp-content/uploads/2018/06/97308.pdf , Acesso em: 14 de Mar. 19. 
ISACA, Cobit 5 - Modelo Corporativo para Governanç a e Gest ão de TI da Organização, 
2012.
MPS.br - Guia de implementação, Disponível em: http://www.softex.br/wp-content/
uploads/2013/07/MPS.BR_Guia_de_Implementação_Parte_2_2013.pdf, Acesso: Abr. 
2014.
PMI, PMBOK- Um Guia do Conhecimento em Gerenciamento de Projetos (Guia 
PMBOK®). — Quinta edição, 2013.
Pressman, R.S. Engenharia de software - uma abordagem profissional, 7a ed. – Porto 
Alegre: AMGH, 2011. 
SCRUM - http://www.scrumguides.org/scrum-guide.html, Acesso: Maio de 2011.230
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 230-235, junho de 2019
Capítulo XXXVII - Relato de Experiência: implantação de um sistema de documentação e gestão das 
redes de comunicação da Universidade Federal de Mato Grosso
Relato de Experiência: implantação de um 
sistema de documentação e gestão das redes de 
comunicação da Universidade Federal de Mato 
Grosso 
Willdson Gonçalves de Almeida1, Jeison Gomes dos Santos1, Jonata B. M. dos Santos1
1Secretaria de Tecnologia da Informação – Universidade Federa l de Mato Grosso (UFMT)
78060-900 – Cuiabá – MT – Bras il
{willdson,je ison,jonata}@ufmt .br
Resumo
O gerenciamento das redes lógicas e de telefonia fixa em Instituições de Ensino Superior (IES) exige um g rande 
esforço técnico por parte da equipe responsável devido a sua complexidade. Dessa forma, este artigo apresenta um 
relato de experiência sobre o processo de implantação de uma ferramenta que possibilita realizar a documentação e 
gestão das configurações d as redes de comunicação administradas pela Secretaria de Tecnologia da Informação da 
Universidade Federal de Mato Grosso.
Palavras-chave: Sistema de documentação, Endereço I P, PHPIPAM.
1. Introdução
As redes de dados em ambientes universitários geralmente possuem uma 
estrutura dinâmica e complexa, decorre nte tanto da intercon exão entre os campi  
quanto pela interliga ção dos diversos prédios das unidades ad min istrativas e 
acadêmicas que com põem cada campus.  Nesse contexto,  percebe-se um  aumento 
progressivo do número de equipamentos em curto s períodos de tempo, seja pela 
implantação de novos projetos ou pela  ampliação da estru tura organizacio nal das 
unidades. 
Tanto em ambiente corporativo quanto em órgãos públicos, é comum o 
gerenciamento de centenas de infor mações das suas red es de comunicação 
usando planilhas e arquivos de texto, sob o risco de perda d e informações e 
inconsistência de dad os, sem um controle efici ente e eficaz. Alguns problemas 
comuns nas orga nizações podem ser l istados, como: dificuldade na gestão de 
processos internos ; falta de padro nização de proce dimentos; e, principalmente, 
falta de process os que propiciem uma efetiva gestão do conhecimento e da 
informação [Silva et al. 2018].
Para prover sol uções inovadoras de Tecnologias da Informação e 
Comunicação (TIC ’s) para a comuni dade universitári a, a Universi dade Federal de 
Mato Grosso (UF MT), por meio de sua Secretari a de Tecnologia da Informação (STI) 
tem priorizado a profi ssionalização da gestão dos recursos de tecn ologia da UFMT 
páginapágina
231
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 230-235, junho de 2019
Capítulo XXXVII - Relato de Experiência: implantação de um sistema de documentação e gestão das redes de comunicação da Universidade Federal de Mato Grossotendo, dentre outros propósitos, o compromisso com a eficiência de sua aplicação 
em benefício da administração e da comunidade universitária [UFMT 2018].
Baseado neste contexto, este artigo apresenta um relato de experiência 
da implantação de uma ferramenta gratuita e de código aberto que possibilita a documentação e gestão das redes de comunicação pertencentes à UFMT , com objetivo de aperfeiçoar e maximizar a eficiência do controle e o gerenciamento das informações de configurações das redes lógicas e de telefonia fixa.
2. Métodos
A documentação de rede é um processo importante que tem como finalidade 
gerenciar e transformar as informações em conhecimento para que possa auxiliar os analistas de Tecnologia de Informação (TI) a solucionar as adversidades cotidianas na sua infraestrutura de rede lógica e de telefonia.
Diante disso, foi realizado um levantamento técnico para verificar quais 
as informações de configuração das redes de comunicações seriam relevantes e auxiliariam no gerenciamento e controle das redes, devendo, portanto, serem documentadas. Dessa forma, dividiu-se em dois grupos de informações, sendo: rede lógica e rede de telefonia fixa. A partir desses grupos, definiu-se o escopo das informações que deveriam ser cadastradas, armazenadas e documentadas.
Em relação à rede lógica, foi definido o cadastro das faixas de endereços 
Internet Protocol (IP) que são utilizados, dividindo-os em duas seções de endereços, uma para redes externas (endereços de internet fornecidos pela operadora de telecomunicação) e outra para redes internas (utilizado nas redes privadas). Também, foi necessário cadastrar as redes locais virtuais (VLANs) que são utilizadas em cada faixa de rede. 
Em relação à rede de telefonia fixa, foi definida como requisito a documentação 
das faixas de ramais DDR (Discagem Direta a Ramal) entregues pela operadora de telecomunicação e que estão habilitadas nas centrais telefônicas, agrupadas por campus. Além disso, percebeu-se a necessidade de detalhar os ramais de cada faixa inseridos anteriormente, com seus respectivos dados.
Devido à dimensão da rede e o escopo de informações a serem tratadas neste 
projeto, foi realizado um estudo de softwares livres que atendessem aos requisitos mencionados, destacando-se o PHPIPAM
1. 
O PHPIPAM é um software livre de código aberto, licenciado sob a General 
Public License (GPL), atualmente na versão 1.3.2, lançada em 27/06/2018, que tem como função armazenar e estruturar todas as informações relativas a infraestrutura de TIC de forma dinâmica, organizada e funcional [Simoes 2017].
Esta ferramenta possui suporte aos bancos de dados MySQL e MariaDB,  
além de gerenciar endereços IPv4 e IPv6; possui suporte a separação por seções; exibe automaticamente o espaço livre para subredes; verifica as alterações de 
1 https://phpipam.net/página
232
estado de subrede;  permite controle de acesso às seções e subredes por grupo 
ou usuário; provê Application Programming Interface (API) para integração com 
outros sistemas; gerenciamento de VLAN; encaminha notificações por email; 
permite a criação de campos personalizados; possui traduções para diversos 
idiomas incluindo português; gera logs de alterações e das checagens de estado 
dos dispositivos (ping check) e possui uma interface intuitiva, trazendo uma visão 
agradável das informações disponibilizadas [PHPIPAM 2019].
O software PHPIPAM foi instalado na versão mais atual 1.3.22 em uma 
máquina virtual com sistema operacional Linux Debian 8, hospedado em um 
ambiente virtualizado. Foram feitos os cadastros das informações pertinentes, 
tanto para rede telefônica como para a rede lógica. Depois, foram realizadas as 
personalizações na interface seguindo a identidade visual da instituição, conforme 
ilustrado na Figura 1.
Figura 1. Dashboard personalizado
3. Resultados
3.1 Organização da documentação e gestão de configuração de rede 
lógica
Na seção rede interna, as subredes foram organizadas da seguinte forma:  1º 
nível – Pasta do Campus a que pertence a subrede, 2º nível – rede privada com 
base na RFC 1918  (10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) e no 3º nível a subrede 
aninhada à rede privada. Esta organização pode ser vista na Figura 2.
2  https://sourceforge.net/projects/phpipam/página
233
Figura 2. Visão de subredes em níveis
Na interface de subrede estão disponíveis informações estatísticas de 
utilização dos endereços (Figura 3 – item 1) e uma visão de subrede com os IPs 
livres e os utilizados (Figura 3 – item 2).  Em cada subrede foram adicionados 
os endereços IPs dos hosts, com suas respectivas informações como nome do 
equipamento, endereço Media Access Control (MAC) e descrição (Figura 3 – item 3).
As VLANs cadastradas foram vinculadas às suas respectivas subredes (Figura 
4 item 1), assim como cada subrede foi vinculada a um equipamento (Figura 
4 – item 2), bem como cada equipamento foi vinculado a seu respectivo rack e 
localização (Figura 4 item 3).
Figura 3. Detalhamento de uma subredepágina
234
Figura 4. VLANs, Dispositivos e Racks
3.2 Organização da documentação e gestão de configuração de rede 
lógica
Em relação ao sistema telefônico, foram cadastradas as faixas de ramais DDR 
fornecidas pela operadora e configurada nas centrais telefônicas, separadas por 
campus.  Foram cadastrados os prefixos das faixas, o campus a que pertencem e a 
faixa de ramal, detalhado na Figura 5.
Figura 5. Faixas de ramais DDR
Em cada faixa foram cadastrados os ramais com referência do local onde está 
instalado, o usuário permitido para utilização do ramal, a descrição da tecnologia 
do ramal (analógico ou IP), além da marca e modelo do aparelho, conforme Figura 
6.página
235
 
Figura 6. Informações cadastradas nos ramais
4. Conclusões
Os resultados obtidos neste trabalho confirmam a viabilidade do uso do 
PHPIPAM para documentação e gestão das configurações de rede, tendo em vista 
que a ferramenta possibilitou uma gestão unificada das configurações de rede 
com estatísticas de utilização de endereços IP por subrede, quantidade de VLANs, 
racks, dispositivos e respectivas localizações.
A partir das informações cadastradas no sistema, os gestores de Tecnologia 
de Informação podem antever possíveis investimentos e planejar ações baseadas 
nos dados de escalabilidade da sua rede, além de torná-la gerenciável e adaptável 
a novas soluções tecnológicas do mercado.
Como trabalhos futuros, pretende-se pesquisar outras funcionalidades da 
ferramenta, como o controle dos links de operadoras a integração das localizações 
com a API do Google Maps para exibição em mapa na própria interface do PHPIPAM.
Referências
PHPIPAM (2019). Open-source IP address management - phpIPAM Feature list. https://
phpipam.net/documents/features/, Acesso em 14 de Março de 2019.
Silva, W. C. M. P ., Santos, R. F., Oliveira, L. A. D., Araújo, A. N. L., da Costa Figueiredo, R. 
M., da Silva, E. A., and Borges, K. C. V. S. (2018).  Modelo de gestão do conhecimento 
para o ministério: processo, ferramental e regras. Technical report, Universidade de 
Brasília, Faculdade UnB Gama.
Simoes, E.     (2017). phpIPAM     –     Solução     para     gerência     de     redes.
https://eltonsimoes.wordpress.com/2017/10/23/phpipam-solução-para-gerencia-de-
redes/, Acesso em 14 de Março de 2019.
UFMT (2018).       Relatório   de   gestão   2016   –   2018:     governança, qualidade 
acadêmica e  pluralidade.      Editora da Universidade Federal de Mato Grosso. http://
editora.ufmt.br/download/2019/Relat%C3%B3rio%20de% 20Gest%C3%A3o%20
2016%20-%202018%20-%20eBook.pdf , Acesso em 14 de Março de 2019.página
236
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de DadosSeBaHo Juntando o Util ao Necessário Serviço de 
Backup, Restore e Homologação de Base de Dados
Guilherme Geronimo¹, Gustavo Tonini¹
¹Superintendência de Governança Eletrônica e T .I. e Comunicação - SeTIC Universidade Federal 
de Santa Catarina - UFSC Florianópolis - SC - Brasil
{guilherme.geronimo,gustavo.tonini}@ufsc.br
Abstract
Atualmente soluções comerciais de backup e restauração de bancos de dados (e.g. Oracle Exadata, SAP Replication 
Server, PostgreSQL streaming replication), apesar de entre-garem uma serie de funcionalidades, saio especificas 
para cada solução de banco de dados, impossibilitando estendê-las para outros bancos. Paralelamente, necessita-se 
prover ambientes de homologação e desenvolvimento que devem ser constantemente sincronizado com o ambiente 
de produção, provendo dados fidedignos aos desenvolvedores e administradores de BD. Ambas as demanda são 
tecnicamente similares e, quando implementadas separadamente, acabam consumindo demasiado recurso 
computacional.
Assim, projetamos uma solução extensível a qualquer banco de dados em ambientes Linux, centralizado em uma 
maquina virtual, integrãvel a interfaces web (e.g. PHPMyAdmin) e passível de ser operada pelo usuário final como 
um auto-serviço.
Este artigo aborda o contexto, requisitos, modelagem e implementação desta solução adotada pela Universidade 
Federal de Santa Catarina (UFSC). Modelamos e implementamos a solução utilizando os bancos de dados MySQL, 
PosgreSQL e SyBASE, que permite qualquer usuário restaurar versoes anteriores dos bancos em poucos minutos, sem 
ocupar recursos extras de armaze-namento e sem interferencia humana.
Palavras-Chave: Banco de Dados, Backup e Snapshot
1. Introdução
A existencia de mUltiplas instancias de diferentes tipos de banco de dados 
e uma realidade das IFES. Isto demanda tanto recursos computacionais quanto 
humanos para se prover o servico de forma consolidada, ou seja, possuindo 
procedimentos que garantam (i) o backup frequente e incremental dos dados, sem 
a parada do servico, (ii) a integridade dos dados, garantindo que o backup e valido, 
e (iii) o rápido acesso aos backups, caso necessario. Infelizmente, cada solução 
de banco de dados possui suas proprias ferramentas para tais procedimentos, 
tornando complexa a utilização de multiplos banco na mesma infraestrutura.
Em paralelo, o fato de existir um ambiente de produção acaba conduzindo 
a necessidade de ambientes de desenvolvimento e homologação, onde, alem de 
servir de “vitrine”para novas funcionalidades, possibilita testa-las com a massa de 
dados real. Para estes fins, e necessario que este ambiente reflita o mais fielmente 
a realidade do ambiente de produção, tecnicamente e temporalmente falando. 
No entanto, procedimentos de dump/restore oneram a performance dos bancos, 
apresentam um tempo de processamento demasiadamente alto e consumem 
espaco de armazenamento desnecessarios.página
237
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de DadosFinalmente, o auto-servico virou um requisito imprescindível em qualquer 
solução, nao somente demandado pelo usuario mas muitas vezes e uma medida 
salutar para a equipe de T .I. . A questao e como integrar diferentes interfaces, em 
diferentes linguagens, com o mínimos de alteração das ferramentas?
Tentando sanar estas e outras demandas, resolvemos o problema em nível 
do armazenamento de dados. Propomos uma solução integrada que utiliza o 
sistema de arquivos ZFS [1] para transferir os dados, o sistema de container Docker 
[2] para executar os bancos de dados e o servidor de mensagens ActiveMQ [3] para 
gerenciar as requisições de restauração centralizadamente.
Implementamos um cenário contendo 2 diferentes bancos de dados em 2 
servidores distintos, centralizando seu backup em um ambiente geograficamente 
distribuído e fora do Centro de Dados principal.
Assim, as contribuições deste trabalho estao em:
• Descrever os requisitos do nosso cenario (Seção 2.1), para que o leitor possa 
entender e, possivelmente, comparar nosso ambiente com o seu.
• Explicar as ferramentas utilizadas ressaltando algumas de suas 
funcionalidades (Seções 2.2 - 2.4), criando uma base de conhecimento 
superficial para o leitor entender nossas escolhas.
• Apresentar nossa proposta de solução (Seção 2.5), baseado nos requisitos 
apresentados previamente.
• Descrever os detalhes tecnicos de implementação e configuração da 
infraestrutura, desenhando a ponte entre o planejado e o implementado. 
Apresentando os resultados obtidos (Seção 3).
• Correlacionar as demandas com a solução e apresentar os próximos passos 
da pesquisa (Seção 4).
2. Método
A fim de desenharmos uma solução para a nossa demanda listamos os 
requisitos funcionais e nao funcionais do nosso cenario na Seção 2.1. Em seguida, a 
fim de familiarizar o leitor, apresentamos os conceitos e principais funcionalidades 
do ZFS, Docker e do ActiveMQ nas Seções 2.2, 2.3 e 2.4. Por fim, unimos as demandas 
aos conceitos e apresentamos nossa proposta na Seção 2.5.
2.1. Requisitos
Listamos abaixo os requisitos funcionais e nao funcionais do nosso cenario.página
238
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de Dados2.1.1. Funcionais
2.1.1.1. Backup Frequente: definimos que o pior caso de backups/retenções 
deveria atender a: backup (i) horaria (uma vez por hora) preservado durante 
15 dias, (ii) diaria por 90 dias, (iii) semanal por 1 ano e (iv) mensal por 5 anos.
2.1.1.2. Restore rápido: prover acesso a uma base de dados antiga em um 
tempo inferior a 5 minutos. Para o retorno integral de uma base, aceita-se um 
tempo inferior a um dia.
2.1.1.3. Ambientes de homologação e desenvolvimento: possibilitar o acesso 
simultaneo a multiplas bases de diferentes pontos no tempo com permissao 
de leitura e escrita.
2.1.2. Não Funcionais
2.1.2.1. Agnostico: a solução deve suportar qualquer banco de dados que 
roda em ambiente linux, que armazena seus dados e arquivos de log em um 
sistema de arquivos.
2.1.2.2. Software Publico: utilizar ferramentas de software livre que nao 
exijam licenciamento e/ou estejam presentes no portal de software publico 
[4].
2.1.2.3. Economia: Prezamos pela redução do consumo de recursos, por 
este motivo a solução deve utilizar o mínimo possível de recursos de rede e 
armazenamento.
2.2. ZFS - A base de tudo
Portado do Solaris para o Linux em 2007, a sistema de arquivos o ZFS permite 
agrupar multiplos discos (virtuais ou nao) em datapools que por sua vez podem 
ser divididos em datasets com configurações personalizadas, que sao montados 
como diretórios. Ressalta-se as seguintes funcionalidade desta solução:
2.2.1. Snapshots: possibilita gravar o estado de um dataset em um dado 
momento no tempo. Permitindo o acesso ao arquivos de forma transparente, 
na forma de um diretório oculto com permissao de somente leitura. As 
alterações feitas (nos arquivos) apos o snapshot sao armazenadas em outros 
locais do disco, criando camadas de blocos alterados.
2.2.2. Clone: possibilita montar um snapshot em uma pasta com permissoes 
de escrita e leitura.página
239
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de Dados2.2.3. Send/Recv: possibilita enviar e receber snapshots entre servidores 
remotos. Caso ja exista uma snapshot antigo na maquina destino, ele envia 
apenas as novas camadas.
2.2.4. Compressao: suporta a mUltiplos algoritmos de compactação de dados 
de forma nativa, sem a necessidade de adaptação das aplicações.
2.2.5. Camadas de Cache: alem da memória RAM, suporta a utilização 
de discos de estado solido como cache de leitura e escrita, melhorando a 
performance das aplicações hospedadas.
Devemos ressaltar dois fatores preocupantes: (i) o excesso de snapshots 
(mais que 2) pode afetar a performance do sistema. Por este motivo, em ambientes 
de produto recomenda-se manter o mínimo de snapshots possíveis; (ii) o envio de 
snapshots e um procedimento simples mas que exige uma sincronicidade entre 
a origem e o destino. Como o send/recv trafega apenas as camada do Snapshot, 
e necessario que a origem e o destino compartilhem pelo menos uma camada 
em comum, como mostra a Figura 1. Em caso de assincronia, como na Figura 2, 
o envio diferencial se torna inviavel e um novo envio completo se faz necessario. 
Para evitar que snapshots sejam apagados sem o devido cuidado, o ZFS prove uma 
marcação de controle (flag “hold”) que pode ser gerenciado por qualquer usuario.
Assim, apesar dos cuidados, o ZFS resolve a questao dos backups frequentes 
sem a parada do servico ou queda de performance dos bancos.
Figura 1. Cenário sincronizado.
 Figura 2. Cenário sem sincronia.
2.3. Docker - Mantendo a infraestrutura homogênea
O Docker e uma solução de para-virtualização baseada em container criada 
2013 que possibilita descrever servicos complexos definidos por software, 
facilitando a documentação e versionamento da configuração ( infraestrutura 
+ aplicações). Ele nos permite equalizar os ambientes de desenvolvimento, 
homologação e produção, isolando suas diferencas em variaveis. Paralelamente, 
conseguimos executar multiplos ambientes (i.e. diferentes bancos de dados) e 
multiplos instancias (i.e. diferentes datas) simultaneamente na mesma maquina 
virtual, encapsulando em conjuntos isolados (“stacks”) de servicos e mapeando 
diferentes portas do servidor para ocontainer.página
240
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de Dados2.4. ActiveMQ - Tratando as requisições
O projeto ActiveMQ, encubado pela fundação Apache desde 2007, prove 
o servico de publicação de informações: gerenciamento de filas de mensagens, 
topicos e inscrições.
Como exemplifica a Figura 3, existem dois tipos de canais de comunicação: 
Filas (Queues) e Topicos (Topic). No primeiro, um ou mais produtores geram 
mensagens que serâo processadas por consumidores destintos, ou seja, as 
mensagens sao entregues, uma de cada vez, para uma fila de consumidores. No 
segundo, de forma contraria, as mensagens sao enviadas para cada consumidor, 
onde todos terâo ciencia de todas as mensagens.
Assim, esta ferramenta vem auxiliar no gerenciamento das instancia dos 
bancos de dados, fazendo a ponte entre as requisições de acesso aos backups e a 
instanciação dos mesmo.
Figura 3. Cenário sincronizado.
 Figura 4. Cenário sem sincronia.
2.5. A Proposta
Nosso ambiente preve a utilização de multiplos servidores de bancos de 
dados em maquinas virtuais distintas e um servidor de backup/restore centralizado, 
como mostra a Figura 4.
Nossa solução é dividida em duas partes: Backup/Retenção e Restauração/
Remoção. A primeira trata da execuçao, transmissao e expiraçao do snapshots. A 
segunda da recepçao dos pedidos de restore, a instanciaçao dos banços de dados 
e remoçao das instançias.
2.5.1. Backup & Retenção
O objetivo da fase de Backup e: (i) Fazer o Snapshot dos dados do banco, 
(ii) enviar os snapshots para o servidor centralizado e expirar os snapshots 
desnecessarios no servidor de produçao.página
241
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de DadosAssim, o processo backup e executado de forma distribuída, em cada servidor 
de produçao. Ao iniciar, o snapshot e criado e associado a uma classe (hourly, 
daily, weekly) definida na hora da execuçao (na linha de comando). Classificar e 
uma medida de usabilidade para facilitar a identificaçao da duraçao dos mesmos 
pelo seu nome, na listagem de snapshots, como mostra a Figura 5. Em seguida, faz-
se a marcaçao de “hold”para evitar a remoçao do mesmo.
datastore/mail@daily_Tue_20190305_2030 2,37G   - 11,2T -
datastore/mail@daily_Fri_20190308_2030 2,37G   - 11,2T -
datastore/mail@hourly_Mon_20190311_0700 1,60G   - 11,2T -
datastore/mail@daily_Mon_20190311_2030 518M   - 11,2T -
datastore/mail@hourly_Tue_20190312_0100 253M   - 11,2T -
datastore/mail@hourly_Tue_20190312_0700 466M   - 11,2T -
datastore/mail@hourly_Tue_20190312_1300 1,57G   - 11,2T -
Figura 5. Lista de Snapshots.
O segundo passo e o envio dos snapshots da produçao (origem) para o 
servidor de backup (destino) Caso haja outros processo de envio rodando, o envio 
e abortado. Identifica-se entao o ultimo snapshot da origem e do destino. Se ambos 
sao iguais, aborta-se pois nao ha o que enviar. Caso nao haja snapshots no destino 
o envio e abortado e recomenda-se fazer o primeiro envio manualmente. Simula-
se entao o envio de todos os snapshots entre (i) o ultimo snapshot do destino 
que esta no servidor local e (ii) o ultimo snapshot que esta no servidor local. Caso 
a simulaçao falhe, aborta-se o envio e o serviço de monitoraçao (Zabbix [5]) e 
notificado. Caso contrario o envio e iniciado e qualquer erro resulta no aborto do 
processo e notificaçao da monitoraçao.
Finalizado o envio, o ultimo snapshot do destino e trancado. Em seguida 
destranca-se: (i) todos os snapshots da origem, menos o ultimo e (ii) o snapshot 
do destino inicialmente identificado como sendo o ultimo. Inicia-se entao o 
procedimento de expiraçao local, onde todos os snapshots destrancados da 
origem sao removidos.
Paralelamente, no destino, o processo de expiraçao e rodado de acordo com 
sua classe e sua política de retençao, e.g. classes hourly rodam 1 vez por hora, 
apagando todos os snapshots destrancados mais antigos que 15 dias.
Dado a criticidade e a complexidade das regras de negocio, desenvolvemos 
um script em Bash para gerenciar o procedimento de criaçao, envio e marcaçao 
dos snapshots. Este pode ser encontrado no site do projeto [6].  
2.5.2. Restauração & Remoção das instâncias
O procedimento de criação e remoção das instancias e uma comunicação 
entre os interessados (clientes, sistemas de CI/CD, interfaces web etc) com servidor 
de backup, mediado pelo servidor de mensagens (ActiveMQ).página
242
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de DadosTermos
Neste ambiente temos as seguintes variaveis:
• Snapshot: Nome do snapshot.
• Dataset: Nome da partido do ZFS onde estao os snapshots.
• Id. do snapshot: String que identifica um snapshot (junção do dataset e 
snapshot).
• Imagens: Nome das imagens docker suportadas no Dataset, e.g. mysql, 
pgsql etc
• Porta do Sérvico: Numeros das portas que o servico foi instanciado.
• Id. da instancia: String que identifica uma instancia de BD (junção do 
dataset, snapshot e porta).
• Interessados: Nome dos usuarios interessados nesta instancia.
Ménsagéns
Para gerenciar este processo, desenvolvemos um protocolo com as seguintes 
mensagens:
• listarSnapshots: Mensagem enviada ao topico “snapshots”aos clientes 
com a lista de snapshots. Contem: identificador do snapshot, imagens e data 
do snapshot;
• listarInstancias: Mensagem enviada ao topico “Instancias”aos clientes com 
a lista de instancias ativas. Contem: identificador da instancia, interessados, 
data do snapshot e porta do servico;
• pedidoDeRestore: Mensagem enviada pelo cliente para a fila 
“restore”requisitando uma restauração. Contem: identificador do snapshot, 
imagem e interessado;
• expirarInstancia: Mensagem enviada pelo cliente para a fila 
“Expire”requisitando a extinção de uma instancia. Contem: identificador e 
interessado;
3. Resultados
A implementação da proposta foi desenvolvida em PHP , esta disponível como 
uma imagem Docker a ser instanciada. No site do projeto [6] pode ser encontrado 
um arquivo docker-compose que, ao ser executado, inicia (i) uma instancia do 
servidor de filas e (ii) uma instancia com um processo que interpreta o protocolo 
desenvolvido. Este processo (daemon em PHP) consome continuamente as 
mensagens das filas e frequentemente atualizando os topicos.
Sobre a implementação do protocolo, as mensagens da Seção 2.5.1 funcionam 
da seguinte maneira:página
243
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de Dados• listarSnapshots: A cada 5 minutos o processo:
1. Lista todos os snapshots do sistema operacional;
2. Busca as imagens suportadas nos metadata de cada dataset;
3. Monta a listagem como um JSON;
4. Limpa as mensagens no topico “snapshots”;
5. Envia a mensagem contendo a nova lista.
• listarInstancias: Apos pedidoDeRestore, expirarInstancia ou no maximo a 
cada 5 minutos:
1. Lista todos os containers ativos;
2. Busca a lista de interessados nos metadatas dos container;
3. Monta a listagem como um JSON;
4. Limpa as mensagens no tópico “instancias”;
5. Envia a mensagem contendo a nova lista.
• pedidoDeRestore: A cada mensagem pedindo um restore o prôçessô:
1. Caso exista uma instância ativa do mesmo snapshot, adiciona o interessado 
na lista de interessados, aciona o listárInstánciás e aborta o processo;
2. Identifica uma porta livre;
3. Clona o snapshot;
4. Inicia um container (baseado no nome da imagem docker passada) 
adicionando informações relevantes via variaível de ambiente (portas, path 
do clone etc);
5. Adiciona o interessado nos metadata do container e aciona o listarInstancias. 
Em caso de erro faz a sanitizaçãô do ambiente, apagando o clone, container 
etc.
• expirarInstancia: A cada mensagem pedindo a extinção de uma instancia:
1. Caso existam multiplos interessados na instancia, remove o interessado da 
lista de interessados, aciona o lista- rInstancias e aborta o processo;
2. Mata e remove o container indicado;
3. Destroi o Clone utilizado;
4. Aciona o listarInstancias.
4. Conclusão
Neste artigo apresentamos uma solução de auto-servico para gerenciamento 
de backups e restauração de bases de dados de forma rápida, segura e autonoma. 
A solução e voltada para banco de dados que rodam em ambiente linux, e que 
possibilitam salvar seus dados em um sistema de arquivo. Ela sana nossa demanda página
244
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 236-244, junho de 2019
Capítulo XXXVIII - SeBaHo Juntando o Util ao Necessário Serviço de Backup, Restore e Homologação 
de Base de Dadospor backups frequentes, gerenciamento de retenção de backups e possibilidade 
de criar instancias de homologação e desenvolvimento.
Infelizmente, devido a implementação de alguns bancos de dados, a 
velocidade da restauração ficou prejudicada, apresentando um tempo de 
aproximadamente 10 minutos de execução, valor acima do esperado.
Como trabalhos futuros planejamos (i) desenvolver plugins para as interfaces 
web PHPMyAdmin e PGAdmin, a fim de consolidar a solução como um auto-servico 
e (ii) integrar com o sistema de integração contínua, i.e. Gitlab-CI.
5. Notas de Reconhecimento
Agradecemos a todos os amigos e colegas que acreditaram nesta ideia, 
investindo seu tempo, suor e confianca para o sucesso do mesmo.
Referências
[1] Zfs on linux. URL https://zfsonlinux.org.
[2] Docker. URL https://www.docker.com.
[3] Activemq. URL http://activemq.apache.org.
[4] Software publico brasileiro. URL https://softwarepublico.gov.br .
[5] Zabbix. URL https://www.zabbix.com/.
[6] Projeto sebaho. URL https://gitlab.setic.ufsc.br/dtr/sebaho .página
245
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 245-250, junho de 2019
Capítulo XXXIX - SINAPSE: Sistema Integrado de Acompanhamento de Projetos e ServiçosSINAPSE: Sistema Integrado de Acompanhamento de 
Projetos e Serviços
Danniel Rocha, Euclydes Melo, Luiz Fernando, Marcos Raniere, Matheus Campanhã, 
Taison Almeida 
Superintendência de Tecnologia da Informação  – Universidade Federal do Piauí (UFPI) – Teresina 
– PI – Brasil
{danniel, euclydesmelo, fernando.mendes, marcosraniere, matheuscampanha, 
taison.almeida}@ufpi.edu.br
Resumo
Devido à crescente necessidade de serviços de Tecnologia da Informação para o pleno funcionamento de Instituições 
Federais de Ensino Superior, a organização e o acompanhamento das demandas desta natureza torna-se a cada dia 
mais importante. Este artigo apresenta as soluções adotadas pela STI da UFPI, evidenciando seus pontos positivos 
e negativos.
Palavras-chave: Acompanhamento de Projetos e Serviços, Sistema Integrado.
1. Introdução
A Universidade Federal do Piauí (UFPI) possui estrutura multicampi e 
hierarquizada, desta forma a Tecnologia da Informação (TI) realiza um papel 
crucial para o funcionamento da instituição. Os serviços relacionados à TI 
são estrategicamente desenvolvidos pela Superintendência de Tecnologia da 
Informação (STI). Segundo o regimento interno da STI, em seu artigo 17, inciso IX, 
uma de suas competências é coordenar as atividades relacionadas ao atendimento 
ao usuário, no que se refere aos serviços providos de TI [STI, 2019]. Essa atribuição 
abrange desde receber as solicitações dos usuários da instituição, cadastrar 
dúvidas, reclamações e sugestões dos usuários relativas aos serviços oferecidos 
pela STI.
Dentre as divisões da STI, há uma específica que atua diretamente sobre 
estas demandas referentes a atendimentos, semelhante a um Service Desk, ou seja, 
“único ponto de contato entre os prestadores de serviços e usuários, no dia-a-dia. 
É também um ponto focal para a comunicação de incidentes e de fazer pedidos de 
serviços. ” [OGC, 2001]. Durante muitos anos o serviço de atendimento ao usuário 
era presencial, porém devido aos diversos fatores como o crescente aumento da 
demanda pelos serviços de TI, às aposentadorias de servidores lotados no setor 
de atendimento e à estrutura multicampi, foi necessário repensar a interface de 
comunicação com os usuários.
Devido a este contexto, algumas soluções informatizadas foram 
implementadas e testadas na STI da UFPI. Este artigo apresenta as soluções página
246
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 245-250, junho de 2019
Capítulo XXXIX - SINAPSE: Sistema Integrado de Acompanhamento de Projetos e Serviçosadotadas, evidenciando seus pontos positivos e negativos, os resultados obtidos 
com os mesmos, além das ideias e aprimoramentos a serem validados futuramente.
2. Métodos
2.1. Sistema de Chamados - SIG  
A primeira tentativa de informatizar o atendimento aos usuários ocorreu 
por meio da ferramenta “Abrir Chamado” disponível nos Sistemas Integrados de 
Gestão – SIG (SIGAA, SIGRH e SIPAC). O sistema de chamados foi adotado em Agosto 
de 2012.  Com este sistema, o usuário não precisaria mais se dirigir à Divisão de 
Atendimento para sanar dúvidas, relatar problemas ou pedir melhorias no sistema. 
Apesar dos benefícios da ferramenta, ela não é integrável com o sistema de gestão 
de projetos Redmine (disponível em https://redminedes.ufpi.br/), que é utilizado 
pela STI para planejamento e acompanhamento das demandas.
De forma resumida, o processo de trabalho consistia em receber o pedido 
do usuário por meio do sistema de chamados, criar uma tarefa  com a  cópia do 
texto no Redmine e então a tarefa era escalada para um responsável por resolvê-
la, o qual ao final era necessário “copiar” a solução na ferramenta do sistema SIG. 
Além disso, a ferramenta não permitia a interação do solicitante com a equipe da 
STI, pois após aberto um chamado, o solicitante era impossibilitado de responder 
quando fosse questionado e não conseguia dar feedback informando se foi 
atendido a contento. Caso algum chamado fosse mal detalhado ou precisasse de 
feedback, o solicitante era orientado a abrir um novo chamado, acarretando em 
overhead e desgaste com os clientes desta STI.
Devido às fragilidades do Sistema de Chamados dos sistemas SIG, muitas 
demandas voltaram a surgir presencialmente, por telefone ou memorandos 
eletrônicos. A proposta de possuir canal único e centralizado para atendimentos 
não se concretizou, além de causar retrabalhos. Diante deste cenário, foi elaborada 
uma solução que contemplasse a otimização do processo e centralizasse de fato as 
demandas. Para isso, durante o ano de 2018 foi desenvolvido um sistema integrado 
ao Redmine, denominado SINAPSE – Sistema Integrado de Acompanhamento a 
Projetos e Serviços.
2.2. SINAPSE
O  Sistema Integrado de Acompanhamento a Projetos e Serviços (SINAPSE, 
disponível em https://sinapse.ufpi.br/) é uma ferramenta que foi desenvolvida 
pela equipe da STI como uma proposta de sistema integrado de acompanhamento 
a projetos e serviços, simplificando e centralizando a comunicação da STI com os 
demais setores da instituição. O SINAPSE se propõe a registrar todas as demandas 
referentes à TI, como solicitações de serviços, sustentações e customizações dos 
sistemas, dentre outros.página
247
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 245-250, junho de 2019
Capítulo XXXIX - SINAPSE: Sistema Integrado de Acompanhamento de Projetos e ServiçosComplementarmente, por ser a interface de comunicação da STI com o 
restante da instituição, ele também contempla o catálogo de serviços de TI, 
possui suporte a inclusão de manuais, frequently asked questions (FAQs) e apoia 
o acompanhamento do andamento das demandas. Antes de cadastrar uma 
nova requisição de serviço por meio desta ferramenta, é necessário acessar os 
manuais e perguntas frequentes. Estas requisições de serviço são nomeadas de 
“chamado” , definido na ITIL como uma requisição formal de um usuário para algo 
a ser fornecido [ITIL, 2019]. O SINAPSE é integrado ao Redmine, onde cada nova 
requisição de serviço abre um ticket no Redmine, evitando os retrabalhos que 
ocorriam na ferramenta “Abrir Chamado” .
O SINAPSE conta com uma interface simples e amigável, na qual é possível 
o demandante registar uma nova requisição de serviço disponível no catálogo 
oferecido pela STI. Para auxiliar na identificação do serviço, a ferramenta possui 
uma estrutura hierárquica de serviços e um campo para busca. A Figura 1 apresenta 
a tela inicial do SINAPSE, contendo o catálogo de serviços da STI.
Figura 1 - Página Inicial do SINAPSE contendo o catálogo de serviços
Por meio da disponibilização de um catálogo de serviços, o SINAPSE  contribui 
para uma melhor gestão demandas por parte da STI. Para cada tipo de serviço, o 
sistema disponibiliza manuais e outros tipos de ajudas com o objetivo de sanar  
dúvidas e evitar a abertura de chamados desnecessários. A Figura 2 apresenta um 
exemplo de serviço contendo manuais, perguntas frequentes, e caso o problema 
ainda persista, o usuário pode abrir um novo chamado. página
248
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 245-250, junho de 2019
Capítulo XXXIX - SINAPSE: Sistema Integrado de Acompanhamento de Projetos e Serviços
Figura 2 - Interface do SINAPSE contendo manuais e FAQs, prévio à abertura de um 
chamado
O SINAPSE  disponibiliza a lista de chamados abertos e finalizados, 
possibilitando acompanhar o andamento de cada um, além de permitir interagir 
com o técnico administrativo responsável pela solução do chamado. Depois que 
o chamado é analisado e resolvido, o solicitante pode visualizar a solução por 
meio de uma tela de detalhamentos do chamado. Para que o cliente não precise 
ficar constantemente conferindo seus chamados, são disparadas notificações por 
e-mail a cada nova interação entre o solicitante e o responsável técnico. O mesmo 
ocorre quando o chamado é finalizado.
Além das funcionalidades da ferramenta em si, o SINAPSE também possui 
integração com o Sistema Integrado de Patrimônio, Administração e Contratos 
(SIPAC). Devido às solicitações provenientes de memorandos eletrônicos, foi 
inserida uma funcionalidade que converte o memorando eletrônico em um novo 
chamado e anexa o documento na solicitação do SINAPSE. A Figura 3 apresenta 
a tela de listagem de memorandos pendentes de recebimento, contendo o botão 
para encaminhar a solicitação ao SINAPSE.
 
Figura 3 - Listagem de memorandos eletrônicos no SIPAC com integração ao SINAPSEpágina
249
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 245-250, junho de 2019
Capítulo XXXIX - SINAPSE: Sistema Integrado de Acompanhamento de Projetos e Serviços3. Resultados
Entre o período de Agosto de 2012 a Janeiro de 2018, foram contabilizados 
11.125 requisições de serviços abertos via sistema de chamados e 9.313 tickets 
abertos no Redmine. No intervalo de 2012 a 2015, os chamados eram monitorados 
apenas via sistema de chamados, porém a partir de Março de 2015 os chamados 
abertos passaram a gerar um ticket correspondente no Redmine, abertos 
manualmente, buscando auxiliar a gestão das solicitações. Entretanto, devido 
às limitações do sistema de chamados, diversas demandas começaram a surgir 
presencialmente, por telefone e por memorandos, ocasionando mais aberturas de 
tickets no Redmine do que pelo sistema de chamados, vide Gráfico 1.
Gráfico 1 - Quantidade de solicitações no Redmine e no sistema de Chamados.
Em Março de 2018 o sistema de chamados foi descontinuado e o Redmine 
passou a ser alimentado diretamente pelo SINAPSE, visto que a cada nova 
requisição de serviço é aberto um novo ticket. Entre Março e Dezembro de 2018 
foram abertos 2.016 chamados. No Redmine a média de tickets abertos por mês caiu 
de 251,2 para 201,6 após a adoção do SINAPSE. Infelizmente não há como garantir, 
mas provavelmente esta queda ocorreu devido à possibilidade de interação entre 
solicitante e responsável técnico, sem a necessidade de criar novos chamados 
para complementar informações de chamados anteriores.
Ressalta-se que a principal dificuldade encontrada atualmente com o SINAPSE 
é a baixa quantidade de manuais, FAQs e outros materiais complementares. Isto 
ocorre devido ao vasto catálogo de serviços disponibilizado pela STI e à quantidade 
reduzida de técnicos administrativos no setor. Espera-se que com mais manuais e 
FAQs disponíveis, a média de chamados diminua cada vez mais.página
250
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 245-250, junho de 2019
Capítulo XXXIX - SINAPSE: Sistema Integrado de Acompanhamento de Projetos e Serviços4. Conclusão
O desenvolvimento de iniciativas, como o SINAPSE, melhoram a comunicação 
e a interação entre a equipe de suporte e os clientes, apoiando a entrega de 
soluções e contribuindo assim, para um aumento na percepção de valor da TI 
pelos clientes. Estes baseiam sua satisfação em função das expectativas e do 
desempenho percebido: o consumidor ficará satisfeito ou altamente satisfeito, 
se o desempenho atender ou exceder às expectativas, ficando insatisfeito se ficar 
abaixo de suas expectativas [Kotler, 1998].
Os resultados iniciais apresentam que houve redução de tickets abertos após 
a implantação do SINAPSE, com indícios de redução de overhead de trabalho para 
a equipe da STI, além dos ganhos no gerenciamento, controle e acompanhamento 
dos chamados, que são evidentes. Outro ponto destacável é o recebimento de 
feedbacks positivos recebidos por parte dos clientes da STI que utilizaram o 
sistema de chamados dos sistemas SIG e o SINAPSE. Futuramente pretende-se 
realizar uma  pesquisa de satisfação para evidenciar tais ganhos.
Referências
ITIL - Glossário. Disponível em: <https://www.pmgacademy.com/pt/glossario-itil/>. 
Acesso em: 24 fev, 2019.
Kotler, Philip. “Administração de marketing: análise, planejamento, implementação e 
controle. ” São Paulo: Atlas, 1998.
OGC, Office of Government Commerce. Service Delivery. Londres – Inglaterra: The 
Stationary Office, 2001.
STI - Regimento interno. Disponível em: < http://ufpi.br/regimento-do-nti>. Acesso 
em: 24 fev. 2019.página
251
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 251-256, junho de 2019
Capítulo XL - SisPAC – Sistema para Levantamento e Consolidação das Necessidades do PACSisPAC – Sistema para Levantamento e 
Consolidação das Necessidades do PAC 
Tarcila G. da Silva, Taiana B. Pereira, Julliany S. Brandão, Daniel F. Oliveira, Enoch C. P. 
L. da Silva¹
¹Centro Federal de Educação Tecnológica Celso Suckow da Fonseca (Cefet/RJ) Maracanã – RJ – 
Brasil
{tarcila.silva, taiana.pereira, julliany.brandao, daniel.oliveira, enoch.
silva}@cefet-rj.br
Resumo
Este artigo apresenta um sistema desenvolvido com o objetivo de facilitar o levantamento das necessidades e o 
planejamento das contratações de bens, serviços e soluções de tecnologia da informação e comunicações do Cefet/RJ 
para consolidação no Plano Anual de Contratação. O sistema buscou otimizar o processo de planejamento e gestão 
das informações, antes realizados manualmente. Embora ainda não tenha sido possível realizar análises estatísticas 
aprofundadas, notou-se que a solução teve uma excelente receptividade por parte dos usuários, que consideram a 
ferramenta eficaz e simples de ser utilizada, o que mostra que além de melhorar a gestão da informação, o quesito 
usabilidade é mais um benefício do sistema.
1. Introdução
A Instrução Normativa Número 1, de 10 de janeiro de 2019 estabelece 
que cada Unidade de Administração de Serviços Gerais (UASG) deve elaborar 
anualmente o respectivo Plano Anual de Contratações (PAC), contendo todos os 
itens que pretende contratar no exercício subsequente.  Para elaboração do PAC, 
a Secretaria de Gestão do Ministério da Economia disponibilizou o Sistema de 
Planejamento e Gerenciamento de Contratações1 (PGC), que constitui a ferramenta 
informatizada, integrante da plataforma do Sistema Integrado de Administração 
de Serviços Gerais (SIASG) [Brasil 2019].
Apesar do PGC prover a plataforma para elaboração do PAC, fazer o 
levantamento de todas as necessidades de bens, serviços, obras e soluções de 
tecnologia da informação e comunicações de uma instituição de grande porte e 
multicampi como o Cefet/RJ é um desafio enorme. Anteriormente, isso era feito 
por meio de planilhas complexas, existia muita confusão no preenchimento delas, 
sendo a consolidação muito dispendiosa e demorada.  Nesse contexto, o SisPAC 
foi desenvolvido com o objetivo de facilitar a identificação das necessidades de 
contratações e a consolidação dessas informações para alimentação do PGC.
Este artigo está estruturado da seguinte forma: na seção 2, o SisPAC 
é apresentado, sendo descritas sua metodologia de desenvolvimento, 
funcionalidades e tecnologias utilizadas; na seção 3, são expostos resultados 
preliminares; na seção 4, é feita a conclusão e são propostos trabalhos futuros; 
por fim, são listadas as referências.
1 https://pgc.planejamento.gov.brpágina
252
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 251-256, junho de 2019
Capítulo XL - SisPAC – Sistema para Levantamento e Consolidação das Necessidades do PAC2. Sistema para Levantamento e Consolidação das 
Necessidades do PAC
O SisPAC é um sistema Web  desenvolvido no Cefet/RJ para auxiliar no 
levantamento das necessidades de aquisições para instituição e apoiar o 
planejamento da contração de bens, serviços e soluções de tecnologia da 
informação e comunicações, bem como, facilitar a alimentação do PGC. O sistema 
permite que pessoas designadas pelos centros de custo informem os itens a serem 
contratados no exercício corrente e no posterior. Por sua vez, pessoas responsáveis 
pelo setor de compras podem gerenciar os itens de contratação e exportar os 
dados das solicitações de compra feitas pelos centros de custo. Antes do SisPAC 
ser posto no ambiente de produção, foi feita uma carga inicial no banco de dados 
com os itens e usuários listados pelo setor de compras, dado o grande volume de 
informação. 
A metodologia escolhida para implementação do sistema foi o 
desenvolvimento iterativo e incremental, pois, segundo Heldman (2018), é a melhor 
escolha para projetos com alguma dessas características: grandes, complexos, 
cujos objetivos e escopo mudam ou é necessário entregar de forma incremental. 
Nesse contexto, o desenvolvimento iterativo é a abordagem para construir software, 
na qual o ciclo de vida é composto por várias iterações sequenciais, onde cada 
iteração é um miniprojeto autossuficiente. Enquanto que entrega incremental é 
a prática de entregar repetidamente um sistema em produção em uma série de 
recursos em expansão [Larman, 2004].  
2.1. Funcionalidades
As principais funcionalidades do sistema foram descritas em diagramas de 
casos de uso da Unified Modeling Language2 (UML). O diagrama de casos de uso visa 
permitir a compreensão do comportamento externo do sistema por intermédio da 
perspectiva do usuário [Guedes 2018].
A Figura 1 apresenta o diagrama que descreve a gestão da solicitação de 
compras e o gerenciamento de itens; a Figura 2 mostra a interação com o relatório 
de solicitações de compra; e a Figura 3 expõe a gestão de usuários e de seus grupos. 
O sistema possui quatro grupos de usuários, representados pelos atores dos casos 
de uso:
• Solicitante: faz solicitações de compra (Figura 1) e consulta relatório com 
suas solicitações (Figura 2);
• Responsável pelo Centro de Custo: engloba as mesmas permissões que o 
grupo Solicitante, visualiza o relatório (Figura 2) e gerencia usuários do seu 
centro de custo (Figura 3);
• Gestor de Compras: possui as mesmas atribuições que o grupo de usuários 
anterior, acrescido que ele vê o relatório com todas as solicitações de compra 
(Figura 2), gerencia todos os usuários (Figura 3) e faz a gestão de itens de 
compra (Figura 1);
2 http://www.uml.orgpágina
253
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 251-256, junho de 2019
Capítulo XL - SisPAC – Sistema para Levantamento e Consolidação das Necessidades do PAC• Administrador do Sistema: faz o mesmo que o grupo Gestor de Compras e 
gerencia os grupos de usuários (Figura 3). 
Figura 1. Caso de Uso da Gestão de Solicitações de Compras e de Itens
Figura 2. Caso de Uso do Relatório de Solicitações de Comprapágina
254
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 251-256, junho de 2019
Capítulo XL - SisPAC – Sistema para Levantamento e Consolidação das Necessidades do PAC
Figura 3. Caso de Uso da Gestão de Usuários e Grupos
2.2. Stack de Desenvolvimento
Ao definir a stack de desenvolvimento optou-se por tecnologias open 
source, pois, de acordo com Rao (2015), têm as seguintes vantagens: o não 
pagamento de royalties, permitir melhores oportunidades de arquiteturas, possuir 
ferramentas para qualquer sistema, facilidade de gerenciamento e acessível aos 
desenvolvedores. 
O sistema foi desenvolvido em Laravel3, um framework que usa a linguagem 
Hypertext Preprocessor4 (PHP) e o padrão de design de projeto Model View Controller  
(MVC), que separa a apresentação, lógica e regras de negócio, oferecendo vantagens 
no aspecto de organização do código [Gabardo 2017]. O Laravel foi escolhido, 
pois, segundo Stauffer (2016), ajuda a construir soluções sem desperdício de 
código, usando padrões modernos de codificação, com uma comunidade ativa 
e um ecosistema de ferramentas poderoso, além disso, de acordo com Gabardo 
(2017), é um dos frameworks PHP mais populares atualmente e possui uma boa 
documentação.
No front-end foram usados Hyper Text Markup Language (HTML), Cascading 
Style Sheets (CSS), JavaScript e a biblioteca Bootstrap5. O sistema usa uma 
Application Programming Interface (API) desenvolvida em Node.js6 para trazer os 
usuários do Sistema Integrado de Ensino (SIE) – módulo de Recursos Humanos, 
que estão armazenados em um banco de dados Db27.   O banco de dados 
3 https://laravel.com
4 http://www.php.net
5 https://getbootstrap.com
6 https://nodejs.org
7 https://www.ibm.com/analytics/us/en/db2página
255
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 251-256, junho de 2019
Capítulo XL - SisPAC – Sistema para Levantamento e Consolidação das Necessidades do PACutilizado por esta aplicação é o MySQL8 e ela é hospedada em um servidor Nginx9. A 
arquitetura das tecnologias usadas no desenvolvimento do sistema é apresentada 
na Figura 4. 
Figura 4. Arquitetura de Tecnologias
3. Resultados Preliminares
No período inicial de desenvolvimento, o SisPAC foi disponibilizado 
em ambiente de homologação, sendo entregue de forma incremental e suas 
funcionalidades ajustadas iterativamente para atender melhor às necessidades 
dos usuários. 
O sistema foi posto recentemente em produção e por esse motivo ainda não 
foi possível coletar dados suficientes para realizar análises estatísticas adequadas, 
no entanto, espera-se maior celeridade e assertividade na etapa de identificação 
das demandas e planejamento das aquisições para instituição, tendo em vista que 
a solução anterior era muito ineficiente. Todavia, o sistema teve uma excelente 
receptividade por parte dos usuários do setor de compras, que são os principais 
beneficiários da solução, que deram um feedback positivo tanto em relação às 
funcionalidades, quanto à usabilidade.
4. Conclusão e Trabalhos Futuros
Apesar da recente implantação do sistema, ele representa uma evolução 
na forma como as necessidades de contratações de bens, serviços e soluções de 
tecnologia da informação e comunicações são identificadas em comparação com 
o modo que era feito anteriormente, tendo grande potencial de agilizar essa etapa 
do processo de compra. Dadas características da solução proposta neste artigo, 
acredita-se que ela pode ser usada por outras instituições cujos centros de custo 
são descentralizados.
8 https://www.mysql.com
9 https://www.nginx.compágina
256
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 251-256, junho de 2019
Capítulo XL - SisPAC – Sistema para Levantamento e Consolidação das Necessidades do PACComo trabalhos futuros, a ferramenta continuará a ser evoluída de forma 
iterativa e incremental, sendo implementados requisitos funcionais e não 
funcionais, como melhorias na usabilidade e funcionalidades para etapa de 
contratação dos itens.
Referências
Brasil (2019) “Instrução Normativa Nº 1, de 10 de janeiro de 2019” , https://www.
comprasgovernamentais.gov.br/index.php/legislacao/instrucoes-normativas/1068-
in-1-de-2019, fevereiro.
Gabardo, Ademir C. (2017) Laravel para Ninjas, Novatec, 1ª edição.
Guedes, Gilleanes T . A. (2018) UML 2: Uma Abordagem Prática, Novatec, 3ª edição.
Heldman, Kim (2018) PMP: Project Management Professional Exam Study Guide, John 
Wiley & Son, 9th edition. 
Larman, Craig (2004) Agile and Iterative Development: a Manager’s Guide, Addion-
Wesley, 1th edition.
Rao, M. Nagabhushana (2015) Fundamentals of Open Source Software, PHI Learning, 
1th edition.
Stauffer, Matt (2016) Laravel: Up and Running a Framework for Building Modern PHP 
Apps, O’Reilly, 1th edition.página
257
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 257-262, junho de 2019
Capítulo XLI - Sistema Eletrônico de Informações (Sei!): um estudo sobre a melhoria dos serviços 
públicos com apoio da inovação em processosSistema Eletrônico de Informações (Sei!): um 
estudo sobre a melhoria dos serviços públicos 
com apoio da inovação em processos
Ana Maria Marques de Paiva¹, Jeison Gomes dos Santos¹, Elisandra Marisa Zambra², 
Paulo Augusto Ramalho de Souza², Renato Neder²
¹Secretaria de Tecnologia da Informação – Universidade Federal de Mato Grosso (UFMT) 
78060-900 – Cuiabá, MT – Brasil
²Departamento de Administração e Ciências Contábeis – Universidade Federal de Mato Grosso 
(UFMT) 
78060-900 – Cuiabá, MT – Brasil
{anamarquespaiva,jeison}@ufmt.br; {elisandrazambra, paramalho}@gmail.com; 
renatoneder@hotmail.com
Resumo
O estudo apresenta os primeiros resultados sobre a avaliação do uso do Sei!, enquanto inovação na gestão de 
processos da UFMT, e uma reflexão acerca dos conceitos de inovação, relacionando-os com tecnologia, Sistemas de 
Informação(SI’s) e gestão de processos na administração pública, pela percepção de servidores técnicos e docentes 
lotados na Faculdade de Administração e Ciências Contábeis (FACC) do Campus Cuiabá, como forma de melhoria 
contínua.
Palavras-chave: Inovação. Gestão de Processos. Sistema de Informação. Sei!
1. Introdução
As demandas da sociedade por melhores resultados na aplicação de recursos 
pela administração pública, bem como pela transparência e lisura na execução de 
seus processos, têm gerado novos desafios, relacionados à tecnologia e inovação, 
para os gestores de instituições públicas, incluindo as universidades.
Levy e Drago (2005) afirmam que “organização de qualidade é aquela que faz 
aquilo a que se destina, bem feito e em tempo oportuno” . Destaca-se que muitos dos 
procedimentos no sistema administrativo podem ser realizados mais eficazmente 
quando da prática da inovação, uma vez que esta é a criação ou melhoria de algo 
com retorno econômico, financeiro ou social, como defendem Grizendi (2016) e 
Bessant e Tidd (2009).
Neste sentido, Lapolli (2003) sugere que o investimento em Sistemas de 
Informações (SI’s) pode auxiliar no processo de inovação tecnológica de modo a 
reduzir falhas e oferecer suporte à tomada de decisão, às atividades e processos, 
podendo, estes últimos, quando geridos devidamente, agregar valor econômico 
ou social, assim como promover a transparência na organização [BALDAN, VALLE 
e ROZENFELD 2014], tornando-se aliados dos gestores públicos, especialmente 
por facilitar o acesso à informação, atendendo as demandas institucionais e da 
sociedade, com vistas à eficiência, eficácia, efetividade e economicidade. página
258
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 257-262, junho de 2019
Capítulo XLI - Sistema Eletrônico de Informações (Sei!): um estudo sobre a melhoria dos serviços 
públicos com apoio da inovação em processosIsto posto, verificou-se que em 2015 foi editado o Decreto nº 8.539 que dispõe 
sobre a utilização do meio eletrônico para realização do processo administrativo 
[BRASIL 2015]. Nesse sentido, a UFMT adotou o Sei! visando atender as necessidades 
da comunidade acadêmica em relação a criação, acompanhamento e trâmite dos 
processos, de forma mais ágil, transparente e, ainda cumprindo o decreto [ALVES 
et. al. 2018]. Diante disso, julga-se como essencial o desenvolvimento de estudos 
que busquem avaliar o uso do Sei! como forma de melhoria contínua do serviço.
Conforme levantado por Pohlmann et. al (2005), os principais obstáculos 
à tecnologia e inovação se encontram na falta de treinamento e disseminação 
de conhecimento. Outra consideração é quanto à circulação da informação 
na universidade, no que se refere às diretrizes e rotinas dos documentos – da 
geração, busca e arquivamento. O desafio é implementar políticas e diretrizes 
de gestão de documentos capazes de apoiar, de forma transparente, a realização 
das rotinas administrativas. Quando se trata de SI’s, sabe-se que o fator humano 
é imprescindível para seu desenvolvimento e operação, sob pena de torná-los 
inúteis [LAUDON e LAUDON 2010].
Assim, este estudo apresenta os primeiros resultados sobre a avaliação do 
uso do Sei!, enquanto inovação na gestão de processos da UFMT , e uma reflexão 
acerca dos conceitos de inovação, relacionando-os com tecnologia, SI’s, e gestão 
de processos na administração pública, considerando a percepção de servidores 
técnicos e docentes lotados na FACC do Campus Cuiabá.   
2. Métodos
Trata-se de um estudo piloto com abordagem qualitativa [VIEIRA 2014], 
do tipo exploratório [VERGARA 2004]. Ao se considerar a polissemia do conceito 
de inovação e, para maior clareza de sua importância e como é implementada 
na UFMT , optou-se pelo desenvolvimento de entrevistas em profundidade com 
técnicos e docentes lotados na FACC do campus Cuiabá da UFMT , com apoio de 
roteiro semiestruturado, de modo a explorar a espontaneidade nas respostas e 
identificar elementos para futuras pesquisas. 
Para Minayo (2004), na pesquisa qualitativa a escolha dos sujeitos não 
é numérica, e acrescenta que estes sejam um grupo que detenha os atributos 
a que se pretende investigar, observando suas experiências e expressões. 
Desta forma, para a seleção dos entrevistados, utilizou-se a técnica chamada 
Snowball, que constitui-se de um método de amostragem por conveniência, um 
mecanismo em que o entrevistado indica o próximo e assim por diante, de modo 
que se identifiquem todos os indivíduos possíveis ou até passar-se a ter certa 
repetitividade ou esgotamento de informações [Heckathorn 2011]. 
Após a coleta e transcrição dos conteúdos das falas dos entrevistados, 
realizou-se a técnica de análise de conteúdo [BARDIN 1979] e utilizou-se o software 
Nvivo como apoio à análise qualitativa de dados, além da técnica de triangulação 
de dados [VERGARA 2004].página
259
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 257-262, junho de 2019
Capítulo XLI - Sistema Eletrônico de Informações (Sei!): um estudo sobre a melhoria dos serviços 
públicos com apoio da inovação em processosSalienta-se que a pesquisa de campo iniciou pela FACC, por ser uma das 
unidades acadêmicas com maior número de servidores e diversidade de perfis de 
usuários.
3. Resultados 
A aplicação de SI’s viabilizam a inovação na gestão e controle de processos, 
e isto ocorreu especificamente nos serviços de protocolo da UFMT , a partir da 
implantação do Sei!. Este sistema foi implantado com propósito de reduzir o tempo 
de realização das atividades administrativas e organizar os fluxos de trabalho 
[AMARAL e UCHOA 2014].
Assim, a partir dos recursos tecnológicos e práticas inovadoras no processo de 
trabalho, este estudo entende a importância da percepção dos técnicos e docentes 
da UFMT diante da implantação do Sei!, uma vez que estes são os principais 
usuários da plataforma. Deste modo, foi possível a extração dos elementos que 
mais se destacaram nas respostas.
Figura 1.  Nuvem de palavras dos técnicos e docentes
Observou-se a congruência da percepção dos entrevistados (Figura 1), que 
revela a percepção do Sei! enquanto inovação de processo na instituição. Outra 
questão que emerge do estudo é a importância das pessoas na disseminação da 
cultura de inovação, e na UFMT esta cultura é mais conceitual e não tão praticada. 
Evidencia também a necessidade de suporte técnico aos servidores com maior 
tempo em exercício de atividade laboral, que tendem a apresentar maior 
dificuldade de adaptação.
Identificou-se que, no que tange à implantação do Sei!, que os aspectos 
técnicos são vistos positivamente, já que melhora as condições de trabalho, 
a qualidade dos serviços entregues e a transparência. Quanto aos aspectos 
associados aos recursos humanos, estes são tidos como insatisfatórios, uma vez 
que apontaram: não ter havido um preparo adequado dos usuários dos serviços, o página
260
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 257-262, junho de 2019
Capítulo XLI - Sistema Eletrônico de Informações (Sei!): um estudo sobre a melhoria dos serviços 
públicos com apoio da inovação em processoscurto período de tempo para adaptação à solução e, a dificuldade dos servidores 
em deixarem seu status quo. 
Na perspectiva dos entrevistados, a implantação do Sei! apresentou aspectos 
satisfatórios (administrativos, de inovação e tecnologia, econômico financeiros, 
ambientais); e insatisfatórios (pedagógicos e socioculturais). Foram observados 
os diversos atributos e princípios, tais como: “redução do tempo de realização 
de atividades administrativas e a organização dos fluxos de trabalho” [AMARAL 
e UCHOA 2014], modelo inovador que “não se baseia na reprodução de práticas 
impostas pelo uso do papel” [AMARAL e UCHOA 2014].
Parte das funcionalidades do Sei! não está disponível aos “usuários 
externos” , como os discentes, por exemplo, e isso é visto como negativo. Outro 
questionamento foi quanto à tempestividade da implantação da solução, pois o 
tempo entre a divulgação e a operacionalização do Sei! não permitiu a maturação 
necessária para a plena operacionalização do SI, acrescentando-se ainda: os 
métodos utilizados na capacitação foram considerados “anti-pedagógicos”; a falta 
de domínio do conteúdo tratado pelos mediadores; a resistência à mudança do 
status quo por parte dos servidores técnicos e docentes.
Nesse contexto, a partir da percepção dos entrevistados, ao se relacionar 
os conceitos de inovação e gestão de processos ao Sei!, o estudo evidencia as 
dimensões: de inovação e tecnologia; econômico financeira; administrativas; 
socioculturais; ambientais e pedagógicas, apresentadas na Figura 2.
Figura 2.  Dimensões extraídas das entrevistas
Observou-se, também, uma maior dificuldade de inovar no setor público, seja 
por barreiras burocráticas, culturais ou pelas características dos serviços [PEREIRA 
et. al 2016], revelada pela fala de um dos entrevistados, de que a UFMT “[...]é um 
tipo de organização que está atrasada ainda, por parte da inovação[...]” e continua, 
“[...]pela própria característica de ser setor público, isso amarra um pouco[...]”.página
261
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 257-262, junho de 2019
Capítulo XLI - Sistema Eletrônico de Informações (Sei!): um estudo sobre a melhoria dos serviços 
públicos com apoio da inovação em processosEntretanto, contrapondo às dificuldades, estão os relatos de benefícios 
alcançados pelo novo modelo de protocolo na UFMT , tais como: agilidade; 
transparência; economia de papel e energia; a facilidade na comunicação oficial; 
tempo de resposta; alcance dos envolvidos; credibilidade; mobilidade; foco nas 
atividades estratégicas da unidade; entre outras. Em resumo, o Sei! é visto como 
como uma inovação de processos, uma vez que reduz as falhas dos serviços de 
protocolo, além de realizá-lo de maneira mais eficaz, o que vai ao encontro do 
entendimento de Lapolli (2003). 
4. Conclusão
O presente estudo buscou analisar diferentes dimensões decorrentes da 
utilização do Sei! enquanto inovação na gestão de processos na UFMT . Desta 
forma, considera-se que os objetivos foram atendidos ao se refletir acerca dos 
conceitos de inovação, tecnologia, SI’s, e gestão de processos na gestão pública, 
relacionando-os à percepção dos servidores técnicos e docentes, assim como 
a exploração das dimensões de inovação e tecnologia; econômico financeira; 
administrativa; sociocultural; ambientais e pedagógicas. 
Nota-se que a inovação ainda é um campo a ser explorado no âmbito da 
administração pública. Percebe-se ainda a necessidade de trazer à praxis seus 
conceitos. Desta forma, acredita-se que o estudo tenha contribuído para revisão 
na implementação de futuras inovações, de modo a serem consideradas as 
dimensões apesentadas, em que sejam construídas metodologias pedagógicas 
que resultem o alcance da disrupção de maneira mais sensível às necessidades 
das pessoas, enquanto recursos humanos.
Conclui-se ainda, que mesmo com uma estrutura mais rígida, o serviço 
público precisa caminhar rumo a Transformação Digital. Logo, a inovação aparece 
como forte aliada. Para tanto, ressalta-se a necessidade de capacitação de recursos 
humanos e promoção da cultura de inovação.
Por fim, para evolução deste estudo, pretende-se ampliar a amostra no 
processo de avaliação de uso do Sei! a todos os campi da UFMT , de forma a explorar 
e validar as dimensões emergidas no estudo.
Referências
Alves, F. P ., Nunes, E. P . S., Ferreira, R. P . (2018). Desafios e Soluções da Universidade 
Federal de Mato Grosso na Implantação do Sistema Eletrônico de Informações. In: XII 
WTICIFES, maio de 2018. Foz do Iguaçu-PR.
Amaral, V., Uchôa, C. (2014). Processo Eletrônico Nacional: sua construção colaborativa 
e suas perspectivas.In: VII Congresso CONSAD de Gestão Pública, março de 2014.
Brasília,DF.
Baldan, R. de L.;Valle, R.; Rozenfeld, H. (2014). Gerenciamento de Processos de Negócios 
BPM: uma referência para implantação prática. 1.ed.-Rio de Janeiro: Elsevier.página
262
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 257-262, junho de 2019
Capítulo XLI - Sistema Eletrônico de Informações (Sei!): um estudo sobre a melhoria dos serviços 
públicos com apoio da inovação em processosBessant, J. e Tidd, J. (2009). Inovação e empreendedorismo. Porto Alegre: Bookman. 
Brasil.
Brasil. (2015). Uso de Meio Eletrônico para Processo Administrativo. Decreto 
Presidencial nº 8.539. Brasília, DF.
Grizendi, E. (2016). Manual de Orientações Gerais sobre Inovação. DF, Brazilian Journal 
of Management & Innovation v.3, n.2, Janeiro/Abril – ISSN: 2319-0639.
Lapolli, P .C. (2003). Implantação de sistemas de informações gerenciais em ambientes 
educacionais. Florianópolis. Universidade Federal de Santa Catarina.
Laudon, K. C.; Laudon, J. P . (2010). Sistemas de Informação Gerenciais. 9. ed. São 
Paulo: Pearson Prentice Hall.
Levy, E. e Drago, P . A.(2005). Gestão pública no Brasil contemporâneo. São Paulo: Casa 
Civil.
Pereira, R. M.; Castro, S. O. C. de; Marques, H. R.; Botelho, L. H. F.; Silva, T .S.; Freitas, A. F. 
(2016). A Informatização de processos em instituições públicas: o caso da Universidade 
Federal de Viçosa. Navus: Revista de Gestão e Tecnologia, vol. 6, nº 1, p.17-29.
Pohlmann, M., Gebhardt, C., Etzkowitz, H. (2005). The development of innovation 
systems and the art of innovation management - strategy, control and the culture of 
innovation. Technology Analysis and Strategic Management.
Simantob, M.; Lippi, R.(2003). Desmistificando a inovação inovar para competir: aula 
1 -Inovação: conceitos, definições e tipologias. In: Guia Valor Econômico de Inovação 
nas Empresas. São Paulo: Globo.página
263
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 263-268, junho de 2019
Capítulo XLII - Transformação de Projetos de Sistemas de Informação da UTFPR Através de Frameworks
ModernosTransformação de Projetos de Sistemas de 
Informação da UTFPR Através de Frameworks 
Modernos
Diogo A. B. Pereira, Marcelo A. Faveri, Rui P. Leite e William H. dos Santos
Universidade Tecnológica Federal do Paraná (UTFPR)
Av. Sete de Setembro, 3165 - Rebouças - CEP: 80230-901 – Curitiba – PR
{diogopereira,marcelofaveri,leite,williamsantos}@utfpr.edu.br
Resumo
Este artigo descreve a aplicação de metodologia ágil juntamente com a adoção de novas tecnologias no Departamento 
de Projetos de Sistemas na UTFPR. Descreve os motivos e os ganhos da adoção dos frameworks Angular   (front-
end) e Demoiselle (back-end), e de ferramentas apoio como o Pencil, Slack e GitLab. Relata, ainda, exemplos da 
transformação ocorrida, como o aumento da produtividade e a entrega de novos sistemas. 
Palavras-chave: métodos ágeis, Angular, Demoiselle, GitLab
1. Introdução
O gerenciamento de projetos no setor público e, em especial, de projetos 
de Tecnologia de Informação (TI), sempre foi um grande desafio. Muitas vezes, 
os profissionais carecem de recursos, tanto humanos quanto de processos, que 
poderiam otimizar o trabalho. Contudo, uma parte do desafio pode ser resolvida 
por meio de técnicas de gestão que possuem a metodologia ágil como base.
A partir de 2017, a Divisão de Projetos de Sistemas de Informação (DIPSIS) 
da Universidade Tecnológica Federal do Paraná (UTFPR) aceitou o desafio de 
promover uma mudança na forma como os novos sistemas seriam desenvolvidos 
a partir de então. A mudança começou por estudar as opções de tecnologias e 
processos disponíveis na época, elaborando em seguida um painel com os prós 
e os contras de cada um. As decisões foram tomadas levando-se em conta a 
atualização tecnológica e a implantação de uma metodologia para o gerenciamento 
de projetos.
Como objetivos dessa mudança, citam-se: reestruturar o desenvolvimento 
de software; atualizar a tecnologia; melhorar o ambiente de trabalho; facilitar a 
utilização de mão-de-obra qualificada através de contratação; definir identidade 
visual para novos sistemas; adotar nova padronização; construir softwares com 
maior agilidade e qualidade; e proporcionar maior participação do usuário final.página
264
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 263-268, junho de 2019
Capítulo XLII - Transformação de Projetos de Sistemas de Informação da UTFPR Através de Frameworks
Modernos1.1. Cenário da TI na UTFPR
A Diretoria de Gestão de Tecnologia da Informação (DIRGTI) da UTFPR, 
responsável por gerir a área de Tecnologia da Informação da Universidade, tinha 
como principal ambiente tecnológico para desenvolvimento de sistemas de 
informação, o Sistema Gerenciador de Banco de Dados (SGBD) Oracle, através de 
sua linguagem de programação Procedural Language/Structured Query Language  
ou PL/SQL1. Isso até o ano de 2016.
Uma das principais desvantagens do uso corporativo dessa tecnologia 
é a centralização em um só local de toda a estrutura do sistema, afetando a 
produtividade do trabalho, a manutenibilidade do sistema a longo prazo, o evoluir 
tecnológico com autonomia, a segurança dos dados armazenados, entre outras. 
Sendo que essas necessidades são melhor atendidas em uma pilha tecnológica 
com camadas especializadas. 
2. Métodos
O processo de transformação da forma de trabalho da DIPSIS, foi baseado 
na aplicação de métodos ágeis, junto a isso, as tecnologias também precisaram 
ser alteradas, abandonando-se a base tecnológica centralizada (PL/SQL) para um 
ambiente tecnológico diverso, separado em camadas especializadas.  O processo 
iniciou-se com a divisão em duas frentes: uma para back-end (tecnologias de lado 
servidor) e outra para front-end (tecnologias de lado cliente).
Para o back-end, a equipe estudou algumas linguagens e foi escolhida o 
JAVA, pois os analistas já tinham conhecimento necessário. Foram considerados: 
Autenticação compatível com o Lightweight Directory Access Protocol  (LDAP) e/ou 
sistemas corporativos. Ter autenticação única entre as aplicações (Single Sign On). 
Adotar o conceito de serviços (webservices) consumidos por várias plataformas. 
Além de ser escalável, com treinamento rápido, que fosse produtiva e com reúso 
de código.
Para o front-end, a análise começou pela busca dos frameworks e bibliotecas 
mais estabelecidos para desenvolvimento de Single Page Applications (SPAs). Os 
dois mais promissores foram o Angular2, do Google, e React, do Facebook. A maior 
vantagem do Angular (o escolhido), foi ser um framework opinativo, que procura 
uma solução padrão para cada problema de programação, além de disponibilizar 
um guia de boas práticas [Zorzo e Bernadi, 2015]. Também, a padronização de 
ferramentas introduzida foi decisivo, tendo em vista a diversificação da mão-de-
obra na UTFPR.
Além das linguagens definidas, foram escolhidas as seguintes tecnologias 
especializadas: para o acesso aos dados: SGBD Oracle, que viabilizou o uso de 
1 Oracle PL/SQL https://www.oracle.com/technetwork/database/features/plsql/, Acesso 18/04/2019
2 Angular Framework. https://angular.io/, Acesso 24/02/2019página
265
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 263-268, junho de 2019
Capítulo XLII - Transformação de Projetos de Sistemas de Informação da UTFPR Através de Frameworks
Modernosoutros SGBDs, como o PostgreSQL3; para os fluxos de negócio: Java EE4 com 
Framework Demoiselle — back-end; para controles de formulário avançados: 
PrimeNg5; e para o tema da interface de usuário o Bootstrap6.
2.1. Métodos Ágeis e Scrum
Ao longo dos anos, vários métodos de desenvolvimento de software 
foram apresentados e utilizados. Dentre eles, os chamados métodos ágeis, são 
considerados mais adaptativos e flexíveis que os tradicionais [Simoyana e Battisiti, 
2015]. Além disso, eles são indicados para cenários em que existe constante 
mudança de requisitos e os resultados devam ser entregues em pequenos espaços 
de tempo. 
Há grande diferença entre o modo de implementação de um projeto baseado 
em um método tradicional e um método ágil, onde ocorrem pequenos avanços e 
melhorias desde o início do processo.
As características de um modelo ágil são: Perspectiva: fora para dentro 
“Os usuários são os especialistas” . Foco: aprender rápido sobre as necessidades 
do usuário final. Abordagem para a tomada de decisão: aprender a partir de 
experimentos e feedback do usuário. Risco: deve ser considerado ao qualificar o 
processo de aprendizagem e Implementação: pequenos passos desde o começo. 
A figura 1, traz uma ideia precisa da metodologia ágil, expondo seus benefícios e 
comparando-a com o modelo de gestão tradicional.
Figura 1. Ideia central do processo ágil [Inova, 2019]
 Para implementar os métodos ágeis, os recursos humanos da DIPSIS 
consistia em 2019 em 4 servidores e 4 estagiários. Com o progresso dos trabalhos, 
3 PostgreSQL https://www.postgresql.org/ , Acesso 18/04/2019
4 Java EE https://www.oracle.com/technetwork/pt/java/javaee/overview/index.html , Acesso 18/04/2019
5 PrimeNg https://www.primefaces.org/primeng/#/ , Acesso 18/04/2019
6 Bootstrap https://getbootstrap.com/, Acesso 18/04/2019página
266
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 263-268, junho de 2019
Capítulo XLII - Transformação de Projetos de Sistemas de Informação da UTFPR Através de Frameworks
Modernoso quadro cresceu, e em 2019 a DIPSIS é formada por 6 servidores, com o mesmo 
número de estagiários, com o diferencial de que desde 2018 conta com uma 
empresa prestadora de serviços de desenvolvimento. O aumento de quadro e a 
empresa de desenvolvimento, possibilitou um ganho na produtividade.
2.2. O Framework Demoiselle
Para a escolha do Demoiselle pesou sobretudo o fato de ser desenvolvido 
e mantido pelo Serviço Federal de Processamento de Dados (SERPRO), empresa 
pública de grande destaque na hierarquia administrativa [Müller et all, 2019]. 
Mas não apenas, outro fator decisivo na escolha decorreu da alteração de rumos 
por que o próprio framework passou, ou seja, a partir das grandes modificações 
sofridas com a versão 3, o conceito adotado foi exatamente ao encontro do que as 
pesquisas realizadas pela equipe indicavam: um modelo de desenvolvimento web 
com disponibilização de webservices.
 Isto é, por meio de interfaces de programação, ou Application Programming 
Interface (APIs) de acesso a processos de negócio, separando profundamente as 
camadas de back-end e  front-end. Também se destaca o fato de viabilizar diversas 
implementações de tecnologias especialmente úteis no desenvolvimento de 
sistemas corporativos. Uma dessas tecnologias que o framework agrega é o 
Swagger, ferramenta que viabiliza documentação auto-gerada de uma definição 
de API, isto é, através de anotações específicas nos webservices, a ferramenta 
gera automaticamente toda a documentação de acesso à API, permitindo aos 
desenvolvedores front-end identificarem rapidamente quais processos de negócio 
estão disponíveis para consumo, de que modo consumí-lo e qual o retorno 
esperado.
 Outra tecnologia especialmente importante disponibilizada, baseado no 
JavaScript Object Notation (JSON)7, é a implementação do JSON Web Tokens (JWT), 
um padrão corporativo de comunicação segura e completamente stateless entre 
duas partes computacionais. É com o uso do JWT que se permite a comunicação 
segura, em termos de autenticação e de autorização, entre front-end e back-end.
2.3. Ferramentas Complementares
Junto com as decisões relativas ao front-end e o back-end, ferramentas 
complementares foram pesquisadas e incorporadas ao processo de 
desenvolvimento. Em seguida são listadas as escolhas realizadas pela DIPSIS.
 Ferramenta de Prototipação: foi utilizado o software gratuito e open source 
Evolus Pencil8 com o tema que remetia ao Bootstrap, com intuito de criar protótipos 
que  facilitassem conversas e homologações com os clientes.
 Ferramenta de Comunicação: para comunicação ágil e o compartilhamento 
7 JSON. (2019) https://www.json.org/json-pt.html, Acesso em 18/04/2019
8 Pencil. (2019) https://pencil.evolus.vn/, Acesso em 18/04/2019página
267
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 263-268, junho de 2019
Capítulo XLII - Transformação de Projetos de Sistemas de Informação da UTFPR Através de Frameworks
Modernosde dados, o Slack9 foi escolhido. O compartilhamento da informação é fundamental 
para melhoria da comunicação em ambientes corporativos, assim, o Slack oferece 
uma plataforma na qual equipes podem se comunicar com texto, além de permitir 
o envio de vídeos e documentos, mantendo tudo organizado e centralizado.
 Ferramenta de controle de Versão: para o controlar o fluxo de trabalho  
colaborativo, foi escolhido o GitLab10, que é um gerenciador de repositório 
baseado em Git. Permite que os desenvolvedores armazenem o código em seus 
próprios servidores corporativos. Os motivos da escolha, como substituto da 
ferramenta anteriormente utilizada, o Apache Subversion, conhecido por SVN, são a 
descentralização, ferramentas e serviços compatíveis, que facilita o gerenciamento 
de vários desenvolvedores.
3. Resultados
Antes da mudança de cenário, as características do trabalho realizado pela 
DIPSIS eram: retrabalhos constantes; mudanças constantes de escopo; demora no 
atendimento; insatisfação dos desenvolvedores e insatisfação dos clientes.
 O projeto piloto foi o reescrita de um módulo do sistema Sistema de Orçamento 
e Gestão. Esse foi escolhido por apresentar grande número de reclamações e grande 
uso. Os erros e a quantidade de horas de manutenção corretiva se aproximaram do 
zero. O segundo, foi o Sistema de Acompanhamento de Projetos. Um novo sistema, 
utilizando todas as definições descritas nesse artigo. A implantação ocorreu em 
março de 2019. 
O sistema servirá como ferramenta para registro e acompanhamento de 
projetos da instituição por servidores ativos, tanto docentes quanto técnicos 
administrativos. Para os gestores foram disponibilizados diversos relatórios 
gerenciais para acompanhamento e tomada de decisão em relação aos objetivos 
da Universidade. Destaca-se ainda o uso do GitLab no gerenciamento dos projetos, 
servindo como ferramenta para registro e acompanhamento de projetos da 
instituição por servidores da UTFPR.
Para o gestor, ela facilitou, além do versionamento, o acompanhamento do 
projeto, as prioridades definidas para cada sprint, além de gerar alguns relatórios 
gerenciais, auxiliando nas decisões críticas do projeto, como por exemplo, na 
alocação de mais recursos ou mudança de prioridades. 
No período de implantação desta nova metodologia na DIPSIS, o front-end  
teve 927 commits realizados por 17 autores diferentes, considerando o total de 
estagiários e colaboradores da empresa de desenvolvimento. Já o back-end teve 
556 commits realizados por 6 autores, apenas servidores da UTFPR.
Ressalta-se ainda que o Demoiselle usa JWT , um padrão aberto e industrial 
de comunicação segura entre as partes sem a necessidade de mecanismos 
sofisticados de manutenção de sessões, pois é stateless. Como resultado da 
9 Slack. (2019) https://slack.com/, Acesso em 07/03/2019
10 GitLab (2019) https://about.gitlab.com/, Acesso em 05/03/2019página
268
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 263-268, junho de 2019
Capítulo XLII - Transformação de Projetos de Sistemas de Informação da UTFPR Através de Frameworks
Modernosutilização da tecnologia, tornou-se possível que dois ou mais application servers  
distribuam a mesma aplicação back-end, viabilizando alta escalabilidade aos 
softwares desenvolvidos [RFC, 2019].
Como resultados macros depois da mudança de cenário, têm-se: maturidade 
da equipe; desenvolvimento modular de sistemas; diminuição na quantidade de 
manutenções corretivas; fluxo de trabalho melhor definido; maior integração e 
motivação da equipe; maior participação do cliente no processo; possibilidade 
de contratação de desenvolvimento; desenvolvimento de forma incremental, 
com entregas em sprints de, em média, 15 dias e separação das tecnologias em 
camadas especializadas.
4. Conclusões
Os resultados mostram que houve ganhos significativos com a adoção das 
novas tecnologias. Além disso, houve grande aceitação por parte da equipe de 
desenvolvimento e dos usuários.
Destaca-se ainda que, em relação ao back-end, a utilização de uma tecnologia 
orientada a objetos e uma separação clara entre as camadas de dados, de acesso 
aos dados, de negócio e de interface com o usuário possibilita, além do ganho 
de produtividade na formulação e no desenvolvimento de novos sistemas, a 
diminuição de falhas operacionais e a possibilidade de intervenções mais precisas 
e pontuais em relação a eventuais manutenções evolutivas e corretivas. 
Por fim, foi uma grande mudança essa pela qual o setor passou, com nova 
postura tanto de gestão quanto tecnológica, com importante e significativa 
atualização tecnológica, que trouxe bons resultados tanto na questão de 
gerenciamento de equipes quanto na aplicação das tecnologias em projetos finais.
Referências
Müller, Ezequiel J.; Both, Marcelo J.; Alves, Roberson J. F. “DELIBRIS: Sistema Bibliotecário 
Utilizando Demoiselle Framework” (2019)  https://www.frameworkdemoiselle.gov.br/
documents/, Acesso 05/03/2019. 
Rede de Inovação no Setor Público InovaGov (2019) Disponível http://inova.gov.br/
abordagem-agil/  21/04/2019.
RFC 7519. (2019) Disponível em https://tools.ietf.org/html/rfc7519, Acesso 05/03/2019. 
Simoyana Felipe de O.; Battisiti, Maria C. G. (2015) “Adaptação e Implantação da 
Metodologia Scrum para Projetos Ágeis numa Autarquia Federal”  IV SINGEP SP .
Zorzo, Rafael; Bernardi, Élder. (2019) “Estudo e Desenvolvimento de Caso de Uso com 
o Framework Angular” https://painel.passofundo.ifsul.edu.br/uploads/arq/, Acesso página
269
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 269-274, junho de 2019
Capítulo XLIII - UFPB-Sentinela: Um Sistema de Segurança para a Cidade UniversitáriaUFPB-Sentinela:
Um Sistema de Segurança para a Cidade 
Universitária
Iron A. de A. Júnior¹, Sandro L. I. Araujo¹, José Augusto L. B. de C. Filho¹, Arthur S. A. 
de Melo¹
¹Superintendência de Tecnologia da Informação – Universidade Federal da Paraíba (UFPB) 
Endereço Postal 58051-900 – Campus Universitário I, João Pessoa - PB – Brazil
{ironaraujo,sandrolopes,joseaugusto,arthur}@sti.ufpb.br
Abstract
The problems related to public security affect all sectors of society, contributing to the increase of crime also in 
public educational institutions. The Sentinela is a security system that aims to maintain the integrity of the entire 
academic community. The solution described in this article is composed of a set of integrated applications, such as: a 
mobile application, a Web application and an institutional authentication service, as well as a system that supports 
all the others.
Keywords: Public Security, Institutional Security, Security System
Resumo
Os problemas relacionados à segurança pública afetam todos os setores da sociedade, contribuindo para o aumento 
da criminalidade, também, nas instituições públicas de ensino. O Sentinela é um sistema de segurança que visa 
manter a integridade de toda a comunidade acadêmica. A solução descrita neste artigo é composta por um conjunto 
de aplicações integradas, dentre elas: uma aplicação para dispositivos móveis, uma aplicação Web e um serviço de 
autenticação institucional, além de um sistema que fornece suporte a todos os demais.
Palavras-chave: Segurança Pública, Segurança Institucional, Sistema de Segurança
1. Introdução
A segurança pública é o estado de normalidade que permite o usufruto de 
direitos e o cumprimento de deveres, constituindo sua alteração ilegítima uma 
violação de direitos básicos, geralmente acompanhada de violência que produz 
eventos de insegurança e criminalidade. A violência urbana persiste como um dos 
mais graves problemas sociais no Brasil, totalizando mais de 1 milhão de vítimas 
fatais desde a década de 90. Estudo divul-gado pelo Escritório das Nações Unidas 
sobre Drogas e Crime (UNODC) mostrou que o Brasil possui 2,8% da população 
mundial, mas acumula 11% dos homicídios de todo o mundo [de Lima et al. 2016].
A comunicação entre cidadãos e autoridades é um fator crítico que pode 
determi-nar a eficácia do poder do estado na proteção da comunidade. Processos 
morosos que incluem identificação do usuário, localização e relato do evento, 
apesar de essenciais, po-dem consumir um tempo precioso na prestação de socorro 
aos usuários do sistema, além de estar vulnerável à falsa comunicação de crime. página
270
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 269-274, junho de 2019
Capítulo XLIII - UFPB-Sentinela: Um Sistema de Segurança para a Cidade UniversitáriaAcompanhado de políticas públicas de segurança, a tecnologia da informação é 
capaz de realizar um papel importante no combate e prevenção da criminalidade.
O projeto Sentinela tem como principal objetivo otimizar a comunicação 
entre a comunidade acadêmica e os agentes de segurança pública que atuam nos 
campi da Universidade Federal da Paraíba (UFPB) utilizando mecanismos rápidos e 
seguros de identificação do usuário, localização em tempo real bem como registro 
em bases de dados tornando mais rápido o registro de ocorrências.
Portanto, esse trabalho descreve toda a infraestrutura utilizada por um 
sistema de segurança da cidade universitária que envolve, desde os participantes 
da comunidade acadêmica, através de um dispositivo móvel, até os agentes de 
segurança pública, de plantão em uma central.
2. Método
Todo o processo de metodologia envolve 4 aplicações: 1) Sistema único de 
identificação de usuários do SIG; 2) Aplicativo Mobile; 3) Sistema de gerenciamento 
de infomação na nuvem; e 4) Aplicação Web para a Central de Segurança.
2.1. Sistema Único de Identificação
Utilizando a tecnologia Open Authentication versão 2 (Oauth2), o Sistema 
Único de Iden-tificação (do inglês: Single sign-on – SSO) [Hammer 2007], 
comumente utilizado para permitir que usuários de Internet possam acessar sites 
utilizando contas de diferentes bases de dados, como o Google, Facebook e Twitter, 
sem expor suas senhas. Dessa forma, permite que os proprietários da informação 
compartilhem recursos com o sistema a ser usado sem que seja necessário a 
exposição de suas credenciais.
 A Superintendência de Tecnologia da Informação (STI) da UFPB implementou 
o sistema SSO, que permite que aplicativos utilizem a informação de membros da 
comu-nidade acadêmica, após a autorização do usuário, de forma mais rápida. 
Dessa forma possível acessar informações confiáveis encontradas nas bases de 
dados acadêmicas e certificadas pelo usuário que fornece as informações.
 Sistemas como o SSO são essenciais para a otimização de identificação do 
usuário de forma confiável, agilizando a autenticação dos usuários, visto que os 
dados estão à disposição da aplicação autorizada.
2.2. Aplicativo de Dispositivo Móvel
Disponível nas principais plataformas de dispositivos móveis, a aplicação 
Sentinela-UFPB pode ser utilizada por qualquer usuário da comunidade 
acadêmica desde que esteja cadastrado nas bases de dados do Sistema Integrado 
de Gestão (SIG) da instituição. O usuário realiza a autenticação no sistema usando 
as credenciais utilizadas no SIG, dessa forma, suas informações e um token de página
271
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 269-274, junho de 2019
Capítulo XLIII - UFPB-Sentinela: Um Sistema de Segurança para a Cidade Universitáriasegurança são fornecidos pela plataforma do SSO para a aplicação móvel.
Desenvolvido usando a tecnologia React Native [Inc 2019], o aplicativo 
consiste na disponibilização de um Botão do Pânico. Com uma interface simples 
e intuitiva, o usuário que se sentir ameaçado poderá acionar essa funcionalidade 
para pedir Socorro aos agentes de segurança da instituição. A solicitação é enviada 
imediatamente à central de segurança onde poderão ser visualizadas as seguintes 
informações: Nome, Telefone, Foto e Localização do solicitante. Assim, a segurança 
saberá com precisão as identificações do usuário e da localização do evento.
Figura 1. Telas do APP Sentinela Mobilepágina
272
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 269-274, junho de 2019
Capítulo XLIII - UFPB-Sentinela: Um Sistema de Segurança para a Cidade UniversitáriaPor não exigir a necessidade de uma ligação e nenhum tipo de descrição da 
ocorrência no momento do chamado de socorro, essa forma de solicitação facilita 
a comunicação entre o usuário e a segurança, visto que as informações sobre o 
solicitante foram previamente fornecidas já estão armazenadas no Smartphone 
do usuário, e serão enviadas automaticamente para a central de segurança no 
momento do chamado. A fim de manter as informações de localização atualizadas, 
periodicamente o dispositivo móvel enviará a geolocalização.
A Figura 1 ilustra as telas da solução do dispositivo móvel na loja de aplicati-vos 
da Google, para o sistema operacional Android. Inicialmente o usuário informará 
suas credenciais, que ficarão salvas no smartphone; já na tela principal estará 
disponí-vel o “BOTÃO DE SOCORRO” , que quando acionado permite a suspensão 
do chamado durante um período de 3 segundos através do botão cancelar. Caso 
o recurso de cancela-mento não seja solicitado, o pedido de socorro será enviado 
para a equipe de segurança, que passará a monitorá o andamento da solicitação. 
A qualquer momento o usuário tem a opção de cancelar o atendimento.
Figura 2. Fluxo da Informação Entre os Sistemas: a) Aplicação móvel; b) Aplicação 
Backend; c) Aplicação Web
2.3. Sistema de Gerenciamento de Informação na Nuvem
Essa parte do sistema é responsável pela orquestração de toda a informação 
que transita entre os sistemas. Este Backend, que é a parte do sistema responsável 
pela manipulação da informação, recebe as informações enviadas a partir dos 
dispositivos móveis dos usuários finais, realiza operações de persistência em 
bancos de dados e ainda informa à aplicação da central de segurança atualizações 
sobre ocorrências nos Campi.
As informações fornecidas pela aplicação móvel serão enviadas para esta 
camada, que posteriormente serão compartilhadas com a aplicação web. Estes página
273
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 269-274, junho de 2019
Capítulo XLIII - UFPB-Sentinela: Um Sistema de Segurança para a Cidade Universitáriadados proverão subsídios para que os responsáveis pela segurança da cidade 
universitária possam tomar as devidas providências.
Todo o fluxo de informação pode ser demonstrada conforme a (Figura 2). 
Em Figura 2.a, o dispositivo móvel envia o pedido de socorro para o sistema de 
gerenciamento na nuvem, em Figura 2.b. Este sistema realiza a persistência da 
informação nos bancos de dados e informa à aplicação web o evento ocorrido. Em 
2.c, o Sistema Web notifica os responsáveis que poderão acompanhar o chamado 
e tomar as providências necessárias para atender o solicitante.
2.4. Aplicação Web Para a Central de Segurança
Com o objetivo de gerenciar os pedidos de socorro enviados pelos usuários do 
aplica-tivo móvel, foi desenvolvido uma aplicação Web utilizando uma ferramenta 
que vem ga-nhando cada vez mais espaço no mercado de tecnologia: React JS. 
Criada pela equipe do Instagram, esta tecnologia foi adicionada aos projetos 
open-source do Facebook, impul-sionando mais ainda sua adoção [Inc. 2019]. 
Para renderização do mapa, o React JS foi integrado a uma biblioteca JavaScript 
de código aberto para mapas interativos: Leaflet [Cam and contributors. 2019].
A aplicação Web consiste na exibição de uma lista de chamados, ordenados 
crono-logicamente, e um mapa, que estará centralizado em um dos campi da 
UFPB, de acordo com o campus selecionado previamente pelo vigilante. A cada 
novo chamado, alertas vi-suais e sonoros serão disparados, e um novo ponto será 
exibido no mapa. Essa marcação exibirá a localização atual do aplicativo móvel, 
que será atualizada constantemente.
Um chamado será removido da lista, assim como seu respectivo ponto 
no mapa, quando o segurança decidir que foi finalizado ou quando o próprio 
usuário do aplicativo móvel cancelar o pedido de socorro. Porém as informações 
permanecerão nas bases de dados para futuras consultas.
3. Resultados
A aplicação não está disponível para uso e o lançamento da versão final, 
inicialmente para Android, está prevista para Julho de 2019. Por envolver um 
cenário específico de uso, os responsáveis pela segurança da instituição deverão 
passar por treinamento e o aplicativo deverá ter ampla divulgação entre os alunos.
Com a implantação do Sentinela, espera-se aumentar a sensação de 
confiança entre os usuários da instituição. Além disso, terá como principal foco 
a inibição de atividades irregulares na universidade, visto que usuários que se 
sentirem ameaçados poderão invocar a segurança através do aplicativo para 
intervir na situação.página
274
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 269-274, junho de 2019
Capítulo XLIII - UFPB-Sentinela: Um Sistema de Segurança para a Cidade Universitária4. Conclusão e Trabalhos Futuros
O sistema Sentinela fornece um amparo aos usuários da instituição, criando 
um canal de comunicação direta com a segurança universitária, o que aumenta o 
sentimento de proteção nos ambientes de ensino.
Seguindo o mesmo entendimento e com o objetivo de manter a 
integridade dos bens da instituição e da comunidade, outras demandas 
poderão ser desenvolvidas e implementadas. Para trabalhos futuros, sugere-se a 
implementação de funcionalidades relacionadas à denúncia de alguma ocorrência 
suspeita, que permitam o envio de imagens, áudios e vídeos e possam ser apurados 
em momento posterior.
Referências
Cam, P . L. and contributors. (2019). React-leaflet. disponível em https://reactleaf letjs.
org acesso em 18 de abril de 2019.
de Lima, R. S., Bueno, S., and Mingardi, G. (2016). Estado, polícias e segurança pública 
no brasil. Revista Direito GV, 12(1):49–85.
Hammer, E. (2007). Explaining oauth. disponível em https://oauth.net acesso em 18 de 
abril de 2019.
Inc., F. (2019). React. disponível em https://reactjs.org acesso em 18 de abril de 2019.
Inc, F. (2019). React native. disponível em https://facebook.github.io/react native/
docs/getting started.html acesso em 18 de abril de 2019.
Johnson, R., Hoeller, J., Donald, K., Sampaleanu, C., Harrop, R., Risberg, T ., Arendsen, A., 
Davison, D., Kopylenko, D., Pollack, M., et al. (2004). The spring framework– reference 
documentation. interface, 21:27.página
275
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 275-280, junho de 2019
Capítulo XLIV - Uma Análise das faturas de telefonia fixa da UFMT utilizando ferramentas gratuitas 
de ETL e Business IntelligenceUma Análise das faturas de telefonia fixa da 
UFMT utilizando ferramentas gratuitas de ETL e 
Business Intelligence
Jonata B. M. dos Santos¹,², Jean Caminha², Ana M. M. Paiva¹, Willdson G. de Almeida¹
¹Secretaria de Tecnologia da Informação– Universidade Federal de Mato Grosso
(UFMT)
78060-900 –Cuiabá  – MT – Brasil
²Instituto de Computação – Universidade Federal de Mato Grosso (UFMT) 78060-900 – Cuiabá – MT 
– Brasil
{jonata, jcaminha, anamarquespaiva, willdson}@ufmt.br
Resumo
O processo ETL ({Extract, Transform, Load} - Extração, Transformação, Carga) é uma técnica de análise de Business 
Intelligence (BI) que executa a extração e manipulação de dados, de forma a obter informações que subsidiam 
os gestores das organizações nas tomadas de decisões. O presente artigo relata a aplicação do processo ETL nas 
faturas telefônicas da Universidade Federal de Mato Grosso (UFMT), utilizando ferramentas gratuitas. Um ato 
administrativo baseado nesta análise, pode gerar uma economia de R$ 11.000,00 em cinco anos para a Universidade. 
Palavras-chave: ETL, Telefonia Fixa, Business Intelligence (BI)
1. Introdução
Os recursos públicos administrados pelas universidades são por vezes 
restritos e insuficientes, ainda que exista necessidade crescente de manutenção e 
melhoria da infraestrutura e dos serviços [Morais et al. 2019].
As empresas e órgãos federais realizam o gerenciamento de centenas de 
faturas, realizando muitas vezes seu controle, somente por meio de planilhas, 
correndo o risco de perda de informações e inconsistência de dados [Dittrich et al. 
2017].
A Universidade Federal de Mato Grosso (UFMT), por meio da Secretaria de 
Tecnologia da Informação (STI), tem priorizado a profissionalização da gestão dos 
recursos de tecnologia da informação e comunicação, implantando metodologias, 
ferramentas e processos que buscam eficiência da aplicação dos recursos públicos 
[UFMT 2018].
A Universidade Federal de Mato Grosso (UFMT), por meio da Secretaria de 
Tecnologia da Informação (STI), tem priorizado a profissionalização da gestão dos 
recursos de tecnologia da informação e comunicação, implantando metodologias, 
ferramentas e processos que buscam eficiência da aplicação dos recursos públicos 
(Business Intelligence), com propósito de melhorar a gestão dos recursos aplicados 
neste serviço.página
276
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 275-280, junho de 2019
Capítulo XLIV - Uma Análise das faturas de telefonia fixa da UFMT utilizando ferramentas gratuitas 
de ETL e Business Intelligence2. Métodos
O ETL (Extract, Transform, Load) é um processo para extrair dados de uma 
base de dados (BD), aplicando métodos para o processamento, modificação e 
inserção em outra base de dados, de forma a facilitar as atividades de análise de 
informações [Ferreira et al. 2010]. O processo ETL é oriundo dos estudos de BI.
Os dados usados neste estudo de caso fazem parte do acervo de faturas de 
telefonia fixa referentes aos meses de Fevereiro a Dezembro do ano de 2018. As 
faturas foram geradas inicialmente no formato (Comma-separated value - .csv) e 
convertidas para o formato (eXceL Spreadsheet - .xls).
Para a execução do processo ETL, foi utilizada a ferramenta Pentaho Data 
Integration (PDI), também conhecida como Spoon ou Kettle. O PDI implementa 
o processo ETL baseado em uma aproximação orientada a metadados [Oliveira 
2012].
A primeira fase do processo ETL refere-se a extração dos dados de diversas 
fontes. O conjunto inicial estudado era composto pelas faturas dos meses de 
fevereiro, março e abril que continham campos adicionais quando comparadas 
com os outros meses subsequentes. Desta forma, foram definidos dois grupos de 
entrada de dados (Figura 1). O grupo 1 para os meses de fevereiro à abril e o grupo 
2 para os demais meses.
Em seguida, foi realizado o processo de transformação de dados (Figura 
1). Nesta fase, foram identificados os campos estratégicos e removidos os dados 
desnecessários e caracteres especiais. Também foram adicionados um campo para 
identificação única da ligação e de tipificação dos dados de cada campo, além dos 
filtros para os tipos de serviços analisados. Finalmente, foi realizada as conversões 
de tempo para a durações das chamadas e de mapeamento de valores nulos.
Por último, foi executada a exportação dos dados transformados em um 
arquivo tipo texto (.txt), com os campos dos dados separados por ponto e vírgula  
(Figura 1).
Figura 1. Processo ETL para as faturaspágina
277
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 275-280, junho de 2019
Capítulo XLIV - Uma Análise das faturas de telefonia fixa da UFMT utilizando ferramentas gratuitas 
de ETL e Business Intelligence Após a finalização do processo ETL pela ferramenta PDI, o arquivo gerado 
foi convertido para o format eXceL Spreadsheet (.xls), necessário para seu uso na 
ferramenta de análise. O arquivo resultante contém apenas os campos de dados 
necessários para o estudo, devidamente padronizados (Figura 2). A análise dos 
dados foi feita com o apoio da ferramenta Power BI1. A ferramenta Power BI é 
desenvolvida pela empresa Microsoft e possui uma versão para uso sem custos, 
permitindo que o usuário visualize facilmente informações por meio de dashboards  
e gráficos.
Figura 2. Resultado do processo ETL.
3. Resultados
As informações geradas pelo processo de extração e análise de dados podem 
auxiliar os gestores a tomarem decisões que otimizem os custos da fatura de 
telefonia fixa da Universidade. O uso do serviço pode ser classificado e visualizado 
quanto ao seu custo financeiro, individualmente para cada ramal.
O montante despendido com a telefonia fixa pela UFMT ao longo do período 
em análise corresponde ao valor de R$ 183.002,34, levando há um gasto médio da 
universidade com a operadora de R$ 11.636,57 por mês. 
A análise individual das ligações telefônicas objetivou a mensurar o custo 
de cada ramal ativo dentro da Universidade. Foi identificado a existência de um 
ramal de custo médio anual de R$ 4.900,00, acima do dobro que o segundo mais 
utilizado. Os 10 maiores custos dos ramais podem ser visualizados na Figura 3. Os 
ramais foram codificados com numeração sequencial.
1 https://powerbi.microsoft.com/pt-br/página
278
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 275-280, junho de 2019
Capítulo XLIV - Uma Análise das faturas de telefonia fixa da UFMT utilizando ferramentas gratuitas 
de ETL e Business Intelligence
Figura 3. Custo médio (em R$) dos 10 ramais mais utilizados
O catálogo telefônico disponível no sítio da Universidade2 foi utilizado para 
realizar a associação dos ramais com as unidades responsáveis. A participação 
percentual em relação ao custo da telefonia (Tabela 1) permite destacar que 
apenas o RAMAL 1 representa 2,68\% do custo total do serviço de telefonia.
Tabela 1. Custo (em %) dos ramais por departamento
Outra análise foi realizada para verificar a destinação das chamadas 
efetuadas. Foi identificado que um único telefone externo recebeu 4.758 ligações 
originadas da Universidade, representado mais que o triplo do segundo telefone 
mais chamado. Os 10 maiores destinos externo das ligações originadas da 
Universidade podem ser visualizados na Figura 4. Os ramais foram codificados 
com numeração sequencial.
2 https://www.ufmt.br/ufmt/site/catalogo/indexpágina
279
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 275-280, junho de 2019
Capítulo XLIV - Uma Análise das faturas de telefonia fixa da UFMT utilizando ferramentas gratuitas 
de ETL e Business Intelligence
Figura 4. Quantidade de ligações telefônicas dos 10 ramais externos mais disca 
dos na UFMT
A partir de pesquisas realizadas na Internet e nos sítios eletrônicos da 
Fundação de apoio institucional3, do Sindicato dos trabalhadores técnicos 
administrativos4, da Caixa Econômica Federal56 e do Banco do Brasil7, foi possível 
identificar os números discados e o seu custo associado, conforme a Tabela 2.
Tabela 2. Custo (em R$) dos ramais discados e o seu respectivo responsável
Dois destinos de ligação chamaram a atenção: A Fundação de Apoio 
Institucional e o Sindicato dos Trabalhadores Técnicos Administrativos. As duas 
organizações estão instalados no mesmo campus da Universidade. Considerando 
que os mesmos possuem contratos separados, um ato administrativo possível é 
a interligação dos serviços de telefonia destes com os da UFMT , com intuito de 
reduzir os custos das chamadas locais. Com essa medida, a projeção de redução 
do custo é de R$ 183,00 ao mês. Isso representará o valor de R$ 2.196 ao ano. Em 5 
anos, a instituição poderá economizar  R$ 11.000,00  em serviço de telefonia fixa.
3 http://www.fundacaouniselva.org.br/novoSite/Fundacao/Contato.aspx
4 http://www.sintufmt.org.br/contato
5 http://www.caixa.gov.br/atendimento/Paginas/default.aspx
6 http://www.caixa.gov.br/seguranca/sac/Paginas/default.aspx
7 https://www.bb.com.br/pbb/pagina-inicial/atendimento#/página
280
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 275-280, junho de 2019
Capítulo XLIV - Uma Análise das faturas de telefonia fixa da UFMT utilizando ferramentas gratuitas 
de ETL e Business Intelligence4. Conclusões
Os resultados obtidos neste trabalho confirmam a viabilidade do uso de 
ferramentas de extração de dados e de análise para maximizar a eficiência do 
serviço público. Com os dados existentes, pode-se obter informações em relação 
ao custo e o uso das ligações telefônicas.
Essas informações podem auxiliar os gestores a tomarem decisões 
importantes, tais como, definir os departamentos que continuarão à fazer ligações 
externas, tendo como base os custos, os benefícios e os objetivos da instituição, e  
implantar políticas de uso para o serviço de telefonia fixa, que vise coibir o uso de 
ligações indevidas.
Como trabalho futuro pode-se analisar os valores cobrados em cada nova 
fatura de telefonia fixa a ser paga, com os valores que estão previsto no contrato, 
evidenciando as cobranças indevidas que possa ocorrer durante a vigência do 
contrato.
Referências
Dittrich, E. P ., Nunes, J. C., Pereira, L. S., and Almeida, R. F. (2017). Sistemas de 
faturas. Monografia (Tecnólogo em Análise e Desenvolvimento de Sistemas), UFPR 
(Universidade Federal do Paraná), Curitiba, Brasil.
Ferreira, J., Miranda, M., and e José Machado, A. A. (2010). O Processo ETL em Sistemas 
Data Warehouse. INFORUM, 2010. Simpósio de Informática, page 757–765.
Morais, G. M., dos Santos, V. F., and Neto, M. T . R. (2019).  Gestão de custos no setor público:  
um estudo em um restaurante universitário. Brazilian Journal of Development, 5(3): 
1913–1933.
Oliveira, O. R. F. (2012).  Extração de conhecimento nas listas de espera para consulta e 
cirurgia. Mathesis, Universidade do Minho.
UFMT (2018).   Relatório de gestão 2016 – 2018: governança,  qualidade acadêmica e 
pluralidade. Editora da Universidade Federal de Mato Grosso. Disponível em:
http://editora.ufmt.br/download/2019/Relat%C3%B3rio%20de%20Gest%C3%A3o%
202016%20%202018%20-%20eBook.pdf . Acesso em: 15 mar. 2019.página
281
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 281-286, junho de 2019
Capítulo XLV - Uma solução de automatização no processo de pagamento de bolsas de auxílioUma solução de automatização no processo de 
pagamento de bolsas de auxílio
André Thiago Pereira Rodrigues¹, Raphael Pires Ferreira²
¹Pró-Reitoria Administrativa – Universidade Federal de Mato Grosso (UFMT)
78060-900 – Cuiabá – MT – Brasil
²Secretaria de Tecnologia da Informação – Universidade Federal de Mato Grosso (UFMT)
78060-900 – Cuiabá – MT – Brasil
{andre,raphael}@ufmt.br 
Resumo
A automatização e integração de sistemas do governo é uma tendência entre os diversos órgãos públicos. O cenário 
atual no fornecimento de bolsas na instituição apresenta inconsistências, duplicações e pagamentos equivocados de 
bolsas. Este artigo descreve como efetuar o pagamento de credores por meio da integração do processo batch do SIAFI 
com o Sistema de Gerenciamento de Bolsas e Auxílios (SGBA). Com a solução implementada espera-se um maior 
controle na concessão e no pagamento e também um melhor aproveitamento dos recursos humanos hoje empregados 
para execução dessa atividade, aumentando a eficiência e a eficácia da gestão administrativa.
Palavras-chave: Integração, SIAFI, Processo Batch
1. Introdução
A Administração Pública deve sempre primar pelos seus princípios expressos 
na Carta Magna, que realiza a sua constituição, como a eficiência. E a integração 
entre as diferentes plataformas tecnológicas vem para subsidiar a tentativa de 
alcançar ou melhorar o alcance desse princípio. Com a implantação do Governo 
Eletrônico Brasileiro no ano 2000 (BRASIL 2000) - foi possível integrar sistemas 
dentre os mais diversos órgãos da administração pública direta, autarquias e 
fundações públicas. No mesmo ano, o SIAFI, por meio do SERPRO (Serviço Federal 
de Processamento de Dados), disponibilizou o subsistema de Contas a Pagar e 
Receber (CPR) (LOPES, 2018).
É por meio do CPR que algumas Instituições Federais de Ensino Superior (IFES) 
fazem o pagamento de Bolsas e Auxílios aos estudantes e quase sempre o fazem 
de forma manual e por meio de um sistema Mainframe chamado popularmente de 
“siafi tela preta” . 
O SIAFI foi criado em meados dos anos 1986-87, por meio de uma parceria 
entre a Secretaria do Tesouro Nacional (STN) e SERPRO para resolver problemas 
de escrituração contábil que à época contava com atrasos de até 45 dias entre 
o encerramento do mês e o levantamento das demonstrações Orçamentárias, 
Financeiras e Patrimoniais do governo (STN). Mota (2005) define o SIAFI como um 
sistema que processa a execução financeira, orçamentária, patrimonial e contábil 
dos órgãos da Administração Federal.página
282
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 281-286, junho de 2019
Capítulo XLV - Uma solução de automatização no processo de pagamento de bolsas de auxílioA necessidade de automatização do pagamento de bolsas e auxílios integrado 
ao SIAFI, surgiu da alta demanda de recursos humanos para fazerem os lançamentos 
mensais destes pagamentos, a necessidade de se obter o controle gerencial sobre 
a política de concessão e pagamento que até então eram feitos apenas por um 
sistema que entrou em desuso e planilhas eletrônicas. A Controladoria Geral 
da União (CGU) no papel de auditora, recomendou que estes processos fossem 
sistematizados.
A integração entre os diferentes tipos de Sistemas de Informação (SI) é 
uma demanda crescente dos diferentes usuários que os sistemas possuem, 
principalmente na esfera educacional, conforme Chandio (2012) relata em 
seus estudos. Com os avanços tecnológicos existentes é possível garantir essas 
integrações de forma rápida, transparente e segura.
O objetivo deste artigo é demonstrar o desenvolvimento de uma solução que 
faz a integração do módulo financeiro do Sistema de Gerenciamento de Bolsas e 
Auxílios (SGBA) com o SIAFI por meio do processo batch. 
O processo de pagamento de bolsas e auxílios na UFMT tem amparo legal no 
Decreto Nº 7.234, de 19 de julho de 2010, que dispõe sobre o Programa Nacional de 
Assistência Estudantil – PNAES e resoluções internas.
Em 2018, foi iniciada a implementação de um novo sistema de bolsas, onde 
a integração com o SIAFI foi priorizada. O estudo de caso começou pela PRAE (Pró-
reitoria de Assistência Estudantil), por ter a maior quantidade de discentes em 
folha de pagamento.
3. Métodos
A metodologia da pesquisa concentrou-se nas documentações de integração 
do SIAFI, artigos da internet e a busca de soluções para o caso em outras IFES. 
Foram considerados os métodos de integração de sistemas balizados pelo Governo 
Eletrônico Brasileiro, como: 
• priorizar a definição da camada de apresentação Web para os dados/
informações dos Sistemas Corporativos do governo baseados em 
arquitetura cliente/servidor 
• BPM (Business Process Management) 
 Estas diretrizes fazem parte do e-PING e são reforçadas pela Secretaria de 
Logística e Tecnologia da Informação do Ministério do Planejamento, Orçamento 
e Gestão.
O sistema de Gerenciamento de Bolsas e Auxílios da UFMT foi projetado com 
seguintes módulos: página
283
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 281-286, junho de 2019
Capítulo XLV - Uma solução de automatização no processo de pagamento de bolsas de auxílio1. Solicitação de Bolsa - formulário de cadastro do discente para concorrer 
às bolsas e auxílios; 
2. Análise de Concessão de Bolsas - as  Assistente Sociais da Pró-reitoria de 
Assistência Estudantil faz a análise dos dados e documentos; 
3. Folha de Pagamento: geração da folha de pagamento; 
4. Autorização de Pagamento: O ordenador de despesas autoriza os 
pagamentos; 
5. Financeiro: geração do arquivo batch para integração com o SIAFI. 
Para a implementação do sistema que integra com o SIAFI, foi utilizada 
a linguagem de programação C#, framework ASP .NET MVC 5, banco de dados 
SQLServer 2008, Servidor de Aplicação: IIS-7.1.
4. Resultados
O SIAFI disponibiliza por meio da transação CONARQBT o layout do arquivo 
para ser montado com os dados dos bolsistas e auxiliados. Detalhes do layout  
desse arquivo podem ser encontrados na documentação oficial do SIAFI. A 
aplicação desenvolvida tem as funcionalidades de efetuar o upload de um arquivo 
com extensão .xlsx com um layout pré-definido. E o download de um arquivo sem 
extensão, no formato requerido conforme documentação do SIAFI.
Na documentação disponibilizado há 2 passos para fazer o pagamento de 
bolsas, o primeiro é Cadastro do Credor (CR), para o caso de o CPF ainda não 
cadastro na base de dados do SIAFI, e o segundo é a criação da Lista de Credores 
(LC), que é a efetuação do pagamento ao credor. Ambos os arquivos são compostos 
por 3 sessões nomeadas de HEADER, DETALHE e TRAILLER.
A estratégia utilizada para efetuar a geração do arquivo foi criar consultas 
em Structured Query Language (SQL) para juntar os dados, gerando uma string  
que foi disponibilizada através de um procedimento armazenado. A aplicação 
desenvolvida em C# faz a chamada deste procedimento que monta a estrutura do 
arquivo no layout requerido pelo SIAFI. Na Figura 1 é possível visualizar a tela do 
sistema para efetuar o download do arquivo.página
284
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 281-286, junho de 2019
Capítulo XLV - Uma solução de automatização no processo de pagamento de bolsas de auxílio
Figura 1. Gerenciamento de Download de Arquivo CR e LC para integrar com SIAFI.
 Conforme a documentação do SIAFI na transação CONARQBT , os arquivos 
de CR e LC, gerados, devem ter a estrutura de nome como, LCxxx e CRxxx, onde o 
“xxx” é a sequência númerica do arquivo. E após a carga do arquivo no sistema de 
transferência de arquivos da STN, o resultado de sucesso da Lista de Credor (LC) 
realizada no SIAFI tela preta pode ser verificada na Figura 3.
Figura 2. Resultado de sucesso de uma LC.
Para verificar o resultado do processamento do arquivo é preciso entrar na 
transação CONPROCBT e colocar o número da LC enviada. O layout do arquivo 
enviado, montado pelo sistema, pode ser verificado na Figura 4. A primeira linha 
do arquivo identifica a data e horário da geração do arquivo, unidade gestora e página
285
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 281-286, junho de 2019
Capítulo XLV - Uma solução de automatização no processo de pagamento de bolsas de auxíliocpf da pessoa que fará o upload do arquivo, esta linha é chamada de Header; as 
próximas 4 linhas é nomeada de Detalhe, e faz referência ao bolsista como o cpf, 
domicílio bancário, valor pago e a unidade gestora; a última linha, nomeada de 
Trailler, é preenchida com o número fixo 9 e a quantidade de registros do arquivo, 
neste caso foram 4 registros.
Figura 3. Layout de um arquivo LC,  gerado pelo sistema.
O SGBA ainda está em fase de desenvolvimento, o módulo financeiro foi 
desenvolvido usando carga de dados como substituição ao módulo 1 - Solicitação 
de Bolsas. Com as cargas de dados espera-se montar uma base de dados com 
todas as solicitações de bolsas, inclusive com a lista de espera, para que o módulo 
das pró-reitorias que oferecem bolsas possa efetuar as inclusões ou alterações de 
bolsistas. 
A coordenação financeira disponibiliza três pessoas, trabalhando oito 
horas diárias, por um período de três a quatro dias consecutivos, para fazer o 
pagamento de cerca de 1000 estudantes bolsistas/auxiliados do campus Cuiabá.  
O procedimento de geração do arquivo e carga no SIAFI pode ser feita por apenas 
1 pessoa, em no máximo 10 minutos. O arquivo no SIAFI é processado após às zero 
horas, e no próximo dia em qualquer horário, a mesma pessoa que fez a carga pode 
fazer a consulta sobre o status do arquivo, como pode ser verificado na Figura 3.
5. Conclusão
Com a solução em fase final de implementação, já se pode notar que haverá 
ganhos na produtividade das equipes que trabalham para viabilizar os pagamentos 
das bolsas, uma vez que o trabalho manual será drasticamente reduzido. Agora, a 
interação com o SIAFI se dará apenas por meio dos arquivos já gerados pelo SGBA, 
reduzindo os erros de digitação que eram frequentes. Com isto, o processamento 
do pagamento de bolsas será feito por menos pessoas em menos dias. 
Com a finalização do sistema de bolsas a gestão financeira deverá agilizar as 
concessões e reduzirá as inconsistências, duplicações e equívocos no pagamento 
de bolsas.
O SIAFI já apresenta algumas evoluções que poderão ser implementadas 
nas próximas versões da solução, como a nova forma de integração através da API 
Integra Siafi - que até o momento pode Registrar e Consultar Documentos Hábeis.
Uma futura solução para a melhoria da integração com o SIAFI sem necessitar 
de arquivos, será efetuar o pagamento de credores por meio de serviços web como 
por exemplo APIs. página
286
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 281-286, junho de 2019
Capítulo XLV - Uma solução de automatização no processo de pagamento de bolsas de auxílioReferências
Camargo, Liriane S. de Araújo e Fazani, Alex Jose (2014) “Explorando o Design 
Participativo como Prática de Desenvolvimento de Sistemas de Informação” Revista 
de Ciência da Informação e Documentação. Ribeirão Preto, São Paulo.
Chandio, Aftab Ahmed, Zhu, Dingju e Sodhro, Ali Hassan. (2012) “Integration of 
Inter-Connectivity of Information System (i3) using Web Services” . Lecture Notes in 
Engineering and Computer Science, Vol. 2195(1).
STN , História.Disponível em: <http://www.tesouro.fazenda.gov.br/historia>. Acesso 
em: 26 fev. 2019.
MOTA, Francisco Glauber Lima. Contabilidade Aplicada à Administração Pública. 6.ª 
ed., Brasília: Vestcon, 2005.
MULLER ,M. J. A. (2002) Participatory design: the third space in HCI, 2002;
LOPES, Alessandra Ávila Lins. A Evolução do SIAFI Enquanto Sistema de Controle Interno 
do Governo Federal. Revista Científica Multidisciplinar Núcleo do Conhecimento. Ano 
03, Ed. 07, Vol. 04, pp. 40-50, Julho de 2018. ISSN:2448-0959
Desenvolvimento do Sistema de Inscrições Online para Programa de Assistência 
Estudantil-SIPAE.<https://eventos.unila.edu.br/wticifes2018/wp-content/
uploads/2018/06/98120.pdf>página
287
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 287-292, junho de 2019
Capítulo XLVI - Universidade Digital: Gerenciamento de Ordem de ServiçoUniversidade Digital: Gerenciamento de Ordem de 
Serviço
Marcos J. Ferreira Neto¹, Diogo C. Silva¹, Ítalo C. L. Silva¹, Rômulo N. Oliveira¹
¹Núcleo de Tecnologia da Informação – Universidade Federal do Alagoas (UFAL)
Av. Manoel Severino Barbosa, Bom Sucesso, CEP:57309-005, Arapiraca - AL - Brasil
{marcos.neto,italocarlo,romulo}@nti.ufal.br, diogo.silva@arapiraca.ufal.br
Resumo
Desde a abertura do Campus Arapiraca da UFAL em 2006, sua infraestrutura está em expansão. Com isso o processo 
de troca de informações e execução de serviços de infraestrutura têm ficado cada vez mais complexo, dificultando 
o seu gerenciamento na COINFRA. Tal setor utiliza tecnologias não integradas e não oficialmente adotadas pelo 
NTI que podem sofrer por falta de segurança e confiabilidade, tornando o trabalho mais redundante. Diante disso, 
este trabalho apresenta um módulo para o gerenciamento das solicitações de serviço realizadas à COINFRA dentro 
do Campus Arapiraca, com o objetivo de automatizar o processo de gerenciamento de solicitações e a comunicação 
entre as partes envolvidas. 
Palavras-chave: Ordem de serviço. Sistema web. Sistema integrado ERP.
1. Introdução
A crescente acessibilidade aos dispositivos computacionais, como: 
computadores, tablets e  smartphones, vem transformando o cenário da gestão 
pública ao longo dos anos (CHAGAS et al, 2017), tornando a adoção de sistemas 
automatizados de gestão cada vez mais imprescindível. Sistemas ERP (Enterprise 
Resource Planning) são soluções estruturadas, criadas para organizar e otimizar 
a cadeia de valor de uma organização, sendo focadas no apoio as tarefas 
administrativas e podem trazer diversos benefícios, dentre eles: agilidade nos 
processos burocráticos, centralização e reutilização de informações e maior 
transparência (CHAGAS et al, 2017).
A medida que a tecnologia avança, os funcionários, tanto do setor público 
como do privado, tendem a adotar por si só o uso de ferramentas de Tecnologia 
da Informação (TI) que possam ajudar de alguma forma o seu trabalho. KOPPER 
e WESTNER (2016) definem sistemas, serviços e processos que não são parte da 
TI corporativa “oficial” como Shadow IT. Segundo KOPPER e WESTNER (2016) a 
Shadow IT pode trazer consequências nos níveis técnicos e organizacionais como:
• Perda de controle, pois os sistemas usados operam fora das estruturas pré-
definidas pela organização;
• Ineficiência e redundância de processos, resultado da perda de potenciais 
sinergias entre diferentes setores;página
288
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 287-292, junho de 2019
Capítulo XLVI - Universidade Digital: Gerenciamento de Ordem de Serviço• Conflitos de recursos, causados pela ineficiência das funcionalidades 
disponíveis na TI oficial;
• Riscos à segurança, devido à falta de conhecimento sobre segurança e 
vulnerabilidade digital por parte do usuário;
• Inconsistência de dados, gerada pela descentralização das informações;
• Problemas na integração de aplicações, ao tentar integrar Shadow IT com a 
TI oficial podem haver falhas de segurança e privacidade como consequência 
do processo.
Na UFAL Campus Arapiraca existem muitas demandas em setores 
administrativos ainda não atendidas pela tecnologia institucional. Como 
consequência, o Núcleo de Tecnologia da Informação (NTI) desenvolve desde 
2013 um projeto de extensão com o objetivo de realizar uma melhor integração 
nas tarefas administrativas e acadêmicas na universidade (PINHEIRO, 2015). 
Como produto do projeto de extensão, módulos de um sistema ERP estão em 
desenvolvimento para atender essas necessidades. Alguns módulos já foram 
implantados, tais como: Repositório Institucional, Gerenciamento de Espaço Físico, 
Repositório de Portarias e Monitoria, que fazem parte do ERP Universidade Digital 
(UD) e já estão em uso na UFAL Campus Arapiraca. Porém faltava um módulo para 
a Coordenadoria de Infraestrutura  (COINFRA) gerenciar a solicitação e realizar o 
acompanhamento de ordens de serviços (OS).
Os processos de solicitações de serviços usados pela COINFRA fazem uso de 
algumas ferramentas que podem ser classificadas como Shadow IT . O uso dessas 
ferramentas é um risco à segurança da informação, podendo causar inconsistência 
(a classificação das informações é praticamente inexistente), perda de dados 
(nos casos em que o serviço de e-mail falha), redundância de informações (para 
que o processo continue, o conteúdo do e-mail precisa ser copiado para outra 
aplicação). Além dos problemas identificados anteriormente, o uso de tais 
tecnologias prejudica o rendimento do setor. Segundo SANTOS (2017, p. 10), “o 
nível de dificuldade na execução das tarefas realizadas pelos funcionários e a 
redução dos gastos da instituição podem sofrer melhoras significativas” ao obter 
sucesso na implantação de um módulo de sistema ERP . Um bom exemplo disso 
é que o gestor da COINFRA não precisaria mais copiar, classificar e identificar 
redundâncias manualmente, além do fato de que a busca de solicitações antigas 
pode ser reduzida a poucos segundos.
2. Métodos
 Para a construção desse módulo foi utilizado o modelo de ciclo de vida 
iterativo e incremental, que possui como principais etapas: análise, projeto, 
implementação e testes (BEZERRA, 2017). Para atender essas etapas, foi necessário: 
1. Realizar reuniões com o gestor do departamento para identificar e 
documentar requisitos funcionais e não funcionais;página
289
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 287-292, junho de 2019
Capítulo XLVI - Universidade Digital: Gerenciamento de Ordem de Serviço2. Modelar uma solução nos moldes da Unified Modeling Language (UML) 
(diagramas de classes, casos de uso) para o bom entendimento durante o 
processo de desenvolvimento (BEZERRA, 2017);
3. Desenvolver o módulo de acordo com a documentação gerada 
anteriormente;
4. Testar o módulo desenvolvido para identificar e corrigir erros e 
funcionamento inadequado.
Para o desenvolvimentos do UD e de seus módulos foi utilizado o Odoo, 
também conhecido como OpenERP . Ele foi escolhido por ser um sistema ERP de 
código aberto (ODOO, 2019) mundialmente utilizado, desenvolvido e mantido 
pela Odoo S.A. (ODOO SA, 2019), empresa responsável pelo desenvolvimento da 
versão Enterprise e pela governança do repositório de código aberto.
O desenvolvimento de aplicativos para o ERP Odoo segue o padrão MVC 
– Model-View-Controller. Tal ERP utiliza um cliente Javascript que executa no 
navegador de forma a padronizar a comunicação entre o cliente e o servidor. 
Esse servidor, escrito em Python, se comunica com o banco de dados PostgreSQL 
usando sua própria biblioteca ORM – (Object Relational Mapper). Cada módulo tem: 
modelos, visões, permissões de usuário, entre outros recursos que proporcionam 
a modularidade.
3. O módulo Solicitação de Serviço
O desenvolvimento do módulo Solicitação de Serviços buscou facilitar o 
gerenciamento e controle do serviços referentes à infraestrutura do campus. 
O módulo permite que qualquer participante da solicitação envie mensagens 
contendo fotos, vídeos ou links.  Durante todo o processo, mensagens são 
enviadas para os participantes a cada mudança de status na solicitação. E todos 
os envolvidos podem visualizar o andamento das OS por meio dos seus estados: 
“Solicitação enviada” , “Análise” , “Encaminhado p/ Execução” , “Em Execução” , 
“Finalizada” , “Cancelada” .
Esse módulo possui três perfis, são eles: “Solicitante” , que pode apenas 
visualizar solicitações criadas por si e não pode alterar a solicitação após a 
visualização do gerente; “Responsável por serviço” , que possui as mesmas 
permissões do solicitante, porém pode acessar e alterar as solicitações atribuídas a 
si para análise e execução; “Gerente de serviço” , que possui as mesmas permissões 
do responsável por serviço, porém pode acessar qualquer solicitação e executar 
quaisquer ações permitidas no módulo. Conforme está representada na Figura 1, 
é possível visualizar as funcionalidades correspondentes aos perfis disponíveis no 
módulo. 
Anteriormente era necessário que o gerente do setor acessasse o e-mail 
e analisasse cada solicitação, separando as válidas e que de fato parecessem 
autênticas, uma vez que o formulário de envio era aberto e não exigia autenticação página
290
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 287-292, junho de 2019
Capítulo XLVI - Universidade Digital: Gerenciamento de Ordem de Serviçoprévia. Já no novo processo, a autenticação da solicitação é feita por meio do 
próprio sistema, uma vez que é necessário que o usuário já possua cadastro e 
tenha seus dados de contato registrados no sistema. 
Na Figura 2 é mostrado o formulário de cadastro para uma nova solicitação 
de serviço. Observando essa figura, nota-se que o módulo facilita o processo 
de triagem, pois ao selecionar um valor no campo “Manutenção” , abre-se outro 
campo com as opções previamente cadastradas para o valor selecionado. E, além 
de evitar erros de digitação e possíveis “trotes” , permite que o responsável pela 
análise do serviço seja adicionado à solicitação e receba uma notificação por 
e-mail referente a um novo serviço a ser analisado.  
As solicitações são atendidas geralmente por ordem de chegada, porém 
o gerente  pode alterar a ordem de acordo com a prioridade definida por ele. O 
gerente também é responsável pela atribuição da OS aos executores dos serviços.
As telas do sistema são ajustáveis aos diferentes tamanhos de telas de exibição, 
sendo possível facilmente estendê-lo para um aplicativo de smartphone usando a 
API do Odoo para aplicativos móveis.
Figura 1: Diagrama de casos de uso do módulo Solicitação de Serviçospágina
291
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 287-292, junho de 2019
Capítulo XLVI - Universidade Digital: Gerenciamento de Ordem de Serviço
Figura 2: Formulário de cadastro para nova solicitação de serviço
4. Resultados e Conclusão
Esse trabalho objetivou o desenvolvimento do módulo de gerenciamento de 
solicitações de ordens de serviço para o sistema ERP Universidade Digital, já em 
uso pela UFAL Campus Arapiraca. O módulo tem como principal função digitalizar, 
integrar e agilizar o processo de solicitações de ordens de serviço na COINFRA, 
criando processos genéricos e adaptáveis, para que possam ser utilizados em 
outros setores da mesma instituição ou em outras, já que o código é aberto.
Como um objetivo periférico, o módulo desenvolvido é o mais extensível e 
adaptável possível, abrindo espaços para fáceis alterações nos fluxos de trabalhos 
e, ainda, possíveis evoluções para uma melhor cobertura e integração nas 
atividades do setor atendido e de outros setores que porventura venham utilizar 
o sistema.
Vale ressaltar que, com a utilização do módulo Solicitação de Serviços, os 
problemas presentes no processo de solicitação de serviço utilizado anteriormente 
foram resolvidos, melhorando o controle e a eficiência do processo como também 
a segurança e consistência dos dados. Por isso, durante a apresentação do módulo, 
ele foi aceito pelos funcionários do setor. 
O processo de testes utilizado consistiu em criar casos de teste manuais, 
de forma a cobrir todo o fluxo de dados do sistema. Como trabalhos futuros 
é possível citar o uso do módulo por alguns solicitantes, buscando identificar 
possíveis limitações e obter sugestões de melhorias antes de diponibilizar para a 
comunidade acadêmica.página
292
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 287-292, junho de 2019
Capítulo XLVI - Universidade Digital: Gerenciamento de Ordem de ServiçoReferências
ARAUJO, M. S. et al. Universidade Digital: Um sistema ERP auxiliar para instituições 
públicas. (2015) In XIII Workshop de Trabalhos de Iniciação Científica e Graduação 
da Escola Regional de Computação Bahia - Alagoas - Sergipe - 2015 - Salvador, BA, 
Brazil. 
BEZERRA, E. (2017). Princípios de Análise e Projeto de Sistema com UML. Rio de 
Janeiro: Elsevier Brasil.
CHAGAS, V., Siqueira, E., and Sun, V. (2017) Uso da Análise Fatorial para geração de 
Índice de Maturidade em Governança de TI no Governo do Estado de São Paulo. In 
CONF-IRM, page 40. 
KOPPER, A. and WESTNER, M.  (2016) Deriving a framework for causes, consequences, 
and governance of shadow it from literature. MKWI 2016 Proceedings, pages. 1687-
1698. 
ODOO. (2019) ERP e CRM de código aberto | Odoo. Disponível em: <https://www.odoo.
com/pt_BR/>. Acesso em: 21 fev 2019.
ODOO SA. (2019) Sobre nós | Odoo. Odoo S.A. Disponível em: <https://www.odoo.com/
pt_BR/page/about-us>. Acesso em: 21 fev 2019.
PINHEIRO, F. R. (2015) UNIVERSIDADE DIGITAL: Gerenciamento de Espaço Físico.  
Disponível em: <http://ud10.arapiraca.ufal.br/repositorio/publicacoes/86>. Acesso 
em: 20 fev 2019.página
293
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 293-298, junho de 2019
Capítulo XLVII - Universidade Digital: preservando e disponibilizando a produção científica 
através do Repositório InstitucionalUniversidade Digital: preservando e 
disponibilizando a produção científica através 
do Repositório Institucional
Italo Silva¹, Diogo Cabral¹, Marcos Neto¹, Rômulo Nunes¹
¹Núcleo de Tecnologia da Informação -- Universidade Federal de Alagoas
  (UFAL) campus Arapiraca
{italocarlo,marcos.neto,romulo}@nti.ufal.br, diogo.silva@arapiraca.ufal.br
Resumo
No âmbito da ciência, as instituições públicas de ensino produzem a cada ano uma quantidade relevante de material 
científico. Além de produzir, elas precisavam disponibilizar este material para a comunidade acadêmica. Neste 
sentido, surgiu o Repositório Institucional (RI) com o propósito de capturar, armazenar, disponibilizar e preservar 
a memória institucional. No caso da UFAL, campus Arapiraca, não foi diferente. À medida que a produção científica 
aumentava, a necessidade por uma solução para gerir o conhecimento produzido ficava mais evidente. Portanto, 
o objetivo deste trabalho é apresentar o desenvolvimento e funcionamento do módulo de Repositório Institucional 
integrado ao projeto Universidade Digital. Os dados obtidos sobre o RI até o momento, com 2654 publicações e quase 
30 mil visualizações mostram a importância da adoção de uma solução como esta em uma instituição pública. 
Assim, podemos destacar como benefícios obtidos com a implantação da solução: disponibilidade e publicidade do 
acervo e uma maior visibilidade para autores.
Palavras-chave: Repositório Institucional, Universidade Digital.
1. Introdução
No âmbito da ciência, as instituições públicas de ensino produzem, a cada 
ano, uma quantidade relevante de material, seja através de trabalhos de conclusão 
de curso, teses, dissertações, artigos entre outros. A princípio, o material produzido 
deveria estar catalogado e armazenado fisicamente na biblioteca da instituição. 
Assim, sempre que alguém necessitasse realizar uma consulta ao acervo deveria 
deslocar-se a biblioteca para realizar a consulta, respeitando os horários de 
funcionamento e a disponibilidade do material. Além disto, alguns aspectos 
corroboravam para que parte da produção não estivesse disponível, dentre eles 
podemos destacar: morosidade dos processos de publicação, os custos envolvidos 
na publicação de documentos e a grande quantidade de documentos impressos. 
Neste sentido, estratégias precisavam ser adotadas no intuito de preservar, 
publicar e divulgar todo o material produzido que remete à memória científica da 
instituição, para que o fluxo de conhecimento e aprendizagem não se perdesse 
no decorrer da vida das organizações [Vianna and Carvalho 2013]. A evolução da 
Tecnologia da Informação (TI) permitiu a criação do repositório institucional (RI), 
fazendo com que as barreiras físicas da biblioteca fossem superadas. 
Os RIs são coleções que capturam e preservam a produção intelectual de 
uma ou mais universidades ou comunidades[Tomael and Silva 2007] ou ainda página
294
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 293-298, junho de 2019
Capítulo XLVII - Universidade Digital: preservando e disponibilizando a produção científica 
através do Repositório Institucionalum conjunto de serviços que uma universidade oferece. O RI caracteriza-se pelo 
fato de ser orientado para a informação produzida no ambiente das instituições, 
sendo desenvolvido, implementado e mantido por elas. Sua concepção está 
intimamente relacionada aos conceitos de aberto (open access), arquivos abertos 
(open archives) e software livres.
No caso da Universidade Federal de Alagoas, campus Arapiraca, não foi 
diferente. À medida que o número de publicações aumentava, a necessidade 
de uma gestão mais eficiente do conteúdo produzido ficava mais evidente. 
Atualmente, o campus oferta 23 cursos distribuídos em quatro Unidades de Ensino, 
entre cursos de bacharelado e licenciatura na modalidade presencial e dois cursos 
de Pós-Graduação Strictu Sensu. Diante desta realidade, a gestão da biblioteca 
do campus demandou a necessidade de desenvolver uma solução que além de 
mitigar os problemas relatados anteriormente, resolvesse um outro que também 
se fazia presente no cotidiano da biblioteca que seria a restrição de espaço físico 
para armazenamento das produções científicas.
Assim, o objetivo deste trabalho é apresentar o desenvolvimento e 
funcionamento do módulo de Repositório Institucional, que está integrado ao 
projeto Universidade Digital (UD) [Araujo et al. 2015]. O UD tem como propósito 
construir um sistema ERP para atender demandas específicas das Instituições de 
Ensino Superior (IES) e distribuir esta solução dentro da licença de software livre. 
Dentre os módulos já contemplados, podemos destacar: Gestão de Espaços Físicos 
[Pinheiro 2015], Transportes, Portarias, Monitoria [Silva et al. 2018]. 
2. Métodos
Nesta seção especificamos os métodos utilizados para desenvolvimento da 
solução e sua respectiva arquitetura. 
2.1 Processo de Desenvolvimento
O processo de desenvolvimento usou o modelo aplicado no trabalho de 
Silva et al. [Silva et al. 2018] e visível na Figura 1. Ela seguiu o seguinte fluxo: uma 
vez que os requisitos foram validados junto ao cliente (1,2,3,4), a documentação 
foi elaborada (5) e a codificação da solução (6) foi realizada. Por fim, o teste de 
aceitação (7,8) foi aplicado, seguido da entrega ao cliente (9) e o produto finalmente 
entra em produção.
2.2 Desenvolvimento
Para realizar a integração do novo módulo ao UD, o desenvolvimento da 
solução foi feito utilizando o framework Odoo1, versão 10.0. Ele é um software de 
código aberto, que está sob a licença AGPLv3 (Affero General Public License, versão 
3), e utiliza o Python, como a linguagem de programação na versão 2.7.
1 https://www.odoo.com/página
295
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 293-298, junho de 2019
Capítulo XLVII - Universidade Digital: preservando e disponibilizando a produção científica 
através do Repositório InstitucionalComo adicional, um portal público2 para busca das publicações foi construído, 
com uma interface amigável e responsiva, em destaque na Figura 1.
Figura 1. Visão Geral do Processo
Ele permite ao usuário realizar buscas por publicações através da aplicação de 
filtros específicos como por exemplo: título, tipo, autor, orientador, coorientador e 
palavra-chave. Uma vez que a publicação foi encontrada o usuário poderá acessar 
o trabalho e fazer o download da publicação.
Figura 2. Tela para busca de publicações
2 http://ud10.arapiraca.ufal.br/repositorio/página
296
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 293-298, junho de 2019
Capítulo XLVII - Universidade Digital: preservando e disponibilizando a produção científica 
através do Repositório Institucional2.3 Arquitetura
A arquitetura da solução é composta por três camadas, como podemos ver 
na Figura 3, com suporte a vários usuários, através do balanceamento de carga3. A 
camada do banco de dados para armazenamento dos dados, a camada de aplicação 
para processamento e funcionalidades e, por fim, a camada de apresentação 
para prover a interface ao usuário. Dentre essas podemos destacar a camada de 
aplicação, que pode ser considerada como o núcleo, onde módulos adicionais 
podem ser instalados, permitindo criar uma instância particular do Odoo. 
Além disto, os módulos seguem o padrão arquitetural Model-View-Controller  
(MVC). Tal padrão resolve o problema de desacoplar o acesso a dados e regra de 
negócio da apresentação dos dados e interação dos usuários, através da introdução 
de um componente intermediário: o controller. 
Figura 3. Arquitetura da Solução
3. Resultados
Até o presente momento, o RI conta com um acervo de 2654 trabalhos. A 
Figura 4 (a) exibe estes trabalhos distribuídos entre os cursos de graduação, 
especialização e mestrado, dentre os quais podemos destacar: Ciências Biológicas, 
Serviço Social, Administração e Enfermagem com mais de 200 títulos cadastrados 
cada um. Além disto, as consultas ao acervo, desde a sua implantação, já chegam 
a quase 30 mil visualizações. A Figura 4 (b) traz estas consultas segmentadas por 
curso, dando ênfase ao curso de Arquitetura com 4812 visualizações, seguido de 
Educação Física com 2830, Ciências Biológicas com 2712 e Administração com 
2210. 
3 Para atender o requisito de qualidade Escalabilidade, que remete à capacidade de acomodar novas 
demandas e/ou manipular uma porção crescente de trabalho com o acréscimo de hardwarepágina
297
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 293-298, junho de 2019
Capítulo XLVII - Universidade Digital: preservando e disponibilizando a produção científica 
através do Repositório Institucional
Figura 4. Resultados
Os benefícios da adoção e utilização do RI dentro da instituição são cada vez 
mais perceptíveis. Além de instrumento de preservação da memória institucional, 
ele garante uma maior disponibilidade de consulta ao acervo, inclusive aos finais 
de semana, sem que necessite o deslocamento até a biblioteca. 
A solução também garante uma maior facilidade para busca dentro do acervo 
através da aplicação de filtros específicos como: autor, palavra-chave, orientador, 
curso, ano e tipo. As cópias dos dados que são realizadas periodicamente, 
garantem uma continuidade na prestação dos serviços em caso de problemas na 
infraestrutura. E por fim, os autores das publicações também são beneficiados, 
uma vez que seus trabalhos ganham uma maior visibilidade não só no âmbito da 
instituição, mas também fora dela.
4. Conclusão
O presente trabalho apresentou o desenvolvimento e implantação de uma 
solução para preservação e publicização da memória institucional. Os números 
apresentados na Seção 4 apontam uma boa quantidade de trabalhos dentro 
do acervo, os quais estão distribuídos entre os vários cursos disponíveis. Além 
disto, a comunidade acadêmica tem despertado o interesse em ver o que se tem 
produzido dentro da instituição. Contudo, estes números ainda podem e devem 
ser melhorados. Uma estratégia seria divulgar nos meios disponíveis tais como: 
rede sociais, e-mails, site institucional.  
Como uma ferramenta em construção, a solução passa por uma avaliação 
constante por meio do feedback dos servidores da biblioteca ou outros membros 
da comunidade acadêmica. Sendo um projeto Open Source e com o propósito de 
atrair mais colaboradores, optou-se por disponibilizá-lo no repositório do GitHub4 
para que  outras instituições possam utilizar, colaborar seja através de sugestões 
de melhorias, inclusão de novos recursos ou até mesmo relatar bugs. 
4 https://github.com/ntiufalara/universidade-digitalpágina
298
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 293-298, junho de 2019
Capítulo XLVII - Universidade Digital: preservando e disponibilizando a produção científica 
através do Repositório InstitucionalE por fim, como trabalhos futuros podemos propor uma melhoria para 
geração automática de citações e bibliografia, incluindo o padrão ABNT e o modelo 
de cadastro para o LateX5. Outra possibilidade seria a inclusão de mecanismos que 
permitissem avaliar a qualidade da produção científica. 
Referências
Araujo, M. S., Cabral, D., Kellen, Y., and SILVA, I. C. L. (2015). Um sistema ERP auxiliar para 
instituições públicas. XV Escola Regional de Computação Bahia - ERBASE,(9):120–127.
Pinheiro, F. R. (2015). Universidade Digital: gerenciamento de espaço físico. Monografia 
(Bacharel em Ciência da Computação), UFAL (Universidade Federal de Alagoas), 
Arapiraca, Brazil.
Silva, I., Oliveira, C., Neto, M., and Oliveira, R. (2018). Universidade Digital: 
descomplicando o gerenciamento da monitoria e tutoria. XII Workshop de Tecnologia 
da Informação e Comunicação das Instituições Federais de Ensino Superior do Brasil, 
WTICIFES.
Tomael, M. and Silva, E. (2007). INSTITUCIONAL REPOSITORIES: guidelines for 
information policies. VIII ENANCIB – Encontro Nacional de Pesquisa em Ciência da 
Informação. 
Vianna, S. and Carvalho, R. (2013). Benefícios da implantação de repositório 
institucional na preservação da memória institucional. XXV Congresso Brasileiro de 
Biblioteconomia, Documentação e Ciência da Informação.
5 https://www.latex-project.org/página
299
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 299-304, junho de 2019
Capítulo XLVIII - Utilização da Central de Ajuda para a Tecnologia da Informação: Estratégias 
para Auxílio ao UsuárioUtilização da Central de Ajuda para a 
Tecnologia da Informação: Estratégias para 
Auxílio ao Usuário
Paulo Freire Sobrinho
Universidade Federal da Grande Dourados (UFGD) 
Caixa Postal 364 – CEP: 79.804-970 – Dourados – MS – Brasil
paulosobrinho@ufgd.edu.br
Abstract
The purpose of this research is the use of methods to find through a centralized mechanism in which to accelerate 
through IT, with the purpose of improving user service support. The main interest is evidence of how the Central de 
Ajuda could be better and more efficient compared to other resources already used to streamline customer service. 
Through the results it was verified the strategies and standardization in the Articles will contribute to the quality 
of care involving IT. It was concluded that there was a reduction of human and time resources, to help through the 
centralized information mechanism reused by users who needed support.
Keywords: Articles, Help Center and Information Technology.
Resumo
Objetivo desta pesquisa é o uso de métodos relativo a encontrar através de mecanismo centralizado em que agilize por 
meio da TI, com o propósito de melhorar o suporte de atendimento ao usuário. O principal interesse é comprovação 
de que maneira a Central de Ajuda poderia ser melhor e eficiente, comparado a outros recursos já utilizados 
para o fluxo do atendimento aos usuários. Através dos resultados constatou-se as estratégias e padronizações nos 
Artigos contribuirão na qualidade dos atendimentos envolvendo a TI. Concluiu-se que houve a redução de recursos 
humanos e de  tempo, para auxílio através do mecanismo de informações centralizadas reutilizados pelos usuários 
que necessitaram de suportes.
Palavras-chave: Artigos, Central de Ajuda e Tecnologia da Informação.
1. Introdução
Segundo Foina (2009), a Tecnologia de Informação (TI) é um “conjunto de 
tecnologias, metodologias e procedimentos que atuam em coleta, tratamento e 
disseminação das informações na organização” [03]. De maneira geral, a TI pode 
ser definida como um conjunto de atividades e soluções obtidas por meio dos 
recursos computacionais, as quais são aplicadas em diversas áreas de atuação, 
pois existe uma grande necessidade de conseguir conceituá-la por completo, 
tendo em vista a sua grande magnitude e expansão usual.
A TI deve auxiliar as organizações a atuarem na sociedade que convive com 
difusões de informações variadas, devido ao desenvolvimento das tecnologias de 
informação e de comunicação para resolver e acelerar resultados [01]. No entanto, 
existem dificuldades mediante ao fornecimento do devido suporte, por meio 
dos serviços especializados e eficientes através dos mecanismos quê facilitem e página
300
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 299-304, junho de 2019
Capítulo XLVIII - Utilização da Central de Ajuda para a Tecnologia da Informação: Estratégias 
para Auxílio ao Usuárioaprimorem o fluxo destas informações, bem como na ampliação de competências 
daquelas que já sabem como aplicá-la.
Baseado nesta problemática, esta pesquisa propõe o uso de métodos através 
de  uma ferramenta para a apresentação e formalização de conhecimentos em TI 
para produzir ou descrever, por meio de desenvolvimento de padrões e soluções 
para auxiliar o seu uso para usuários de organizações. Pois o objetivo é relativo 
a encontrar maneiras de aumentar, aperfeiçoar e garantir melhor qualidade das 
informações através de mecanismo centralizado em que agilize por meio da TI, 
para oferecer redução de tempo, aumento de confiabilidade, segurança e on-line 
de fácil acesso com o propósito de auxiliar os usuários do seu uso, especificamente 
para suporte de atendimento a partir do direcionamento adequado, através de 
informações úteis e essenciais para a utilização eficaz dos sistemas e serviços para 
as grandes organizações [01]. 
Neste contexto para conseguir soluções que atendam a estes fatores e outros, 
torna-se necessário encontrar maneiras através de vários estudos a partir da 
utilização de conteúdos centralizados on-line, por meio da ferramenta chamada de 
Central de Ajuda, a qual foi desenvolvida pela Coordenadoria de Desenvolvimento 
de Tecnologia da Informação (COIN), da Universidade Federal da Grande Dourados 
(UFGD).
Por isso, a proposta deste artigo é de apresentar determinada solução 
utilizada para que o conjunto de serviços e sistemas desenvolvidos os quais são 
utilizados pela Universidade, com o intuito de melhorar a eficiência e o uso dos 
recursos tecnológicos disponíveis para que os usuários façam uso da TI, existente 
dentro desta organização. 
2. Métodos e Materiais
Quanto à abordagem utilizada nesta pesquisa é qualitativa, a partir da 
investigação empírica, através de critérios subjetivos1, dentro de uma Universidade 
durante o período de 2 (dois) anos entre 2013 e 2014, no qual envolve a TI e os 
usuários com a utilização da Central de Ajuda. Pois o principal interesse seria o 
questionamento mediante a comprovação de que maneira a Central de Ajuda 
poderia ser melhor e tanto quanto mais eficiente comparado a outros recursos 
já utilizados até aquele momento, por exemplo, manuais em formato PDF/DOC, 
instruções na página da Universidade e etc. 
O fato de todas as informações ser encontradas em ambiente centralizado 
com ótima visualização e melhor manutenção do material, acessível de maneria 
rápida e simples por qualquer usuário tanto, interno ou externo da Universidade 
que deseje consultar e buscar informações de como usar algum serviço ou 
sistema da Central de Ajuda através do link http://ajuda.ufgd.edu.br/, junto com 
determinada Área Administrativa acessado pelo link http://ajuda.ufgd.edu.br/
1 *Subjetivos, neste caso, a subjetividade ocorre devido a enorme quantidade de informações em múltiplos 
registros de características e tecnologias heterogênicas, os quais poderiam necessitar, possivelmente, das 
autorizações formais para poderem ser analisadas e posteriormente, não haveria tempo hábil para ser 
ajustadas e tabuladas.página
301
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 299-304, junho de 2019
Capítulo XLVIII - Utilização da Central de Ajuda para a Tecnologia da Informação: Estratégias 
para Auxílio ao Usuárioadmin os quais todos os conteúdos do site são hospedados com segurança e 
estabilidade em único Servidor de Internet. Para realização da manutenção de 
conteúdo, por exemplo, adicionar, alterar ou remover conteúdo através Ambiente 
Administrativo (precisa ser administrador) que possui um editor de texto simples e 
usual, de modo que o seu conteúdo é codificado para Hypertext Markup Language 
(HTML) automaticamente, pela Central Ajuda.
Para melhor apresentação dos artigos, houve a necessidade de padronizar 
os conteúdos presentes na Central Ajuda UFGD, sendo que o uso do ambiente de 
Máquina Virtual utilizado especificamente para um Sistema Operacional, a fim de 
ser usado como protótipo para a extração prática do material, por exemplo, criação, 
descrição, obtenção de imagens e demais recursos importantes com propósito da 
geração elaboração dos Artigos2. Pois a partir desta fase é então, padronizados 
todos os conteúdos dos materiais para auxiliar de maneira ágil tanto, o usuário e o 
atendente, através da formatação do texto (fontes, tamanhos, cor, etc.), tipos das 
imagens, descrição de numeração dos itens e passos.
Ainda sobre a Central de Ajuda, no qual possui em sua estrutura, determinadas 
subdivisões entre Serviços e Sistemas, tornando assim, simplificado para 
obtenção da informação com a finalidade de organizar internamente, todos os 
seus conteúdos. Os Serviços e Sistemas podem ser comparados ao Catálogo de 
Serviços [02], conforme explicado a seguir:
• Os Serviços possuem maior quantidade, por isso, torna-se mais adequado 
para várias situações e soluções, pois são referentes aos aplicativos 
externos, equipamentos, rede de computadores, etc. 
• Os Sistemas são referentes a todos os aplicativos internos, os quais foram 
desenvolvidos pela instituição, pois nesta subdivisão tem a finalidade 
de facilitar e orientar o usuário sobre o funcionamento e a utilização do 
mesmo, no qual houve dificuldade de categorizar qual o procedimento 
deve ser aplicado, pois o seu acesso é limitado, devido ao envolvimento 
de informações sigilosas e derivadas das mudanças e melhoramentos 
contínuos, os quais são comuns em sistemas em organizações de grande 
porte [01], pois neste caso, a Universidade pesquisada compreendia que 
muitas informações desta área poderiam ser esclarecidas diretamente, 
com o setor externo responsável pelo gerenciamento da manipulação 
destas informações.
A Central de Ajuda através desta sua estrutura de subdivisões comprovou 
a sua  eficiência de modo para conduzir o usuário, pois a organização destas 
informações encontra-se de maneira objetiva, ágil e segura.
Foram desenvolvidas estratégias após cada um dos artigos ser preparados em 
conjunto com o solicitante, desse modo havia então, a necessidade da divulgação 
interna por e-mails no setor de TI e caso, não surgisse novas recomendações de 
alterações, posteriormente, era comunicado a todos. Esta fase de divulgação, 
2 *Artigos, neste caso, a palavra é exatamente, escrita no plural que é referente a todos os textos presentes na 
Central Ajuda, os quais estão dentro das subdivisões, Serviços e Sistemas.página
302
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 299-304, junho de 2019
Capítulo XLVIII - Utilização da Central de Ajuda para a Tecnologia da Informação: Estratégias 
para Auxílio ao Usuárioconcretizava os artigos, após ser enviado por e-mails, para no mínimo, um usuário 
para cada um dos setores existentes dentro da Universidade. Ainda, sobre o critério, 
por meio do, envio de e-mail para cada usuário por setor havia sido mapeados 
o que possuíam melhor interesse ou habilidade em TI e os tornando assim, em 
stakeholders.
Com isso, foi realizado outro mapeamento, no qual descobriu-se que a 
maioria dos usuários acompanhavam o conteúdo e sabia que aqueles Artigos 
mais recentes divulgados continham informações que poderia ser úteis durante 
aquele período. No geral, os usuários da Universidade acreditavam que os Artigos 
já continha todas as orientações sobre o processo interno do setor de TI e inclusive 
as soluções.
Na maioria casos, o atendente conseguia diminuir o tempo de instruções por 
telefone e conforme a situação, apenas o fato de ser explicar exatamente, qual 
entre os Artigos solucionaria a situação, já concluía o atendimento. Também foram 
testados outras medidas, por exemplo: 1) após ser orientado o item, na Central de 
Ajuda que o usuário deveria seguir, sendo informado que poderia tentar executar 
as instruções; 2) caso, não conseguisse, o usuário poderia retornar a ligação (este 
fato contribuiu diretamente, pois normalmente o usuário conseguia concluir 
sozinho); 3) e caso, não tivesse solucionado, seria então, refeitos os procedimentos 
com o usuário, com isso, aumentando as chances do problema ser solucionado.
Outro fato relevante, foi que após a Central de Ajuda produzir excelentes 
resultados, confiabilidade e estabilidade, onde percebeu-se que houve demandas 
maiores para a elaboração de Artigos. Porém, por medidas de qualidade e 
divulgação do conteúdo, houve a necessidade de limitação de apenas um por 
semana em média, pois assim, seria possível garantir que o material obtivesse 
entendimento, consistência,  aceitação, memorização pelos usuários dos setores 
externos a TI.
3. Resultados
Neste sentido, percebeu-se que através do surgimento e uso da Central de 
Ajuda, tornou-se um forte aliado, quê, cada vez mais, é utilizado internamente 
para os Sistemas e Serviços existentes a área de TI. Pois os Artigos foram tratados 
em muitas situações como importantes documentações por todos os envolvidos, 
principalmente, que compõe os artefatos em sistemas legados e serviços antigos 
ou pouco utilizados recentemente. 
Para as diversas situações, de modo que existiam os Artigos relativos, aos 
quais os usuários solicitavam suporte tanto, por telefone, e-mail ou pessoalmente, 
e eram direcionados pelo atendente a acessar o link https://ajuda.ufgd.edu.br/  
no qual orientava até os itens relacionados com a solução sobre a determinada 
demanda. Com isto, tornando-se assim, objetivos todo estes processos e 
garantindo o melhoramento da qualidade no fluxo das solicitações baseados nos 
atendimentos.página
303
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 299-304, junho de 2019
Capítulo XLVIII - Utilização da Central de Ajuda para a Tecnologia da Informação: Estratégias 
para Auxílio ao UsuárioCom isto, observou-se que a Central de Ajuda contém excelente integração, 
baseado em funcionalidades e necessidades, principalmente para os usuários 
e profissionais em TI. Pois a partir de estudos e análises constatou-se que após 
definidas as padronizações nos Artigos e organizados a estrutura dos textos em 
instruções de como proceder, os quais contribuirão em muito para os atendimentos 
envolvendo a TI. E partir deste modelo de padronizações estruturas os quais 
diretamente, foram baseados no próprio fluxo de atendimento, pois o princípio 
seria quê, ao invés, de ser explicado para o usuário todos os passos que ele deveria 
proceder por telefone, e-mail ou pessoalmente, reduziria o tempo de atendimento 
e aumentaria a qualidade destas informações, e evitando conforme os Artigos de 
ser explicado novamente, para o usuário.
Devido ao grande potencial que a padronização dos Artigos contidos na 
Central de Ajuda, os quais tornaram-se úteis também como fontes de estudos 
pelos estagiários e técnicos ainda durante a fase de treinamento, através da 
familiarização com os Serviços e Sistemas, reduzindo com isto, custos de tempo 
e demais recursos operacionais que seriam necessários na capacitação. Pois, 
neste último requisito é relevante, devido os treinamentos consumiram muito 
tempo para qualquer organização [01], já que envolve a experiência no caso, de 
outro profissional sênior. Partindo deste contexto, os esforços aplicadas pelas 
organizações, muitas vezes, não possuem condições financeiras suficientes e prazos 
de projetos tecnológicos reduzidos, rotatividade de profissionais direcionados a 
suporte, infraestrutura e desenvolvimento com quantitativo mínimo,  comparando 
as demandas de serviços e produtos existentes.
Outro aspecto favorável sobre o uso Central de Ajuda está relacionado 
diretamente com a procura (localizado no botão “Clique para buscar”), pois possui 
enorme potencial, facilitando o acesso e uso do Artigos. No entanto, conforme 
relatado informalmente, por muitos usuários, os quais ainda utilizam raramente, 
este recurso de procura.
Portanto, os métodos de estudos mostraram adequados, de maneira 
que podem ser utilizados em Universidades, diversas outras organizações e 
principalmente, órgão públicos e ainda com a utilização da ferramenta em formato 
padronizado, no qual concretizou o fundamento desta pesquisa.
4. Conclusão
Conclui-se quê o objetivo desta pesquisa foi atingido, pois foi proposto e 
estudado determinada solução que pudesse auxiliar o usuário e contribuir para 
o atendimento perante o setor de TI poder realizar o suporte necessário quando 
solicitado de maneira centralizada. O estudo buscou fazer uma análise econômica, 
tanto em recursos humanos e a redução de tempo para atender diretamente a 
todos que necessitam ou estão envolvidos com a TI. 
O diferencial dos outros métodos e materiais já mencionados comparado a 
Central de Ajuda, consiste exatamente, em padronizações e atendimentos baseado 
em TI através do uso de site que pode ser utilizado como um mecanismo de página
304
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 299-304, junho de 2019
Capítulo XLVIII - Utilização da Central de Ajuda para a Tecnologia da Informação: Estratégias 
para Auxílio ao Usuárioinformações centralizadas em são codificadas para a integração de determinados 
repositórios para ser utilizados e/ou reutilizados pelos usuários que necessitaram 
de orientações de profissionais experientes para realização do suporte de como 
proceder diante do fluxo de informações.
Espera-se que a elaboração deste artigo possa contribuir para todos os 
profissionais envolvidos com a TI, no sentido de disseminar o uso de novos métodos 
e materiais, apresentando soluções práticas, viáveis, seguras, compatíveis e 
tornando assim, esse material em certa referência para os novos pesquisadores, 
interessados em manter as informações dos produtos e serviços em TI acessíveis 
de maneira prática e simples principalmente, nos ambientes universitários com 
intuito que possa aumentar a economia em escala e a eliminação de qualquer 
gerenciamento de informação que possa ser ineficiente.
Referências
[01] Almeida, Jocely Santos Caldas; Oliveira, Maria De Fátima Lima Chaves Figueiredo 
de. Tecnologia da Informação (TI) e o Desempenho Competitivo das Organizações. 
Disponível em <http://www.convibra.com.br/upload/paper/adm/adm_3123.pdf>. 
Acessado em 11/03/2019.
[02] Conselho Administrativo de Defesa Econômica (CADE). 2014. Catálogo de Serviços 
de TI do Cade. Diretoria Administrativa (DA), Coordenação-Geral de Tecnologia 
da Informação (CGTI). Brasília. Disponível em <http://www.cade.gov.br/acesso-
a-informacao/publicacoes-institucionais/tecnologia-da-informacao/subpasta/
catalago_de_servico.pdf>. Acessado em 15/03/2019.
[03] Foina, Paulo Rogério. 2001. Tecnologia de Informação: planejamento e gestão. 
São Paulo: Editora Atlas.305
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 305-310, junho de 2019
Capítulo XLIX - Utilização de Serviço de Registro, Autenticação e Preservação de Documentos para 
a Emissão de Diplomas Digitais
Utilização de Serviço de Registro, Autenticação 
e Preservação de Documentos para a Emissão de 
Diplomas Digitais
Marcelo Soares¹, Raphael P atrício¹, Georgenes Lima¹, Jeysibel  Dantas¹
1Superintendência de Tecnolo gia da  Informação (STI) – Universidade Federal da Paraíba (UFPB) 
Caixa Postal 58.055-000 -- Paraí ba -- PB  – Brasil
{marcelo.soare s,raphael,gli ma,jeysibel}@sti .ufpb.br 
Abstract
There are often reports in the media of falsification of documents, including in academic fie ld, whose f al se 
diplomas can easily be illegally acquired. The Digital Video Applications Laboratory of the Federal University of 
Paraíba (UFPB) has developed a solution to overcome this type of problem. The Digital Document Registration, 
Authentication, and Preservation (RAP) service combines the use of digital signatures and distributed ledger 
technologies to provide a secure platform that enables electronic document registration, validation of authenticity, 
and long-term preservation of such documents. This paper describes the use of the RAP service in issuance of digital 
diplomas at UFPB. The Integrated Management System for Academic Activities was adapted to integrate with the 
developed service, so that digital diplomas registered in a public ledger are available to students and can be validated 
by any interested party.
Resumo
Frequentemente são noticiados na mídia casos de falsi ficação de documentos, inclusive no âmbito acadêmico, no 
qual diplomas falsos podem ser facilmente adquiridos de maneira ilícita. O Laboratório de Aplicações de Vídeo 
Digital da Universidade Federal da Paraíba (UFPB) desenvolveu uma solução para contornar este tipo de problema. 
O serviço de Registro, Autenticação e Preservação de Documentos Digitais (RAP) combina o uso de assinaturas 
digitais e tecnologias de livro-razão distribuído para fornecer uma plataforma segura que permite o registro de 
documentos eletrônicos, a validação da autenticidade e a preservação a longo prazo de tais documentos. Este 
trabalho descreve a utilização do serviço RAP na emissão de diplomas digitais na UFPB. Foi feita uma adaptação 
do Sistema Integrado de Gestão de Atividades Acadêmicas (SIGAA), para a integração com o serviço desenvolvido, 
de tal modo que os diplomas digitais registrados em um livro público ficam à disposição dos discentes e podem ser 
validados por qualquer parte interessada.
1. Introdução
Atualmente, nas universidades brasi leiras, o diplom a é um papel im presso 
e assinado de próp rio punho pelos  respectivos respo nsá veis, e registra do em um 
livro físico mantido  na instituição seguin do regulamentaçã o próp ria. Diante dos 
recentes casos not iciados na mídia  sobre falsificaçã o de diplomas [G1 2 019], ou 
mesmo a emissão indevida de diplomas por institu ições de ensino credenciadas 
[G1-PE 2017, Globo 2018], é possí vel perceber a vulnerabilidade desse modelo. 
Em geral, o processo de verificaçã o de autentici dade de um diploma  é lento e 
burocrático, inib indo os interessa dos a realizá-lo.  Não são raros os casos de pessoas 
que assumiram cargo s públicos [UOL 2017] o u outros postos de t rabalho valendo-
se das fragilidades  desse modelo, suscetí vel de falsificação. Cla ramente, há uma 
páginapágina
306
demanda por meios que dificultem fraudes envolvendo documentos emitidos por 
instituições de ensino, sobretudo os diplomas, que correspondem ao documento 
de maior valia em uma vida acadêmica. 
 Nesse contexto, o Laboratório de Aplicações de Vídeo Digital (LAVID) da 
UFPB desenvolveu, com o apoio da Rede Nacional de Ensino e Pesquisa (RNP), 
o Serviço de Registro, Autenticação e Preservação Digital de Documentos (RAP) 
[Costa et al. 2018]. O RAP1 é uma plataforma que combina o uso de tecnologias 
consolidadas, como a certificação digital, e tecnologias de livro-razão distribuído 
(ou DL Ts, do inglês Distributed Ledger Technologies), como a blockchain2, para a 
autenticação e preservação de documentos digitais. Em meio ao desenvolvimento 
do RAP , o Ministério da Educação (MEC) publicou uma portaria que se alinha à 
proposta do projeto de pesquisa. A portaria Nº 330, de 5 de abril de 2018 menciona 
que, após a devida regulamentação, as instituições terão o prazo de dois anos 
para implementarem o diploma digital que deverá ser assinado por chaves da 
Infraestrutura de Chaves Públicas Brasileira (ICP Brasil) [da Silva 2018]. 
 O atual processo de emissão e registro de diplomas da UFPB é feito através 
do Sistema Integrado de Gestão de Atividades Acadêmicas (SIGAA), mantido 
pela Superintendência de Tecnologia da Informação (STI). Este trabalho relata a 
experiência resultante da cooperação técnica entre a STI da Universidade Federal 
da Paraíba e o LAVID do Centro de Informática da mesma instituição, na integração 
do SIGAA com a plataforma RAP para a emissão de diplomas em formato digital. 
Este documento está estruturado da seguinte forma: A Seção I introduziu o tema 
em questão e sua problemática, a Seção II descreve os métodos utilizados, a 
Seção III apresenta os resultados alcançados e por fim a Seção IV apresenta as 
considerações finais.
2. Métodos
O início do processo de desenvolvimento se deu com a concepção do 
diploma digital. O documento tradicional é impresso fisicamente em papel com as 
assinaturas feitas de próprio punho. O diploma digital foi concebido em arquivos 
do tipo PDF/A3 assinados digitalmente com chaves privadas da ICP Brasil. No Brasil, 
um documento assinado digitalmente por uma chave da ICP Brasil possui validade 
jurídica [Cardoso et al. 2001]. O artefato final de um diploma digital consiste 
em um arquivo PDF/A contendo o conteúdo original do diploma, as assinaturas 
digitais e os certificados digitais com suas respectivas chaves públicas usadas para 
a validação das assinaturas e identificação dos assinantes.
2.1. Mapeamento dos Dados
Associado a um artefato de diploma digital, o registro no serviço RAP 
armazena um conjunto de metadados relativos ao aluno e ao registro do diploma 
1 http://gt-rap.lavid.ufpb.br/
2 Blockchain é uma implementação de um livro-razão distribuído que usa uma estrutura de dados para o 
armazenamento de transações em blocos encadeados [Taylor et al. 2016, Natarajan et al. 2017].
3 Extensão de documento PDF para o armazenamento em longo prazo.página
307
na instituição. Uma vez que o RAP é uma solução genérica projetada para atender 
a diferentes tipos de documentos em diferentes contextos, foi necessário realizar 
um mapeamento da estrutura de dados presentes no atual modelo de emissão de 
diplomas da UFPB para o preenchimento dos metadados de acordo com a Portaria 
nº 1.095, de 25 de outubro de 2018, que padroniza os procedimentos para emissão 
e registro de diplomas nas Instituições de Educação Superior (IES) [da Silva 2018].
2.2. Estratégia de Integração
A integração do SIGAA com o RAP foi realizada seguindo duas abordagens. 
Para o registro dos diplomas, foi utilizada a estratégia de integração via banco de 
dados relacional. Ao gerar um diploma digital, o SIGAA insere seus metadados 
e sua representação, um arquivo PDF, em uma tabela, que é constantemente 
consultada por um deamon do RAP . O deamon verifica o status de cada registro 
e, após concluída a última assinatura digital, o diploma é enviado para o serviço 
RAP , que o registra e insere o hash do diploma digital na  blockchain da plataforma 
Ethereum. As consultas de situação de registro e a autenticação dos diplomas são 
feitas por meio de web  services. Ao receber um documento de diploma digital para 
validação, o SIGAA usa a API RESTful do RAP e exibe o resultado da autenticação 
para o usuário.
2.3. Implantação
A equipe de desenvolvedores da STI implementou a geração dos diplomas 
digitais em uma nova seção dentro do módulo Diplomas existente no SIGAA. O 
módulo Diplomas não sofreu alteração no seu processo de emissão e registro dos 
diplomas físicos, de tal modo que todas as etapas de verificação de pendências 
foram realizadas, sendo a geração do diploma digital apenas uma extensão de 
etapa final da emissão, em alternativa à impressão, conforme pode ser visto na 
Figura 1.
Figura 1 - Fluxo de emissão de diploma digitalpágina
308
 Como parte do processo de implantação, foi necessário o envolvimento 
das pessoas envolvidas no atual processo de emissão de diplomas na instituição. 
Os responsáveis pelas assinaturas do diploma físico adquiriram os certificados 
digitais e suas respectivas chaves através de uma Autoridade Certificadora da ICP 
Brasil. Para a assinatura digital dos diplomas, foi desenvolvido pela equipe do 
LAVID, um software assinador que permite a assinatura de documentos em lote.
3. Resultados
Para a primeira emissão de diplomas digitais, foram selecionados vinte (20) 
discentes dos cursos de graduação em Ciências da Computação e Engenharia da 
Computação. Os alunos tiveram os seus diplomas emitidos e assinados fisicamente, 
registrados em um livro físico na instituição e também os seus diplomas digitais, 
assinados digitalmente e registrados em um livro público (blockchain da Ethereum). 
Os alunos receberam os seus respectivos diplomas digitais em  pen drives marcados 
com QR Code, que representa uma URL única da página de validação do diploma 
digital, conforme pode ser visto na Figura 2. Em uma próxima versão, o SIGAA 
disponibilizará um acesso no Portal Discente para que os alunos que receberam os 
diplomas digitais possam recuperar os arquivos sempre que desejarem. Também 
foi desenvolvida uma área no portal público do SIGAA para que o arquivo do 
diploma digital possa ser validado, conforme descrito na estratégia de integração. 
Atualmente, uma consulta de autenticidade de diploma na UFPB dura em média 
três dias. Com o diploma digital, a consulta poderá ser feita em segundos através 
do portal público do SIGAA. O custo para o registro dos diplomas na  blockchain foi 
de $0,01 (um centavo de dólar) por cada transação. 
Figura 2 - Tela de validação de um diploma digital na área pública do SIGAApágina
309
4. Conclusão
Este trabalho apresentou a solução adotada pela UFPB para tornar o 
processo de emissão e registro de diplomas mais seguro, permitindo que tanto 
a comunidade acadêmica quanto a sociedade possam utilizar um serviço para a 
validação dos diplomas emitidos pela instituição de maneira simples e rápida. 
Percebeu-se que a plataforma RAP fornece uma camada adicional de segurança, já 
que uma representação resumida do diploma é registrada em uma DL T e qualquer 
interessado de posse de um diploma digital pode verificar o registro. A unidade 
de Emissão e Registro de Diplomas da UFPB indicou uma grande procura por 
parte da comunidade acadêmica por informações sobre como receber o diploma 
digital. A equipe também relatou a importância do diploma digital para o aumento 
da eficiência do setor, visto que consultas de autenticidade podem ser feitas 
diretamente pela plataforma, trazendo mais comodidade para a sociedade.
 De acordo com a experiência obtida na UFPB, a equipe da STI considerou 
aplicável o modelo de integração desenvolvido, como também considerou a 
utilização da plataforma RAP como uma potencial solução de longo alcance para 
as fraudes envolvendo diplomas, por ser uma solução facilmente integrável ao 
sistema SIGAA, utilizado por diversas instituições federais de ensino brasileiras. 
Fraudes relacionadas à emissão indevida de diplomas, poderiam ser combatidas 
através da utilização de uma plataforma como a RAP por órgãos reguladores 
para a verificação de indicadores de quantidade de diplomas emitidos em cada 
instituição. Os próximos passos do projeto envolvem o estudo da escalabilidade 
da solução e a revogação de diplomas registrados na DL T . 
Referências
Cardoso, F. H., Gregori, J., Tavares, M., Sardenberg, R. M., and Parente, P . (2001). Me- 
dida proviso´ria no 2.200-2, de 24 de agosto de 2001.
Costa, R., Faustino, D., Lemos, G., Queiroga, A., Djohnnatha, C., Alves, F., Lira, J., and 
Pires, M. (2018).  Uso na˜o financeiro de blockchain:  Um estudo de caso sobre o 
registro, autenticac¸a˜o e preservac¸a˜o de documentos digitais acadeˆmicos.  In 
Anais do I Workshop em Blockchain:  Teoria, Tecnologias e Aplicac¸o˜es (WBlockchain 
- SBRC 2018), volume 1, Porto Alegre, RS, Brasil. SBC.
da Silva, R. S. (2018). Portaria no 1.095, de 25 de outubro de 2018. Filho, M. (2018). 
Portaria no 330, de 5 de abril de 2018.
G1  (2019).    Jovens  que  sonham  com  diploma  universita´rio  sa˜o  v´ıtimas  de  
golpe  no interior. https://g1.globo.com/fantastico/noticia/2019/01/27/jovens-que-
sonham-com- diploma-universitario-sao-vitimas-de-golpe-no-interior.ghtml.
G1-PE (2017). Relato´rio do mec aponta mais de 1,3 mil diplomas vendidos por 
faculdades em  pe, diz deputado. https://g1.globo.com/pernambuco/educacao/
noticia/relatorio- do-mec-aponta-mais-de-13-mil-diplomas-vendidos-por-
faculdades-em-pe-diz- deputado.ghtml.página
310
Globo, O. (2018).  Operac¸a˜o enquadra escolas que emitiram 350 mil diplomas falsos e 
movimentaram  r$  700  milho˜es  em  5  anos. https://oglobo.globo.com/rio/operacao- 
enquadra-escolas-que-emitiram-350-mil-diplomas-falsos-movimentaram-700- 
milhoes-em-5-anos-23096704.
Natarajan,  H.,  Krause,  S.,  and  Gradstein, H. (2017). Distributed ledger 
technology (dlt) and blockchain. FinTech note; no. 1. Washington, D.C. : World Bank Group. 
http://documents.worldbank.org/curated/en/177911513714062215/Distributed- 
Ledger-Technology-DL T-and-blockchain. [Online; accessed 06-August-2018].
Taylor, S., Brown, R. G., Lehdonvirta, V., Ali, R., Sasse, A., Godsiff, P ., Godsiff, P ., Mulligan, 
C., and Curry, P . (2016). Distributed ledger technology: beyond block chain. Technical 
report, Government Office for Science.
UOL (2017). Suspeito de ter diploma falso, professor da ufpb deu aula em 8 faculda- 
des. https://educacao.uol.com.br/noticias/2017/04/30/suspeito-de-ter-diploma-
falso- professor-da-ufpb-deu-aula-em-8-faculdades.htm.página
311
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 311-316, junho de 2019
Capítulo L - Utilizando a Gestão de Conhecimento para facilitar o desenvolvimento em projetos de
softwareUtilizando a Gestão de Conhecimento para 
facilitar o desenvolvimento em projetos de 
software
Rodrigo A. Costa1
1Centro de Tecnologia da Informação e Comunicação – Universidade Federal do Amazonas (UFAM)
Caixa Postal 69080-900 – Manaus – AM – Brasil
{rodrigocosta}@ufam.edu.br
Resumo
O desenvolvimento de software é uma das fases necessárias para a correta execução de um projeto de TI. Devido 
às constantes demandas de uma Instituição de Ensino para a melhoria dos serviços oferecidos à comunidade é 
preciso utilizar mecanismos que favoreçam a gestão e o compartilhamento de informações para promover agilidade 
no desenvolvimento de sistemas. Este artigo apresenta os resultados preliminares de um trabalho de iniciação 
de um processo de Gestão de Conhecimento que visa a melhoria dos processos internos referentes à atividade de 
desenvolvimento de software dentro da Coordenação de Sistemas da Universidade Federal do Amazonas.
1. Introdução
Para se produzir um produto ou sistema é necessário seguir uma série 
de passos previsíveis, ou seja, um guia, que ajude a chegar a um resultado de 
qualidade, dentro do tempo previsto. No caso do desenvolvimento de software, 
esse guia é o processo de software. Um processo de software pode ser visto como 
o conjunto de atividades, métodos e práticas que guiam pessoas na produção de 
software [Sommerville, 2007]. 
 Na Universidade Federal do Amazonas (UFAM), o Centro de Tecnologia da 
Informação e Comunicação (CTIC) é responsável pela execução dos projetos de 
software, desde sua fase inicial, composta por entrevistas com os potenciais clientes 
ou usuários do sistema, elaboração do projeto e modelagem, desenvolvimento da 
solução utilizando linguagens de programação e ambientes próprios, treinamento 
do usuário final e, por fim, a manutenção em suas diversas formas (corretiva, 
evolutiva e preventiva).
 No entanto, mesmo seguindo um processo de software bem definido, 
fazendo uso de metodologias para desenvolvimento ágil, como SCRUM e 
ferramentas que auxiliam na distribuição e visualização de responsabilidades 
como Kanban, os projetos desenvolvidos dentro da Coordenação de Sistemas 
ainda possuem alguns problemas:
1. Projetos entregues fora do prazo;
2. Falta de padrão durante o desenvolvimento de software;página
312
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 311-316, junho de 2019
Capítulo L - Utilizando a Gestão de Conhecimento para facilitar o desenvolvimento em projetos de
software3. Excesso de reutilização de código de módulos desenvolvidos anteriormente.  
Isso dificulta a refatoração dos novos componentes por conter referências de 
partes de código fora do que foi planejado, como bibliotecas e scripts.
Esses problemas ocorrem com frequência e à medida que não são 
solucionados, os projetos desenvolvidos ficam com um aspecto “engessado” 
impossibilitando até mesmo a realização de atividades como atualização de versão 
utilizada no ambiente de programação. Algumas causas para esses problemas são:
1. Falta de servidores para a execução dos projetos. As equipes alocadas 
para a realização de um projeto são pequenas. Dependendo da complexidade 
e prazo estipulado, varia de 1 a 2 pessoas e isso para projetos complexos 
impossibilita a inspeção adequada para manter a qualidade desejada.
2. Prazo de entrega inadequados e cultura da urgência. Muitos projetos 
são solicitados com um período curto para realização, dificultando o 
desenvolvimento adequado frente à complexidade do cenário. A cultura 
organizacional ainda precisa ser mudada para alinhar as estratégias entre 
todos os envolvidos no processo.
3. Falta de documentação dos sistemas desenvolvidos. Devido aos 
prazos curtos e as equipes serem pequenas, muitos sistemas e módulos 
desenvolvidos não possuem uma documentação técnica referente à 
modelagem feita ou mesmo, dicas de codificação que poderiam agilizar 
processos futuros de manutenção.
Este trabalho tem como objetivo demonstrar o processo para criação de 
um repositório, Base de Conhecimento (BC), utilizando a ferramenta Redmine 
que possa auxiliar na criação e manutenção de funcionalidades a partir do 
compartilhamento de informações e conhecimento técnico de cada profissional, 
além da possibilidade de manter uma documentação à cerca dos módulos 
desenvolvidos.
2. A Gestão de Conhecimento na Administração Pública
O conhecimento que uma organização consegue ter e sua capacidade de criar 
e utilizar esse conhecimento é a habilidade central para manter uma vantagem 
competitiva e inovar [Rebelo and Conte, 2015]. A Gestão do Conhecimento (GC) 
pode ser entendida como a possibilidade de gerar valor a partir de bens intangíveis, 
incluindo qualquer atividade relacionada com a captura, uso e compartilhamento 
do conhecimento pela organização.
 A criação da GC é uma das formas encontradas para melhorar o conhecimento 
organizacional, dar celeridade nas resoluções dos problemas e reduzir gastos 
com correção de incidentes que ocorrem com certa frequência nos projetos 
desenvolvidos. Seguindo o raciocínio de [Batista, 2012] muitas organizações não página
313
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 311-316, junho de 2019
Capítulo L - Utilizando a Gestão de Conhecimento para facilitar o desenvolvimento em projetos de
softwareconhecem o termo “gestão do conhecimento” , mas executam práticas que podem 
ser consideradas de GC.
 Na Administração Pública é preciso encontrar mecanismos que favoreçam 
a boa execução dos serviços oferecidos. Devido ao cenário encontrado: falta 
de servidores, falta de processos gerenciais bem definidos e até mesmo falta 
de recursos para aquisição de materiais e treinamentos, trabalhar com uma 
ferramenta que estimule a gestão do conhecimento pode facilitar as atividades do 
dia a dia, reduzindo o tempo de execução e promovendo a melhoria contínua de 
habilidades funcionais.
3. Métodos
Este trabalho foi dividido em três fases: 1) Levantamento das funcionalidades 
a serem padronizadas com a implantação do repositório; 2) Adequação da 
ferramenta Redmine para uso como ambiente único de consulta e, 3) Construção 
da Base de Conhecimento pelos servidores responsáveis. Cada fase é detalhada a 
seguir:
1. Levantamento das funcionalidades. Inicialmente, foi realizada uma 
varredura nos módulos já desenvolvidos para mapear as funcionalidades que 
mais se repetiam e apresentavam potencial de reuso. Elas foram selecionadas 
e passaram a ser organizadas na Base de Conhecimento. A Tabela 1, lista as 
funcionalidades mapeadas e a quantidade que cada uma se repetia dentro 
dos módulos existentes:
Item Funcionalidade Quantidade
1 Alternar obrigatoriedade de campos 16
2 Desabilitar inputs dos formulários 18
3 Exibir/ocultar campos e elementos 25
4 Habilitar/desabilitar campos para edição 13
5 Listagem de registros a partir de uma consulta 48
6 Utilização de máscaras em campos numéricos 22
7 Verificar similaridade entres strings 10
8 Verificar seleção de radiobuttons 15
9 Verificar marcação de checkbox 27
10 Protótipos de telas, formulários e abas 39
11 Utilização de janelas modais 25
Tabela 1. Funcionalidades mapeadas e quantidade que são utilizadas.
2. Adequação da ferramenta Redmine. O Redmine é um software livre e de 
código aberto, licenciado sobre os termos da GNU General Public License. Essa 
ferramenta funciona originalmente como gerenciador de projetos [Redmine, 
2015]. Dentro da metodologia empregada para captar as informações, por 
possuir múltiplas funcionalidades, o Redmine oferece apoio sistematizado 
para o processo de Gestão de Conhecimento.página
314
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 311-316, junho de 2019
Capítulo L - Utilizando a Gestão de Conhecimento para facilitar o desenvolvimento em projetos de
software3. Construção da Base de Conhecimento. Após a adequação do Redmine, 
cada Analista, dos projetos em desenvolvimento, ficou responsável por definir 
um procedimento para cada funcionalidade mapeada conforme tivesse 
acesso a ela. Quando outro Analista precisasse desenvolver um trecho de 
código que já fora mapeado, ele poderia consultar o que já tinha sido feito e 
utilizar ou, atualizar o que foi definido (Figura 1). Esse processo foi realizado 
até a definição de um procedimento padrão para todas as funcionalidades.
Figura 1. Esquema do processo de construção da Base de Conhecimento.
4. Resultados
Após a realização de todas as fases definidas para a construção da Base de 
Conhecimento, o resultado foi um repositório contendo os onze procedimentos 
padronizados, revisados pelos analistas e com acesso para todos os servidores 
lotados na Coordenação de Sistemas (Figura 2). 
A estrutura final dos procedimentos ficou dividida assim: título da 
funcionalidade implementada, descrição do funcionamento com instruções 
e pré-requisitos para uso, o passo a passo para utilização do procedimento, 
palavras-chave para busca e, o nome do(s) módulo(s) onde é possível visualizar a 
funcionalidade em uso.
Figura 2. Visão parcial da tela do Redmine (BC) com o procedimento definido.página
315
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 311-316, junho de 2019
Capítulo L - Utilizando a Gestão de Conhecimento para facilitar o desenvolvimento em projetos de
software Algumas dessas atividades eram feitas apenas por um único servidor, 
causando uma dependência operacional muito grande e, consequentemente, 
demora na execução da atividade. Isso ocorria devido: 1) Falta de documentação 
específica e, 2) Falta de compartilhamento das informações. Com a criação da Base 
de Conhecimento foi possível enxergar outros processos dentro da Coordenação 
de Sistemas que puderam ser padronizados e compartilhados (Tabela 2).
Categoria Processo Procedimentos
Configuração 
de sistemasAmbiente de programação 6
Gerência de banco de dados 3
Instalação e uso do DSpace 11
Instalação e uso do Docker 4
Manutenção 
de sistemasControle de versão 5
Sistema de auditoria 3
Portais institucionais numéricos 3
Sistema 
AcadêmicoAdministração de banners 2
Geração de relatórios 10
Processamento de matrícula 7
Uso de 
sistemasConsulta de projetos (Lira) 4
Tabela 2. Processos mapeados na Coordenação de Sistemas.
A partir da construção da BC foi possível também mapear outros processos, não 
necessariamente ligados à atividade de desenvolvimento, mas que apresentavam 
repetição na execução e capacidade de compartilhamento, tais como: scripts para 
geração de relatórios, manutenção de sistemas, consultas para processamento de 
matrícula, configurações de ambientes e programas.
Uma vez que esses novos processos são padronizados e mantidos em uma 
base de consulta comum, pode-se compartilhá-los entre todos os servidores, além 
da possibilidade de nivelar o conhecimento da equipe, melhorando até mesmo na 
distribuição das atividades a serem desenvolvidas pelos próprios servidores dento 
da Coordenação.
5. Conclusão
Este artigo relata a experiência na construção de uma Base de Conhecimento 
utilizando a ferramenta Redmine. A criação da BC propiciou um aprendizado 
maior sobre como codificar sob um mesmo padrão e corrigir erros nos módulos 
desenvolvidos. Ela tem por objetivo transformar conhecimento tácito em ativo 
organizacional favorecendo o aprendizado, definir padrões de desenvolvimento e 
criar uma documentação básica a ser utilizada sempre que necessário.
Todo esse processo de construção e manutenção da BC possibilita a 
ampliação das habilidades técnicas, em especial, dos servidores da Coordenação 
de Sistemas, promove uma capacitação a custo zero e imediata de toda a equipe página
316
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 311-316, junho de 2019
Capítulo L - Utilizando a Gestão de Conhecimento para facilitar o desenvolvimento em projetos de
softwareenvolvida na execução dos projetos, diminui a dependência para a realização de 
atividades que até então eram de conhecimento exclusivo de alguns e, organiza 
uma documentação sobre o código desenvolvido referente aos projetos.
Isso demonstra que realizar atividade de Gestão do Conhecimento a nível 
gerencial possibilita o desenvolvimento de um conjunto de ações com o objetivo 
de fomentar o conhecimento organizacional entre todos os envolvidos. Essas 
ações quando bem definidas estimulam a criação, explicitação e disseminação 
de conhecimentos no âmbito interno da organização, com objetivo de atingir à 
excelência organizacional.
É importante ressaltar que a BC está em constante desenvolvimento e 
atualização pelos analistas de cada projeto, conforme a necessidade de uso e 
consulta na realização dos projetos. A elaboração da BC foi considerada positiva 
não só entre os servidores da Coordenação de Sistemas, mas também para os 
outros servidores do CTIC que puderam conhecer o funcionamento dos processos 
internos, bem como iniciar a atividade para elaboração de manuais, padronização 
e compartilhamento da informação nas próprias Coordenações.
Referências
Batista, F. F. (2012). Modelo de gestão do conhecimento para a administração pública 
brasileira: Como implementar a gestão do conhecimento para produzir resultados em 
benefícios do cidadão. IPEA. 
Rebelo, J. and Conte, T . (2015). Framework de gerência do conhecimento e aprendizagem 
organizacional com fatores de influência para organizações de software. WTDQS.
Redmine (2015). Redmine. http://www.redmine.org. Acessado em 25 de janeiro de 
2019
Sommerville, I. (2007). Engenharia de Software. Pearson, 6a edição.
Souza, I. M. and Samuel, F. (2012). Gestão do conhecimento na gestão pública: Desafios 
do programa ciência sem fronteiras. http://periodicos.uesb.br/index.php/praxis/
article/viewFile/1789/1627.página
318
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 318-323, junho de 2019
Capítulo LI - Análise do Processo de Interações Acadêmicas Através do Mapeamento de Fluxo de 
Valor na UFRGSAnálise do Processo de Interações Acadêmicas 
Através do Mapeamento de Fluxo de Valor na 
UFRGS
Marlon Soliman1, Joao Francisco da Fontoura Vieira1 Nicolas Dentzuk1, Erica Kranz de 
Oliveira1, Priscilla Freire dos Reis Pontes1,  Éverson Josué Santos1
1Escritório de Processos – Universidade Federal do Rio Grande do Sul (UFRGS)
Av. Paulo Gama, 110, Anexo I Reitoria, Sala 209. Porto Alegre - RS
marlon.soliman@proplan.ufrgs.br, joao.vieira@proplan.ufrgs.br, nicolas.
dentzuk@proplan.ufrgs.br, erica.oliveira@proplan.ufrgs.br, priscilla.pontes@
proplan.ufrgs.br, everson.santos@proplan.ufrgs.br
Resumo
Na Universidade Federal do Rio Grande do Sul (UFRGS), o processo para aprovação de ações finalísticas a serem 
realizadas entre a Universidade e outras partes interessadas passa por um processo moroso e burocrático, conhecido 
como Interações Acadêmicas. Esse artigo relata como a ferramenta de Mapeamento de Fluxo de Valor (MFV) foi 
utilizada para apoiar a análise e a elaboração de recomendações para esse processo. Os resultados mostram que o 
MFV permitiu o correto esclarecimento do problema, direcionando as estratégias e ações a serem tomadas.
Palavras-chave: Mapeamento de fluxo de valor; Interações; Universidades; Produção enxuta; Gestão pública.
1. Introdução
Instituições de Ensino Superior (IES) agregam valor a sociedade através 
de suas atividades finalísticas, notadamente ensino, pesquisa e extensão. 
Frequentemente, para o desenvolvimento dessas atividades, é necessário que 
as IES estabeleçam relações com demais partes interessadas da sociedade, tais 
como órgãos governamentais, empresas públicas/privadas e fundações de apoio. 
Esse relacionamento é formalizado através de um instrumento legal (convênio, 
contrato ou protocolo de intenção/cooperação) celebrado entre as partes para 
execução de um objeto específico. Na Universidade Federal do Rio Grande do Sul 
(UFRGS), ações dessa natureza são chamadas de “Interações Acadêmicas” , e são 
consideradas estratégicas para a excelência acadêmica e viabilidade financeira da 
instituição. 
 A celebração de uma interação acadêmica na UFRGS inicia-se com o 
preenchimento da proposta pelo coordenador da interação no sistema eletrônico 
desenvolvido pela própria Universidade para este fim. As propostas tramitam por 
diversas instâncias avaliativas (Comissões, Pró-Reitorias, Procuradoria Geral, entre 
outros) que se manifestam pela realização da proposta. O processo de tramitação 
finaliza com a assinatura do instrumento legal pelas partes. Esse processo, no 
entanto, é reconhecidamente moroso e burocrático na UFRGS, o que acarreta 
no prejuízo para suas atividades finalísticas, pois a execução de uma interação 
acadêmica só pode ser de fato iniciada após a assinatura do instrumento legal.página
319
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 318-323, junho de 2019
Capítulo LI - Análise do Processo de Interações Acadêmicas Através do Mapeamento de Fluxo de 
Valor na UFRGS Na tentativa de sanar esse problema, o Escritório de Processos (EP) da UFRGS 
vem utilizando conceitos e práticas da Produção Enxuta [Womack and Jones 2003] 
para dar visibilidade aos desperdícios contidos no processo. A Produção Enxuta é 
uma filosofia gerencial com origem na indústria automotiva, mas que vem sendo 
amplamente utilizada para a melhoria de processos e serviços administrativos em 
organizações públicas e privadas [Bateman et al. 2018; Soliman et al. 2018]. Assim, 
esse artigo tem como objetivo apresentar como o problema dos elevados tempos 
de tramitação e erros no processo de Interações Acadêmicas foi abordado através 
do mapeamento de fluxo de valor (MFV), uma das ferramentas mais tradicionais da 
Produção Enxuta [Locher 2011].
2. Métodos
A análise do processo e a elaboração das recomendações foram realizadas 
por meio do cumprimento de quatro etapas metodológicas: (I) identificação da 
expectativa dos coordenadores; (II) mapeamento do estado atual; (III) mapeamento 
do estado futuro; e (IV) elaboração das recomendações.
 Para a etapa (I), um questionário eletrônico (Google Forms) foi desenvolvido e 
enviado para todos os coordenadores de interações acadêmicas (ativos e inativos). 
O questionário foi composto de quinze perguntas, sendo três para caracterização 
do tipo de interações realizadas pelo coordenador consultado; sete relacionadas a 
expectativa de tempo de tramitação das propostas de interação; e cinco referentes 
a funcionalidades do atual sistema eletrônico de tramitação, além de um espaço 
livre para comentários gerais. Os dados foram analisados através de estatísticas 
descritivas.
 Para etapa (II), tomou-se por base os diagramas de processo (em notação 
BPMN) já disponíveis no portal de processos da UFRGS. Dados quantitativos 
referentes aos tempos de atravessamento (lead time), volume de interações 
tramitadas e percentual de interações analisadas sem diligências (%C&A, do 
inglês %Correct & Accurate) foram extraídos do sistema eletrônico de tramitação e 
tradados no Excel®. Após, o mapeamento do estado futuro (etapa III) foi realizado 
pela equipe de trabalho do EP , partindo-se de premissas práticas de estudos 
prévios sobre o problema e dados levantados da expectativa dos coordenadores 
(tempo aceitável de tramitação). Por fim, elaborou-se recomendações (etapa IV) 
para que a situação futura seja alcançada.
3. Resultados
3.1. Identificação da expectativa dos coordenadores e atores
Para a identificação da expetativa dos coordenadores quanto ao tempo 
aceitável de tramitação de interações acadêmicas, o questionário foi enviado 
para todos os coordenadores que já tramitaram propostas de interações (N = 179). 
Deste, 64 (36%) responderam, de onde foi possível identificar que 82% consideram 
até 2 meses como um tempo aceitável de tramitação.página
320
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 318-323, junho de 2019
Capítulo LI - Análise do Processo de Interações Acadêmicas Através do Mapeamento de Fluxo de 
Valor na UFRGS3.2. Mapeamento do estado atual
O mapa de fluxo de valor da situação atual construído está demonstrado na 
Figura 1, onde cada caixa representa um órgão avaliador pertencente a estrutura 
da Universidade. Os tempos mostrados na linha do tempo abaixo representam os 
tempos médios de atravessamento em cada etapa do processo. Tramitações do 
tipo “B” correspondem a contratos de pequeno porte, que por seguirem um fluxo 
simplificado de aprovação não passam pelas etapas “CIUS” e “CONSUN” .
Figura 1. Mapa de fluxo de valor de interações acadêmicas (estado atual)
A partir do mapeamento do estado atual, foi possível observar que das 400 
interações propostas desde a implementação do sistema, apenas 144 tiveram 
sua tramitação totalmente finalizada, sendo que 40 propostas foram canceladas/
indeferidas e 216 ainda se encontram em avaliação pelos órgãos competentes. Nota-
se que o tempo médio de tramitação é de, em média, 247,1 dias, portanto 312% 
acima do tempo aceitável (até 60 dias). Em parte, esse tempo de atravessamento 
é decorrente da elevada incidência de diligências, mostrado pelos baixos valores 
de %C&A nas etapas do processo. O RFPY (Rolled First Pass Yeld) revela que a 
probabilidade de uma interação ser não receber nenhuma diligência ao longo de 
todo o processo é de apenas 3,5%.página
321
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 318-323, junho de 2019
Capítulo LI - Análise do Processo de Interações Acadêmicas Através do Mapeamento de Fluxo de 
Valor na UFRGS3.3 Mapeamento do estado futuro
Para o desenho da situação futura, inicialmente dividiu-se o processo em 
dois segmentos, conforme mostrado pela linha vertical tracejada na Figura 2. 
Esta segmentação foi relevante pois as atividades compreendidas no segmento 1 
correspondem a etapas cujo desempenho pode ser influenciado em grande parte 
através de ações gerenciais. Já as etapas do segmento 2 estão mais sujeitas a 
variabilidades que fogem do alcance da Universidade (ex: tempo para recebimento 
das vias assinadas pelas outras partes interessadas), e por isso são de mais difícil 
intervenção. A estratégia inicial adotada para o estado futuro foi no sentido de 
promover a estabilidade do processo e remover os desperdícios das atividades 
que estão compreendidas no segmento 1, para posteriormente reavaliar e enxugar 
as atividades do segmento 2.
Figura 2. Mapa de fluxo de valor de interações acadêmicas (estado futuro)
Para o segmento 1, estabeleceu-se tempos máximos de atravessamento para 
cada órgão avaliador, que devem ser atingidos com ações de melhoria específicas 
nestes setores. O compromisso com estes tempos pode ser interpretado como 
um fator crítico de sucesso do projeto do estado futuro, pois tem-se uma visão 
operacional de “quanto” e “onde” o processo precisa melhorar para atingir o 
resultado global desejado. 
Devido à alta incidência de diligências (retrabalhos) no processo atual, 
referências mínimas para o %C&A também foram estabelecidas. Para as etapas página
322
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 318-323, junho de 2019
Capítulo LI - Análise do Processo de Interações Acadêmicas Através do Mapeamento de Fluxo de 
Valor na UFRGSdo segmento 1, estabeleceu-se como meta %C&Amínimo = 90%. Para as etapas do 
segmento 2, devido à natureza não-avaliativa dessas atividades, entendeu-se que 
é possível obter-se um %C&A = 100%. Como resultado, o novo RFPY calculado é de, 
no mínimo, 43%. Um comparativo entre o estado atual e a futuro é apresentado 
na Tabela 1.
Tabela 1. Comparativo entre o estado atual e futuro
Global Atual (dias) Futuro (dias) % Melhoria
Lead Time total 247,1 171,8 30,5%
Lead Time segmento 1 131,3 56,0 57,3%
RFPY 3,5% 43,0% 1128,6%
Deve-se notar que a condição ideal (i.e. toda a tramitação em menos de 60 
dias) ainda não é atingida neste estado futuro. Como o fluxo atual é muito instável, 
optou-se pelo estabelecimento de metas menos rígidas para este primeiro 
momento. Em um próximo ciclo, as metas propostas serão ajustadas para que o 
tempo total de atravessamento não exceda os 60 dias. A modificação do processo 
através de ciclos incrementais está alinhada com as diretrizes de melhoria contínua 
da Produção Enxuta. 
3.4. Elaboração das recomendações
Após a análise comparativa da situação atual e da situação futura, as 
recomendações elaboradas pelo Escritório de Processos da UFRGS foram as 
seguintes:
a) Cada ator do processo deve reunir sua equipe e levantar possíveis ações 
(brainstorm) para adequar-se ao tempo máximo de tramitação do estado 
futuro. Essas ações serão validadas pelo EP , que auxiliará na elaboração do 
plano de ação para implantação das estratégias traçadas;
b) Cada ator do processo deve identificar precisamente quais informações 
são relevantes para a avaliação da interação acadêmica no seu setor. 
Essas informações serão utilizadas para realizar melhorias nas telas de 
preenchimento do sistema de tramitação, com objetivo de evitar diligências 
causadas pelo preenchimento incorreto/incompleto dos formulários; e
c) Cada ator do processo deve ser capaz de avaliar seu próprio desempenho 
no processo (tempo de atravessamento, %C&A e %interações concluídas 
no prazo). Para tanto, o EP desenvolverá ferramentas padronizadas que 
permitam os setores mensurarem seus indicadores.
Os mapas construídos e as recomendações propostas foram apresentadas 
em reunião com os atores do processo (aprox. 35 pessoas), onde os mesmos 
foram instruídos de como devem proceder para implementar as estratégias 
recomendadas. Uma nova reunião para acompanhamento das ações está prevista 
para maio/2019.página
323
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 318-323, junho de 2019
Capítulo LI - Análise do Processo de Interações Acadêmicas Através do Mapeamento de Fluxo de 
Valor na UFRGS4. Conclusão
A utilização da ferramenta de mapeamento de fluxo de valor mostrou-se 
capaz de identificar precisamente o “quanto” e “onde” o processo de interações 
acadêmicas na UFRGS precisa melhorar para atingir as expectativas dos 
coordenadores. Também foi possível identificar o impacto causado pela incorreta 
e/ou incompleta instrução das propostas, o que resulta em elevado número de 
diligências. O esclarecimento do problema através do MFV serviu como base de 
argumentação para a interlocução com os atores do processo, pois foi possível 
mostrar como o desempenho individual de cada setor contribui para o desempenho 
da Universidade como um todo, reduzindo-se assim a resistência à mudança e 
conquistando-se o apoio das partes envolvidas.
Referências
Bateman, N., Radnor, Z. and Glennon, R. (2018). Editorial: The landscape of Lean across 
public services. Public Money and Management, v. 38, n. 1, p. 1–4. 
Locher, D. (2011). Lean Office and Service Simplified: The Definitive How-To Guide. 1. 
ed. New York: Productivity Press. 
Soliman, M., Saurin, T . A. and Anzanello, M. J. (2018). The impacts of lean production 
on the complexity of socio-technical systems. International Journal of Production 
Economics, v. 197, p. 342–357. 
Womack, J. P . and Jones, D. T . (2003). Lean thinking: banish waste and create wealth 
in your corporation. New York: Productivity Press.página
324
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 324-329, junho de 2019
Capítulo LII - Aplicação da Gestão de Processos em uma Instituição Federal de Ensino Superior: 
o caso da Divisão de Orçamento da UFERSAAplicação da Gestão de Processos em uma 
Instituição Federal de Ensino Superior: o caso 
da Divisão de Orçamento da UFERSA
Lívia R. Barreto1, Geisa M. R. de Vasconcelos1, Amanda Braga Marques2  
1Escritório de Processos – Universidade Federal Rural do Semi-Árido (UFERSA)
Caixa Postal 59.625-900 – Mossoró – RN – Brasil
2Engenharia de Produção – Universidade Federal Rural do Semi-Árido (UFERSA) – Mossoró – RN – 
Brasil
livia.barreto@ufersa.edu.br, geisa.vasconcelos@ufersa.edu.br, amanda_
bmarques@hotmail.com 
Resumo
O presente trabalho objetiva descrever a metodologia de implantação da Gestão de Processos de Negócio em uma 
divisão orçamentária de uma Instituição Federal de Ensino Superior. Trata-se de mostrar como se deu a evolução 
dessa trajetória, bem como propor a modelagem e implantação de melhorias nos processos que perpassam esse setor. 
A utilização desse modelo norteará as futuras ações da equipe da Divisão de Orçamento e servirá de base para os 
próximos passos do planejamento e execução orçamentária da universidade.
Palavras-chave: Gestão de Processos de Negócio; Divisão Orçamentária; Universidade.
1. Introdução
Estudar o processo de uma organização é o primeiro passo para entrar no 
dinamismo das mesmas. Elas vivenciam um cenário de crescente competitividade 
e, para atender essa realidade, elas vêm buscando soluções para melhor estruturar 
e integrar seus processos diante de sua importância a qualquer tipo de instituição 
seja privada ou pública, criando maior flexibilidade e agilidade em suas operações 
[CATELLI, SANTOS, 2004].
A necessidade de organizações públicas de se adequarem aos novos 
programas de aperfeiçoamento de processos tem se mostrado uma constante nos 
últimos anos. A gestão por processos, inicialmente desenvolvida no setor privado, 
tem sido utilizada também no setor público. Nesse setor, a relevância da gestão 
por processos de negócio é percebida por meio da maior eficácia e eficiência 
alcançada a partir da reestruturação organizacional, juntamente com os processos 
multifuncionais [GULLEDGE JR., SOMMER, 2002].
Para aperfeiçoar os processos, deve-se entender o modo como esses fluem 
através da organização. Esse entendimento é vital para a mudança planejada, 
pois nenhuma equipe de projeto pode mudar aquilo que não entende e nenhuma 
mudança pode ser colocada em prática se não houver o porquê para tal 
[CASTELLANELLI 2012]. página
325
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 324-329, junho de 2019
Capítulo LII - Aplicação da Gestão de Processos em uma Instituição Federal de Ensino Superior: 
o caso da Divisão de Orçamento da UFERSAA Divisão de Orçamento (DIORC) tem por competência elaborar, em 
consonância com o Plano de Desenvolvimento Institucional (PDI), a proposta 
orçamentária anual da Instituição, coordenar as atividades relacionadas à 
gestão orçamentária e financeira, como alterações da proposta orçamentária, 
remanejamento e distribuição orçamentária. Mapear os processos que são de 
responsabilidade dessa divisão é essencial diante da importância e criticidade dos 
processos que envolvem a movimentação de recursos financeiros, os quais estão 
diretamente ligados à manutenção da integridade e impessoalidade. Dessa forma, 
quanto mais padronizados e definidos os processos, melhor a transparência do 
uso do dinheiro público.
 O objetivo deste artigo é analisar e discorrer sobre o processo de implantação 
do mapeamento de processos baseado no BPM (Business Process Management ou 
Gestão de Processos de Negócios) em estruturas da gestão pública. Esta abordagem 
foi utilizada em uma divisão orçamentária, integrante da estrutura administrativa 
da Universidade Federal Rural do Semi-Árido, responsável por gerenciar a execução 
orçamentária, receber e prestar contas dos recursos financeiros da instituição.
2. Métodos
Quanto à finalidade, esta pesquisa é aplicada, uma vez que está voltada 
para a obtenção de conhecimentos que têm por objetivo a aplicação em situações 
particulares. São gerados conhecimentos, para, na prática, solucionar problemas 
específicos [Gil 1996].
Quanto à abordagem, esta pesquisa é qualitativa, pois utiliza informações 
obtidas em benchmarking e entrevistas feitas com servidores para entendimento 
do processo. Para então esses dados serem analisados e aplicados no estudo. 
[Fantinato 2015].
O objeto deste estudo é a Divisão de Orçamento, que está situada na Pró-
Reitoria de Planejamento (PROPLAN), da Universidade Federal Rural do Semi-
Árido (UFERSA), a qual é uma instituição pública de ensino superior potiguar 
com sede na cidade de Mossoró e campi nas cidades de Angicos, Pau dos Ferros e 
Caraúbas, atendendo a mais de 8.500 estudantes. A UFERSA foi fundada em 1967 
como Escola Superior de Agricultura de Mossoró (ESAM), específica para ciências 
agrárias, sendo transformada posteriormente em universidade federal em julho 
de 2005, contando com 47 cursos atualmente.
Por se tratar de uma instituição jovem, cujo corpo técnico-administrativo 
também foi renovado em razão da federalização, a UFERSA apresenta deficiência de 
padronização e formalização de processos, tão bem como a falta de disseminação 
de práticas comuns. Aliado a isso, diante do cenário nacional de recursos 
financeiros limitados, torna-se ainda mais necessário que ações de planejamento 
sejam adotadas. página
326
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 324-329, junho de 2019
Capítulo LII - Aplicação da Gestão de Processos em uma Instituição Federal de Ensino Superior: 
o caso da Divisão de Orçamento da UFERSAEste estudo contemplou cinco fases. Na primeira fase apresentou-se o 
Escritório de Processos e toda a sua metodologia, mostrando aos servidores como 
interpretar os seus processos e como seria a execução do plano de trabalho.
A segunda etapa constituiu-se de reuniões com o intuito de entender o 
processo como um todo, suas entradas e saídas e como ele está sendo desenvolvido 
em cada etapa. Após as reuniões, seguiu-se com a elaboração dos fluxogramas dos 
processos.
A terceira etapa foi a reunião de análise crítica da modelagem AS IS e as 
sugestões de melhorias indicadas pelos servidores que executam o processo.
Na quarta etapa, entregou-se a modelagem TO BE, apresentando para 
a equipe que executa o processo, bem como para os pró-reitores envolvidos, 
avaliando criticamente se os diagramas representam a realidade desenvolvida no 
setor com as possíveis melhorias a serem implantadas.
Com o fluxograma validado, a quinta e última fase caracterizou-se pela 
homologação e encerramento do processo. Após o encerramento, a modelagem 
é enviada para o portal do Escritório de Processos da Universidade, onde fica 
disponível para toda a comunidade acadêmica. 
3. Resultados
Como resultado da metodologia, foi inicialmente realizada uma reunião 
com os servidores lotados na Divisão de Orçamento (DIORC) para a apresentação 
do Escritório de Processos da UFERSA e sensibilização acerca dos conceitos e 
importância da implantação da gestão de processos na instituição. Além disso, 
foram identificados os processos que perpassam este setor e que deveriam ser 
mapeados, bem como, constituído o escopo de trabalho e cronograma das 
reuniões. Como consequência dessa reunião inicial, foram identificados cinco 
processos de responsabilidade do setor e que deveriam ser mapeados na seguinte 
ordem:
- Elaboração da Proposta Orçamentária;
- Alteração da Programação Orçamentária da Despesa;
- Alteração da Programação Orçamentária da Receita;
- Distribuição e Acompanhamento Orçamentário;
- Descentralização Orçamentária para os Campi.
Dessa forma, seguindo o escopo de trabalho determinado, na segunda 
etapa foi realizada a coleta de requisitos para o primeiro processo a ser mapeado. 
Durante essa fase foram realizadas entrevistas com os servidores, observação in 
loco e análise documental. Em seguida, o servidor do EP responsável modelou o 
processo utilizando o software Bizagi Modeler, obtendo a versão AS IS e o manual 
do processo. Essa fase e as demais foram repetidas para cada um dos cinco 
processos identificados na fase inicial. página
327
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 324-329, junho de 2019
Capítulo LII - Aplicação da Gestão de Processos em uma Instituição Federal de Ensino Superior: 
o caso da Divisão de Orçamento da UFERSASeguindo a metodologia foi realizada na terceira etapa uma reunião de 
análise crítica com os servidores e os pró-reitores responsáveis, visto que, estes têm 
uma visão global dos processos e estão diretamente interessados nos resultados 
e melhorias a serem implantados. Foram ainda explanadas as atribuições dos 
servidores da DIORC e do EP , bem como, apontadas as melhorias a serem realizadas 
no processo para atendimento legal e também maior eficiência.
Na quarta etapa, foram analisadas as melhorias a serem implementadas e 
estas foram divididas em melhorias de curto e de longo prazo. As melhorias de 
curto prazo estão relacionadas a ajustes legais, adequações das planilhas de 
programação e proposta orçamentária, a elaboração de tutoriais para executar 
as ações nos sistemas do Governo Federal: Sistema Integrado de Monitoramento 
Execução e Controle (SIMEC), Sistema Integrado de Administração Financeira 
(SIAFI), Sistema Integrado de Patrimônio, Administração e Contratos (SIPAC), 
Tesouro Gerencial, entre outros. As melhorias de curto prazo foram prontamente 
implantadas no processo, ocasionando o redesenho do mesmo. Já as melhorias de 
longo prazo, as quais se referem à implantação de um sistema para levantamento 
das necessidades orçamentárias das unidades administrativas, dependem do 
cronograma de atividades do setor de desenvolvimento de TIC, logo, estas 
foram adicionadas à lista de espera do setor. Para consolidar foi realizada uma 
nova reunião com os servidores da DIORC e os pró-reitores envolvidos para que 
estes analisassem a versão TO BE do mapeamento e aprovassem as mudanças 
desenvolvidas. A Figura 1 traz o fluxo do processo de elaboração da proposta 
orçamentária na sua versão TO BE. Para informações mais detalhadas sobre a Figura 
1 consultar o link https://ep.ufersa.edu.br/wp-content/uploads/portfolioep/gof/
elaboracao/index.html#list .
Figura 1. Processo de Elaboração da Proposta Orçamentária publicado 
Por fim, na última fase, com toda a equipe reunida, o processo na sua 
versão TO BE foi homologado e foram estabelecidos indicadores para monitorar 
o desempenho dos processos, reforçando o caráter da impessoalidade e do 
profissionalismo, além de influenciar a cultura do resultado. Os principais 
indicadores identificados para o acompanhamento dos processos foram:página
328
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 324-329, junho de 2019
Capítulo LII - Aplicação da Gestão de Processos em uma Instituição Federal de Ensino Superior: 
o caso da Divisão de Orçamento da UFERSA- Percentual de atendimento dos pedidos pelo MEC;
- Quantidade de alterações orçamentárias (classificar natureza);
- Percentual de execução orçamentária para os campi;
- Percentual de execução orçamentária total;
- Percentual de pedidos internos atendidos no prazo.
Para encerrar a aplicação foi estabelecido o ciclo de melhoria contínua para 
cada processo, ou seja, a periodicidade em que os mesmos devem ser reavaliados 
para que o ciclo possa ser cumprido e mais uma vez reiniciado, acarretando em 
processos sempre atualizados. O resultado desta aplicação foi disponibilizado 
no portfólio de processos da UFERSA (https://ep.ufersa.edu.br/portfolio/), o qual 
busca dar acesso público e transparência aos processos da Universidade mapeados 
e seus documentos complementares. 
4. Conclusão
O modelo de aplicação proposto e concluído em todos os cinco processos 
elencados agregou muito valor para o setor, o qual antigamente possuía as 
atividades fortemente concentradas na diretora da divisão, que há muito tempo 
ocupa este cargo. Depois do mapeamento concluído foi possível não somente 
delegar mais atividades, pois todos os servidores passaram a conhecer o modelo 
padrão, como também o acompanhamento das atividades fora do campus sede, 
através dos indicadores estabelecidos.
Além disso, foi identificado o quanto os tutoriais fizeram a diferença na 
rotina dos servidores, pois esclareceu muitas dúvidas em relação aos sistemas 
do Governo Federal, bem como institucionalizou o conhecimento para todos os 
campi. Vale ressaltar ainda os benefícios diante de possíveis novos servidores que 
possam chegar ao setor, em termos de treinamento e execução dos processos 
envolvendo o orçamento da instituição.
Os processos foram divulgados no portal do Escritório de Processos para toda 
a sociedade, refletindo os princípios da Administração Pública de transparência 
e integridade envolvendo o orçamento e a execução dos recursos públicos da 
universidade. Sendo ainda essencial para as atividades da Auditoria Interna 
(AUDINT) e para responder os questionamentos e relatórios dos órgãos de controle 
externo.
O modelo apresenta-se na sua fase embrionária em termos de 
acompanhamento dos indicadores para aferição de resultados, considerando o 
pouco tempo de aplicação e o desconhecimento dos servidores fora do campus 
sede das atividades desenvolvidas pelo EP . Nesse sentido, visando uma ampla 
socialização desse conhecimento e buscando sanar alguma limitação própria 
dessa fase inicial, deve-se continuar a capacitação através do curso de Gestão de 
Processos e utilização do Bizagi Modeler.página
329
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 324-329, junho de 2019
Capítulo LII - Aplicação da Gestão de Processos em uma Instituição Federal de Ensino Superior: 
o caso da Divisão de Orçamento da UFERSAÉ importante ressaltar que apesar do breve tempo de constituição do 
Escritório de Processos da UFERSA suas ações têm se tornado significativas dentro 
da Pró-Reitoria de Planejamento, a qual tem dado todo o suporte para que esse 
novo modelo de gestão seja disseminado em toda a Universidade.
Referências
CATELLI, A.; SANTOS, E.S. 2004. “Mensurando a criação de valor na gestão pública” . 
Revista de Administração Pública. São Paulo, v. 38, n. 3, p. 423-449, 2004.
CASTELLANELLI, Carlo Alessandro. “Reestruturação organizacional apoiada no 
BPM (Business Process Managament): Uma perspectiva holística nas IFES” . Revistas 
Espacios, Caracas, v. 37, n. 30, p.8-10, 01 mar. 2019. Disponível em: <https://www.
revistaespacios.com/a16v37n30/16373010.html>. Acesso em: 01 mar. 2019.
FANTINATO, Marcelo. “Métodos de Pesquisa” . 2015. Disponível em: <http://each.
uspnet.usp.br/sarajane/wp-content/uploads/2015/09/Métodos-de-Pesquisa.pdf>>. 
Acesso em: 18 mar. 2018.
GIL, Antonio Carlos. “Como elaborar projetos de pesquisa” . São Paulo: Atlas, 1996.
GULLEDGE Jr., T . R.; SOMMER, R. A. “Business process management: public sector 
implications” . Business Process Management Journal, Vol. 8 No. 4, p. 364-376, 2002.página
330
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 330-335, junho de 2019
Capítulo LIII - Aplicação de BPM na gestão do TED estabelecido entre o INCRA e a UFMTAplicação de BPM na gestão do TED estabelecido 
entre o INCRA e a UFMT
Maurício F. L. Pereira1, Olivan Rabelo2, Nilton H. Takagi1, Anne C. Betoni Cardoso3, 
Josiel Maimone Figueiredo1
1Instituto de Computação – Universidade Federal de Mato Grosso
Av. Fernando Correa da Costa, 2367 – 78.060-900 – Cuiabá– MT – Brazil
2Escritorio de Inovação Tecnológica – Universidade Federal de Mato Grosso Av. Fernando Correa 
da Costa, 2367 – 78.060-900 – Cuiabá– MT – Brazil
3Escritorio de Projetos e Processos – Universidade Federal de Mato Grosso
Av. Fernando Correa da Costa, 2367 – 78.060-900 – Cuiabá– MT – Brazil
{mauricio,nilton,josiel}@ic.ufmt.br, {olivanrabelo, annebettoni}@gmail.com
Abstract
This paper presents some benefits obtained with the involvement of Project Management Office and Technological 
Innovation Office in an academic/scientific project. The offices’ objective in this work was supporting the project 
coordination to achieve his targets. To do this offices care about operational tasks e process modelling and leave the 
coordination concentrated only in their scientific questions. This kind of management can be classified as innovative 
to the university and can help to bring new projects and financial resources.
Resumo
Este trabalho apresenta os benefícios obtidos em um projeto científico em que houve o envolvimento dos escritórios 
de projetos, processos e de inovação tecnológica no apoio a sua gestão. No trabalho são apresentados os métodos 
empregados para permitir que coordenação científica pudesse concentrar maiores esforço nas questões científicas, 
deixando o apoio a gestão operacional e de modelagem dos processos a cargo dos escritórios. Esse modelo de 
envolvimento tem um caráter inovador na universidade e pode ser um ponto importante para a absorção de novos 
projetos e para a sustentabilidade da mesma.
1. Introdução
A Universidade Federal de Mato Grosso (UFMT) tem como razoes 
preponderantes de sua existência o ensino, a pesquisa e a extensão universitária. 
Com esse tripé, ela tem realizado ao longo dos anos um importante papel de 
estabelecer parcerias que viabilizem o avanço na formulação e execução de políticas 
públicas e, especialmente, a missão de produzir e disseminar o conhecimento 
nos diversos campos do saber. E para melhorar sua forma de atuação, a UFMT 
tem evoluído sua gestão através da incorporação das metodologias de gestão de 
processos e projetos de forma transversal em todas as atividades, principalmente 
no contexto da pesquisa e dos projetos científicos. Nesse contexto, em 2017 a 
UFMT estabeleceu parceria com Instituto Nacional de Colonização e Reforma 
Agrária (INCRA) para, através de uma pesquisa científica, diagnosticar os sistemas 
agrários, elaborar o Cadastro Ambiental Rural (CAR) e finalmente fomentar o 
acesso as políticas públicas para o desenvolvimento e realização de pesquisas em página
331
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 330-335, junho de 2019
Capítulo LIII - Aplicação de BPM na gestão do TED estabelecido entre o INCRA e a UFMTprojetos de assentamentos. Nesse projeto, o objetivo e obter os dados de 18.000 
famílias assentadas em dois estados da federação. Foram estabelecidas 17 metas 
que por sua vez estão subdivididas em 44étapas, que abordam os eixos ambiental, 
socioeconômico, políticas públicas e pesquisas acadêmicas.
Na universidade, o projeto e coordenado pelo Instituto de Biociências (IB) 
e para sua execução somou-se o know-how de gestão de processos e projetos 
de dois outros órgãos internos, o Escritório de Inovação Tecnológica (EIT) e o 
Escritório de Projetos e Processos (EPP). Assim, o papel dos órgãos EIT e EPP tem 
sido o de apoiar uma equipe multidisciplinar de pesquisadores e técnicos, com 
o objetivo de contribuir para que todas as metas estabelecidas no projeto sejam 
cumpridas dentro do prazo estabelecido e com um alto grau de satisfação por 
parte do INCRA. Nesse contexto EIT e EPP vêm desde o início colaborando com 
o andamento do projeto, disseminação de boas práticas de gestão é inovando 
ao trazer a aplicação de BPM [Ko et al. 2009, Van Der Aalst 2013, Ko 2009] é de 
metodologias de gerenciamento de projetos em um projeto científico. Foi realizado 
o mapeamento de processos que não eram suficientemente claros no projeto ou 
que eram de conhecimento de uma pequena parte do time do projeto, o que antes 
dificultava a discussão e a evolução desses processos. Assim o objetivo deste 
trabalho e apresentar algumas mudanças propostas pelos escritórios e como 
elas tem ocasionado impacto no projeto. Na próxima seção faz-se a apresentação 
de métodos utilizados no trabalho.
2. Métodos
A incorporação das metodologias de gestão de processos e projetos ocorreu 
pela atuação de um gerente exclusivo para o Projeto que auxiliou também na 
integração dos aspectos burocráticos com os aspectos de execução, mapeando 
as informações para diagramas na notação Business Process Model and Notation  
(BPMN) e gráficos de Gantt. Assim, inicialmente, as metas e etapas do projeto 
foram oficializadas através de um Termo de execução Descentralizada (TED) que 
foram apresentadas de forma descritiva no projeto é submetido para análise da 
demanda do INCRA. Nesse documento estavam inseridas informações diversas, 
tais como contratos entre as partes, prazos iniciais e finais de algumas atividades, 
eixos de desenvolvimento do trabalho, listas de material de consumo, dentre 
outros documentos. Uma descrição detalhada de processos chaves e de produtos 
a serem entregues era necessária para alinhamento de todo o time do que deveria 
ser realizado no prazo de 24 meses. Assim, na etapa inicial o papel dos times do EIT 
e EPP foi o de construir, a partir desse documento, a Estrutura Analítica do Projeto 
(EAP) e organizar as metas do projeto. Posteriormente com o detalhamento em 
atividades e estimando o prazo, tornou-se mais evidente quais pacotes seriam 
entregues ao INCRA e em qual momento. Também foi possível mapear a interação 
entre as metas e a dependência entre elas, uma informação que era mais difícil 
de se obter diretamente do documento do TED. Essa versão inicial foi sendo 
aprimorada ao longo do andamento do projeto, na medida que as pessoas tinham 
um melhor entendimento do que deveria ser feito. Assim, o primeiro mês de página
332
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 330-335, junho de 2019
Capítulo LIII - Aplicação de BPM na gestão do TED estabelecido entre o INCRA e a UFMTatuação foram dedicados a estruturação do planejamento relacionado a cada 
meta. Para desenvolvimento da EAP e do cronograma utilizou-se a ferramenta 
GPWeb [GPWeb 2019], de modo que o time de execução tivesse acesso on-line e 
os responsáveis por cada meta pudessem analisa-las e propor melhorias. Todas as 
alterações e atualizações no andamento do projeto ficaram a cargo do pessoal do 
EPP .
Além dessas melhorias nas documentações do planejamento e execução, 
Uma segunda ação proposta foi a forma de monitoramento das ações que culminou 
na realização de reuniões de status semanais. Nesse acompanhamento semanal, 
cada membro da equipe de pesquisadores e técnicos apresenta os acontecimentos 
relativos as metas sob sua responsabilidade. A equipe de apoio a gestão apresenta 
alguns indicadores de desempenho do projeto, tais como percentual de progresso 
de cada meta, indicadores de evolução das coletas e validação dos dados 
coletados. O controle desses indicadores foi automatizado a partir do GPWeb. Ao 
longo do projeto se convergiu para um único membro da equipe como responsável 
por informar o progresso de cada meta no GPWeb sendo capturado o status em 
reuniões específicas. Com o andamento do projeto, detectou-se três metas de 
maior relevância ao INCRA, sendo elas:
1. As atividades realizadas para coleta de dados nos lotes distribuídos em 
projetos de assentamentos indicados pelo INCRA;
2. As atividades ligadas a liberação de crédito para os assentados, que 
permitia aos beneficiários obter recursos para fazer as melhorias em suas 
propriedades ou meios de produção;
3. A elaboração do projeto de software para supervisão ocupacional dos 
lotes.
Mesmo com o detalhamento do TED, desenvolvendo a EAP e o cronograma, 
ainda não estava claro o ciclo, as fases e as iterações entre as atividades do projeto. 
Muito do conhecimento sobre o projeto estava de forma tácita em parte da equipe, 
era necessário deixar explícito para uniformizar o entendimento e poder evoluir 
a forma de trabalho. O papel da equipe de gestão, nesse momento, foi elucidar 
esses processos com os responsáveis pelas metas e mapear o processo AS-IS. 
Essa definição também incluiria desenvolvimento do software de supervisão 
ocupacional previsto no TED e que ainda estava em planejamento. Para o 
mapeamento em BPM utilizou-se a ferramenta Bizagi Modeler. Para cada uma das 
metas chaves escolhidas para o mapeamento foram organizadas 3 reuniões. Nas 
reuniões houve a participação dos indivíduos da equipe com mais expertise sobre 
as atividades que seriam realizadas para se atingir as metas. Com isso, foi possível 
entender o papel que cada um realizava visando detalhar como os processos 
ocorrem atualmente (As-Is).página
333
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 330-335, junho de 2019
Capítulo LIII - Aplicação de BPM na gestão do TED estabelecido entre o INCRA e a UFMT3. Resultados
Um resultado importante para o projeto foi a organização e aprimoramento da 
EAP desmembrando as metas e seus produtos. Uma vez elaborada, ela colaborou 
sobremaneira para o entendimento do escopo do projeto e para o planejamento 
das atividades durante as reuniões de status. O uso do GP-Web no apoio a gestão 
deu a equipe uma ferramenta importante de avaliação do andamento das metas 
direcionamento nas reuniões de status.
Baseado na EAP , em centenas de formulários aplicados e nas 
atividades de coleta, validação e disponibilização de dados, organizou-se um 
macroprocesso com 4 etapas que corresponde ao ciclo de entrega ao INCRA. 
Esses macroprocessos correspondem as etapas necessárias para a aceitação 
das entregas. Desses macroprocessos, 3 foram detalhados em processos e sub-
processos. O macroprocesso “Organizar mobilização no assentamento” era de 
responsabilidade de uma subcontratante e já estava devidamente claro no TED. 
Das 18 mil famílias previstas no TED, o INCRA segregava em listas de famílias e 
mapas dos lotes que deveriam estar em cada ciclo. Tanto os macroprocessos e 
seus desdobramentos foram modelados utilizando-se a notação BPMN que depois 
de finalizados e validados foram disponibilizados no portal do projeto para que 
todos os stakeholders tivessem acesso. Na Figura 1 pode ser visualizado parte da 
modelagem realizada.
Figura 1. Macroprocessos de coleta de dados em campo do TED modelados pelo 
escritório de projetos da universidade
Para a segunda meta principal do projeto do projeto, foram modeladas as 
atividades relacionadas a preparação de projetos de crédito para implantação 
de melhorias na infraestrutura dos lotes. Os projetos só são desenvolvidos para 
os lotes cujos beneficiários estão em situação regular junto ao INCRA. Essa meta 
tem como produto a elaboração de 7.200 projetos de credito, contudo no TED 
não se tramitam esses projetos até a aprovação do INCRA. Assim, juntamente 
com responsável por essa meta, a equipe EIT/EPP conduziu o levantamento dos página
334
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 330-335, junho de 2019
Capítulo LIII - Aplicação de BPM na gestão do TED estabelecido entre o INCRA e a UFMTrequisitos, dos produtos a serem entregues e a modelagem do processo desde a 
verificação das demandas do beneficiário até a entrega do projeto finalizado para 
liberação de crédito pelo INCRA. Nessa meta os técnicos de campo elaboram o 
projeto técnico da melhoria. Apôs 3 reuniões de discussão disponibilizou-se 
um documento de orientação aos técnicos de campo, a modelagem em BPMN 
das atividades que devem ser realizadas e a forma de interação com o INCRA. A 
Figura 2 apresenta um dos documentos em notação BPMN desenvolvido. No caso 
da terceira meta, do desenho de software de supervisão ocupacional houve a 
participação da equipe de Tecnologia da informação do projeto e a presença de 
técnicos do INCRA. Foi realizado o levantamento de requisitos funcionais e não 
funcionais do sistema, o desenho de telas do mesmo e a organização da base de 
dados. Este será um sistema alimentado por base externas e no projeto se propôs 
um modelo de como integrar essas diferentes bases
Figura 2. Modelagem BPMN do processo de liberação de credito
4. Conclusão
Os projetos científicos possuem, em geral, a característica de ter um 
coordenador responsável por diversos aspectos, como orientação de alunos, 
compra de equipamentos, definição de prazos, preparação de artigos, dentre 
outras atividades. O que se observa nesse projeto, que apesar do caráter científico 
houve uma inovação na forma de gerenciamento das metas, pois houve a inserção 
de equipes especializadas em gestão para apoio a equipe de pesquisadores, 
liberando-os para se dedicarem a sua área de conhecimento. Tendo em vista que 
essa equipe e multidisciplinar, com agrônomos, engenheiros, geógrafos, dentre 
outras, há uma um aumento da complexidade para o gerenciamento de atividades 
do projeto. Assim, e importante destacar que foi possível a coordenação científica 
concentrar o seu maior esforço nas questões científicas, pois havia a equipe EIT/
EPP para apoiar na gestão operacional do projeto e na modelagem dos processos. 
Isso tem auxiliado no sucesso projeto, algo que tem sido reconhecido pelo INCRA 
em suas avaliações sobre o trabalho.
O uso de ferramentas de gestão tem obtido um papel importante para 
evolução do trabalho, pois as metas são avaliadas semanalmente e a equipe do página
335
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 330-335, junho de 2019
Capítulo LIII - Aplicação de BPM na gestão do TED estabelecido entre o INCRA e a UFMTescritório sempre realiza intervenções que visem estimular o andamento das 
metas que estão em atraso. O escritório também tem colaborado na documentação 
de entregas de produtos, de modo a evitar conflitos ou discussões a respeito do 
escopo do trabalho. E o mapeamento em BPMN foi e está sendo essencial para 
disseminar e evoluir o conhecimento sobre o projeto, ponto importante para 
absorver novos projetos e para a sustentabilidade da atuação da Universidade na 
sociedade como um todo.
Referencias
GPWeb (2019). Gpweb - grandes planos. Disponível e m 
https://softwarepublico.gov.br/social/gpweb.
Ko, R. K., Lee, S. S., and Wah Lee, E. (2009). Business process management (bpm) 
standards: a survey. Business Process Management Journal, 15(5):744–791.
Ko, R. K. L. (2009). A computer scientist’s introductory guide to business process 
management (bpm). XRDS, 15(4):4:11–4:18.
Van Der Aalst, W. M. (2013). Business process management: a comprehensive survey. 
ISRN Software Engineering, 2013.página
336
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 336-341, junho de 2019
Capítulo LIV - Execução de Melhoria Contínua Baseado em BPMN e Indicadores na Gestão de Serviços 
de TIC:Estudo de caso na Diretoria de Sistemas do NTI/UFPEExecução de Melhoria Contínua Baseado em BPMN 
e Indicadores na Gestão de Serviços de TIC: 
Estudo de caso na Diretoria de Sistemas do NTI/
UFPE
Renato V. Mendes1, Suzanna S. Dantas2
1,2 Núcleo de Tecnologia da Informação - Universidade Federal de Pernambuco (UFPE) Recife - PE 
- Brasil
{renato.mendes, suzanna.sandes}@ufpe.br
Resumo
Este trabalho investigou o uso de gestão de processos de negócios em um ambiente de serviços de tecnologia de 
informação e comunicação (TIC), para obtenção de melhores resultados de desempenho. Um estudo de caso foi 
realizado no Núcleo de Tecnologia da Informação da UFPE. Utilizando abordagens como BPMN e gestão por 
indicadores em um processo de melhoria contínua foi possível melhorar os resultados à medida que novos ciclos de 
melhoria eram realizados.
Palavras-chave: Gerenciamento de Processos de Negócio; BPMN; Indicadores; Melhoria Contínua; Gerenciamento 
de Serviços de TIC
1. Introdução
Um objetivo comum a todas as organizações é fornecer um produto/serviço 
para satisfazer as necessidades de seus clientes, agregando valor aos seus serviços 
junto à organização. Para empresas privadas, essa geração de valor resulta 
principalmente em lucro e crescimento da empresa. Já para organizações públicas 
e sem fins lucrativos o foco é no fornecimento deste produto ou serviço com a 
melhor qualidade e o menor custo possível. No entanto, independente do tipo de 
organização, o principal ponto de sucesso organizacional é maximizar o retorno 
sobre o investimento realizado [Mendes, 2018].
Porém, as organizações precisam passar por diversas mudanças 
organizacionais para alcançar seus resultados. Nesse contexto, a gestão de 
processos de negócio (Business Process Management – BPM), por meio da definição, 
desenho, controle e transformação contínua de processos de negócio ajudam as 
organizações a alcançarem seus objetivos [ABPMP , 2013].
A gestão de processos de negócio demanda uma definição e monitoramento 
contínuo de indicadores para garantir que os objetivos organizacionais sejam 
alcançados. Tais indicadores são necessários para o controle desses processos, 
pois “possibilitam o estabelecimento de metas quantificadas” , essenciais “para a 
análise crítica do desempenho da organização, para o processo decisório e para o 
replanejamento” [TAKASHINA e FLORES, 1996].página
337
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 336-341, junho de 2019
Capítulo LIV - Execução de Melhoria Contínua Baseado em BPMN e Indicadores na Gestão de Serviços 
de TIC:Estudo de caso na Diretoria de Sistemas do NTI/UFPECada vez mais a realidade das organizações é marcada pela grande 
dependência por recursos de Tecnologia da Informação e Comunicação (TIC). 
Esse fato exige que a Gestão de Serviços de TIC esteja presente no dia a dia das 
empresas. Esta Gestão visa alocar de forma adequada os recursos disponíveis 
e gerenciá-los de modo integrado, fazendo com que a qualidade dos serviços 
seja percebida pelos clientes da organização, evitando problemas na entrega e 
operação de serviços de TIC [Mendes, 2018].
A Universidade Federal de Pernambuco (UFPE), é uma instituição de educação 
superior de ensino, pesquisa e extensão, sediada na cidade de Recife. Trata-se 
de uma autarquia educacional, mantida pela União e vinculada ao Ministério de 
Educação (MEC) que possui um público de aproximadamente 60 mil pessoas, entre 
alunos, professores e servidores técnicos-administrativos. Este público é usuário 
de um ou mais serviços de TIC oferecidos pela universidade. No contexto da UFPE, 
o Núcleo de Tecnologia da Informação (NTI) tem como principal função prover 
serviços de TIC que apoiem as atividades de ensino, pesquisa, extensão e gestão da 
UFPE e que sejam relacionados aos Sistemas Corporativos, Redes, Equipamentos 
de TIC e Softwares, Telefonia, E-mail, Aquisições de Bens e Serviços de TIC, Centro 
de Dados, Sites e Portais.
Em 2013, foi constatado um cenário caótico na prestação de serviços de TIC 
da UFPE. Os problemas causados pela forma como os serviços eram prestados 
causavam uma grande insatisfação nos clientes, tais como: diversos pontos de 
entrada para a solicitação das demandas de TIC, ausência de controle unificado 
dos registros dos chamados, grande dependência de profissionais especializados, 
desempenho inconsistente das equipes, ausência de um catálogo de serviços 
formalizado, ausência de acordos de nível de serviços e ausência de indicadores 
de qualidade e satisfação dos usuários.
Após a constatação desse cenário, foram feitos vários estudos e pesquisas 
sobre como resolver o problema e melhorar a qualidade dos serviços de TIC. A 
pesquisa resultou na implantação de uma Central de Serviços de TIC (CSTIC) 
unificada do NTI, tendo como projeto piloto a implantação nos serviços prestados 
pela Diretoria de Sistemas de Informação (DSIS) do órgão.
2. Métodos
O fluxo de atividades deste trabalho seguiu as seguintes etapas, de acordo 
com a figura 1:
• Definição de estratégia: Com base no diagnóstico é planejada a estratégia 
para resolução dos problemas identificados.
• Adequação do processo: A partir da estratégia definida é redesenhado 
o processo de acordo com as mudanças do ambiente e com o intuito de 
solucionar as falhas identificadas no diagnóstico.
• Implantação: Neste passo é colocado em uso efetivo a nova versão do 
processo.página
338
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 336-341, junho de 2019
Capítulo LIV - Execução de Melhoria Contínua Baseado em BPMN e Indicadores na Gestão de Serviços 
de TIC:Estudo de caso na Diretoria de Sistemas do NTI/UFPE• Identificação de indicadores: Os indicadores de desempenho do processo 
são definidos com o intuito de verificar a efetividade do processo.
• Monitoramento: Neste momento os indicadores de desempenho e as 
mudanças do ambiente são monitorados com intuito de averiguar os 
resultados e a necessidade de melhorias, seja por mudanças no ambiente ou 
não alcance dos resultados esperados.
• Análise de resultados: Nessa etapa os itens/artefatos do monitoramento 
são avaliados quanto ao atendimento/alcance dos resultados esperados.
 • Melhoria do processo: Com o resultado da análise dos resultados alcançados 
e de acordo com o novo objetivo, são feitas melhorias no processo, 
considerando também as mudanças ocorridas no ambiente.
Figura 1 - Fluxo de atividades do trabalho
3. Resultados
3.1 Definição da Estratégia
Para a definição da estratégia foi montado um grupo de trabalho com 
participantes de diversas áreas do NTI. Como resultado foram acordadas algumas 
definições quanto ao novo processo, além da necessidade da criação do catálogo 
de serviços prestados pela DSIS e a necessidade da migração de chamados em 
abertos na ferramenta independente da DSIS para a nova ferramenta unificada do 
NTI, o OTRS.página
339
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 336-341, junho de 2019
Capítulo LIV - Execução de Melhoria Contínua Baseado em BPMN e Indicadores na Gestão de Serviços 
de TIC:Estudo de caso na Diretoria de Sistemas do NTI/UFPE3.2 Adequação do processo
Com a estratégia definida, um grupo de trabalho composto por membros da 
DSIS passou a elaborar o novo processo. A elaboração ocorreu através de encontros 
regulares (normalmente uma vez por semana), onde o líder do grupo apresentava 
o resultado das discussões da reunião anterior e facilitava as discussões. O 
resultado de cada reunião era registrado através da atualização do processo, que 
serviria de entrada para a próxima reunião. Após várias iterações desse processo 
de trabalho, o que durou alguns meses, o grupo chegou a um consenso sobre o 
processo de atendimento.
3.3 Primeiro Ciclo
A implantação da primeira versão ocorreu em 14 de dezembro de 2015. Para 
este momento foram treinadas as equipes de TIC e os usuários foram orientados 
quanto ao uso da nova ferramenta. Além disso foi realizada a migração de chamados 
para o OTRS, migrando apenas aqueles que ainda não tivessem concluídos. No 
final foram migrados aproximadamente 190 chamados (desconsiderando, para 
simplificar o processo, as datas originais de abertura dos chamados e o histórico, 
mas registrando a referência para o chamado original).
Com as mudanças no processo e na ferramenta de registro de demandas 
e sem um acompanhamento em tempo real de indicadores, foi priorizado o 
acompanhamento dos indicadores relativos ao quantitativo de chamados abertos, 
chamados fechados e chamados ainda em atendimento, agrupados por mês.
Durante os primeiros 8 meses de implantação, percebeu-se que o quantitativo 
de chamados permanecia em aproximadamente 200 em atendimento. Mesmo 
ocorrendo mudanças estruturais na diretoria, unificando em um único setor o 
atendimento dos serviços de modo a diminuir o gargalo enfrentado pela existência 
de mais de uma coordenação nas tomadas de decisão, não se percebeu melhoras 
nos indicadores de quantitativos de chamados.
Em uma análise mais aprofundada, percebeu-se que a equipe não estava 
seguindo corretamente o processo definido e ainda tinham dificuldades quanto 
ao uso da nova ferramenta. Um fato percebido sobre isso era que os chamados 
acabavam sendo esquecidos, aumentando cada vez mais o tempo de resolução 
(Leadtime). Adicionalmente, a falta de resposta por parte dos demandantes causava 
também aumento no leadtime. Como a equipe falhava em seguir o processo, os 
chamados sem retorno começaram a acumular com o passar dos meses.
O processo passou por ajustes para garantir que os chamados sem respostas 
ou os solicitados por usuários sem autorização para tal serviço, fossem fechados o 
quanto antes. Complementar a isto adequou-se a ordem das atividades de acordo 
com feedbacks da equipe e passou-se a contemplar a existência de fornecedores 
externos. Para garantir uma maior adesão ao processo, foram definidas novas página
340
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 336-341, junho de 2019
Capítulo LIV - Execução de Melhoria Contínua Baseado em BPMN e Indicadores na Gestão de Serviços 
de TIC:Estudo de caso na Diretoria de Sistemas do NTI/UFPErodadas de apresentação do processo e treinamentos de uso do OTRS, além de 
um acompanhamento por parte da área de qualidade em relação a aderência a 
nova versão do processo.
3.4 Segundo Ciclo
A segunda versão do processo foi implantada em outubro de 2016 com o 
objetivo de melhorar os resultados relacionados ao quantitativo de chamados 
e nos tempos de resolução. Foram adicionados como indicadores a serem 
acompanhados na nova versão do processo: o tempo de resolução de chamados 
(leadtime) médio e o tempo em cada time (cycletime).
Além dos indicadores de desempenho do processo, foi decidido o 
acompanhamento do indicador de aderência ao processo, sendo monitorados 
através do OTRS o registro das atividades previstas no processo. Com as ações de 
capacitações previstas, conseguimos atingir ao final de dezembro de 2016, uma 
adesão ao processo de aproximadamente 90%.
Na sequência o foco foi redirecionado à melhoria no desempenho, apoiado 
por um monitoramento em tempo real dos indicadores de desempenho, 
facilitando o acompanhamento da gestão e a tomada de ações para o alcance 
dos objetivos definidos. Como resultado das ações realizadas, foi possível uma 
redução significativa no quantitativo de chamados em atendimento, que caíram 
em aproximadamente 50%, e no tempo médio para resolução, que após atingir o 
ápice de 93 dias em outubro de 2016, reduziu para aproximadamente 17 dias em 
junho de 2017.
 No entanto, 17 dias ainda era um tempo longo para resolução dos chamados. 
Durante a análise dos resultados alcançados foi percebido que membros da equipe 
ficavam com tempo ocioso, seja esperando receber uma nova demanda, seja 
aguardando retorno do cliente. Para melhorar os resultados obtidos, a subdivisão 
da coordenação em times especialistas (atendimento, dados e manutenção) foi 
desfeita, ficando apenas um pequeno time de apoio aos testes de software, além 
de outros pequenos ajustes. Com isso todos os analistas passaram a atender todos 
os tipos de demandas, sem haver uma divisão obrigatória entre análise de negócio, 
manutenção de sistemas ou banco de dados.
3.5 Terceiro Ciclo
Através de uma reunião com toda equipe as mudanças foram apresentadas 
e validadas. Com isso a nova versão do processo foi implantada no início de 
outubro de 2017, com a meta de reduzir a fila de chamados em atendimento para 
no máximo 30 chamados e diminuir o tempo médio de resolução para 4 dias úteis.
As mudanças realizadas mostraram novamente uma melhora nos indicadores. 
O tempo médio de atendimento reduziu logo nos primeiros meses, se estabilizando página
341
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 336-341, junho de 2019
Capítulo LIV - Execução de Melhoria Contínua Baseado em BPMN e Indicadores na Gestão de Serviços 
de TIC:Estudo de caso na Diretoria de Sistemas do NTI/UFPEde acordo com o objetivo a partir de maio de 2018. Até novembro de 2018, onde os 
dados foram acompanhados, o quantitativo de chamados em atendimento ainda 
não atingiu o objetivo, apesar de ter se aproximado em alguns meses. No entanto 
a redução ainda foi boa, tendo alcançado uma redução de aproximadamente 50% 
em relação à versão anterior.
4. Conclusão
As constantes mudanças das organizações, sejam elas referentes a mudanças 
de legislações, busca por melhores resultados, adequação a novas estratégias, 
entre outras, têm exigido maior agilidade dos seus gestores para enfrentá-las. 
Nesse contexto o BPM tem se destacado como meio para trazer o alinhamento 
da estratégia organizacional às necessidades dos clientes através de uma gestão 
focada em processos.
Adicionalmente, o uso de BPMN para facilitar a comunicação dos stakeholders, 
um monitoramento de indicadores de desempenho para garantir a efetividade e 
consequentemente subsídio para a adequação dos processos organizacionais, 
permitem às organizações a melhoria contínua de seus processos.
Foi através da adoção dessas abordagens, executando ciclos de melhoria 
contínua baseado no PDCA, que o NTI da UFPE tem conseguido melhorar os 
resultados na prestação de serviços, como abordado neste artigo.
Referências
Mendes, Renato V. (2018) “Execução da Melhoria Contínua de Processos Baseados em 
BPMN em um Instituição Pública”
ABPMP . Guia para o Gerenciamento de Processos de Negócio (BPM CBOK). Corpo 
Comum de Conhecimento. 1ª edição, 453 p. 2013.
TAKASHINA, N.; FLORES, M. (1996) “Indicadores da qualidade e do desempenho: como 
estabelecer metas e medir resultados” , Rio de Janeiro: Quality Mark.página
342
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 342-347, junho de 2019
Capítulo LV - Grau de Maturidade em Processos: um estudo da evolução da Gestão por Processos na 
UFJFGrau de Maturidade em Processos: um estudo da 
evolução da Gestão por Processos na UFJF
Fábio Silva de Figueiredo1, Leonardo Ciuffo1, Alcimar Honório1, Wagner Ramalho1 
1Escritório de Processos – Universidade Federal de Juiz de Fora (UF JF)
Juiz de Fora – MG – Brasil – 36.240-000
{fabio.figueiredo, Leonardo.ciuffo, alcimar.honorio, wagner.ramalho}@ufjf.
edu.br 
Resumo
Tendo por base uma pesquisa de natureza qualitativa-quantitativa, este relato de experiência busca avaliar o 
quanto a Universidade Federal de Juiz de Fora evoluiu em seu modelo de maturidade em processos, a partir da 
criação do Escritório de Processos (EP). Os resultados demonstram que a estruturação de ciclos de maturidade 
orientou o processo evolutivo do EP e que esse prosperou nas metas inicialmente propostas, tornando-se referência à 
Administração da instituição, o que resultou na mudança do escopo de atuação do EP.
1. Introdução
A necessidade de se responder mais eficientemente às demandas da 
sociedade por uma universidade pública de qualidade, tem feito com que a direção 
dessas instituições busque, cada vez mais, ferramentas e técnicas de gestão que 
sejam capazes de responder efetivamente a essa demanda. 
A adoção da Gestão por Processos pode representar um instrumento 
significativo nesse contexto – na medida em que busca, dentro de seu conceito 
mais básico, reunir um conjunto de recursos para realizar uma atividade a contento 
e que agregue valor para o cliente (TREGEAR; JESUS; MACIEIRA, 2010).
Assim, com o crescimento e o sucesso das ações de BPM – Business Process 
Management, na Universidade Federal do Rio Grande do Sul, conforme menciona 
Viera, et al. (2016), somado as experiências de sucesso que outras Universidades 
Federais obtiveram nos últimos anos, a área de processos surgiu como alternativa 
viável à obtenção de melhores resultados na gestão dessas instituições, sobretudo 
por guardar forte correlação com a qualidade dos serviços e o foco no atendimento 
ao cidadão.  
Nesse contexto, com o objetivo de implantar a Gestão por Processos na 
instituição, a Universidade Federal de Juiz de Fora, a partir de 2016, estruturou 
seu Escritório de Processos (EP), de forma a contribuir mais efetivamente para a 
desburocratização de procedimentos administrativos, modernização e automação 
de processos. 
Para tanto, a UF JF valeu-se de um Modelo de Governança de Processos – 
MGOP (BRASIL, 2016), que fundamentou e orientou a implantação de práticas página
343
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 342-347, junho de 2019
Capítulo LV - Grau de Maturidade em Processos: um estudo da evolução da Gestão por Processos na 
UFJFcoordenadas de Gestão por Processos na instituição. Tal modelo, além de ter 
estabelecido o Ciclo da Gestão por Processos e a sua estratégia de implementação, 
estabeleceu também ciclos de maturidade em BPM como forma de alavancar a sua 
estratégia de implantação da Gestão por Processos.
Sendo assim, diante desse modelo de maturidade, é fundamental para os 
membros do Escritório de Processos e também para a própria instituição (principal 
patrocinadora desse projeto) avaliar o quanto se avançou na Gestão por Processos, 
tanto qualitativamente quanto quantitativamente e se as metas inicialmente 
propostas foram alcançadas, sendo esse o objetivo do presente estudo. 
Ressalte-se, por fim, que mais importante que demonstrar o desempenho do 
setor na condução das políticas de gestão por processos, tal avaliação auxiliará 
os gestores e demais membros da área a continuar conduzindo essas políticas 
de forma permanente e dinâmica, integrando-as aos objetivos estratégicos da 
organização, a fim de alcançar o estado de gerenciamento proativo de processos1 
tal como definido pelo CBOK (BPM, 2013).  
2. Maturidade em Processos
A maturidade em processos é estabelecida por características que definem o 
estado atual de uma organização na gestão de seus processos, ou seja, é o “ponto 
no qual os processos são explicitamente definidos, administrados, medidos, 
controlados e otimizados” (BPM, 2013, p. 346).
Uma organização madura em seus processos está mais bem preparada para 
controlar riscos e problemas por meio de uma visão compartilhada, linguagem 
comum, objetiva e, principalmente, baseada em indicadores e práticas em 
contínuo processo de melhoria (BPM, 2013).
Assim, avaliar a maturidade em processos consiste basicamente em medir o 
quanto os processos da organização são explicitamente definidos, gerenciados, 
medidos e controlados de forma que os objetivos organizacionais sejam atingidos 
consistente e eficientemente (SIQUEIRA, 2005). 
Entretanto, para se obter melhorias relevantes e sustentáveis na gestão dos 
processos organizacionais é imprescindível que a organização conheça com clareza 
o seu estágio atual de maturidade e a consequente capacidade de absorção de 
novas técnicas de gestão e melhoria de desempenho. Muitas iniciativas fracassam 
em decorrência de objetivos confusos e irrealistas ou da escolha de técnicas 
incompatíveis com o estágio de maturidade gerencial da organização (SIQUEIRA, 
2005). 
Portanto o uso de modelos para avaliar a maturidade em processos oferece 
suporte a uma “estratégia de gerenciamento e definição de processos como um 
1 Quinto e último nível da curva de maturidade em processos definido pela ABPMP – Association of Business 
Process Management Professionals. As organizações que atingem esse estágio de maturidade em processos 
têm suas capacidades de negócios amplamente amadurecidas e implementadas por meio de uma sistemática 
contínua de planejamento e gerenciamento de seus processos (BPM, 2013).página
344
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 342-347, junho de 2019
Capítulo LV - Grau de Maturidade em Processos: um estudo da evolução da Gestão por Processos na 
UFJFroteiro mostrando o estado atual de processos e um plano para melhorá-los. ” As 
avaliações podem também ser prescritivas e “ajudar a organização na criação de 
planos de ação ou um roteiro geral de BPM” (BPM, 2013, p. 345-346).
3. Contextualização e Estudo de Caso
O setor, objeto do presente estudo, é o Escritório de Processos da Universidade 
Federal de Juiz de Fora – EP . Subordinado à Pró-reitoria de Planejamento – 
PROPLAN, foi instituído em janeiro de 2016, com a missão de implantar a Gestão 
por Processos na UF JF, tendo, para isso, estabelecido ciclos de maturidade em 
processos a serem alcançados. Cada um dos ciclos foram previstos para um 
período que varia de 6 meses (1º ciclo) a 1,5 ano (2º e 3º ciclos) a partir da efetiva 
formalização do EP , conforme se pode observar no quadro seguinte:
Quadro 1: Ciclos de maturidade em BPM da UFJF
1º ciclo de 
maturidade 
em BPM- Ações de benchmarking em BPM;
- Capacitação inicial da equipe do Escritório de Processos em BPM e Modelagem de 
processos;
- Elaboração do Modelo de Governança de Processos;
- Estruturação do Escritório de Processos;
- Definição do Processo de Mapeamento e Modelagem de Processos na UF JF.
2º ciclo de 
maturidade 
em BPM- Modelagem e Análise de processos prioritários com vistas à automação pelo SIGA (Sistema 
Integrado de Gestão Acadêmica);
- Verificação de possibilidades para ampliação do escopo de atuação do Escritório de 
Processos;
- Publicação do portfólio de processos;
- Ações para fomento da cultura BPM na UF JF;
- Ações de treinamento em BPM na UF JF.
3º ciclo de 
maturidade 
em BPM- Capacitação dos gestores dos processos em BPM e Modelagem de Processos;
- Gestão de Portfólio de Processos: utilização de ferramentas de gestão integrada de 
processos;
- Implementação do Comitê de Integração em Processos;
- Gestão do dia-a-dia e dos riscos dos processos (de pessoas, de processos e de tecnologia);
- Gestão de desempenho / construção do painel de indicadores corporativos;
- Implantação da agenda de melhoria contínua de processos e ciclos de revisão (de forma 
permanente e por tempo indeterminado).
Para consecução de seus objetivos, o EP conta com uma estrutura de gestão 
baseada em Projetos, na qual, diante de uma nova demanda, um novo projeto 
é aberto definindo-se o escopo, cronograma, entregas, stakeholders, equipe e o 
gerente do projeto. Em princípio, essas demandas eram espontâneas, ou seja, os 
setores clientes (órgãos internos à UF JF) demandavam serviços ao EP na medida 
de suas necessidades e o EP as atendia de acordo com a sua própria capacidade 
operacional e a anuência do Pró-reitor de Planejamento.
Neste modelo, para que o EP aceitasse uma nova demanda, era necessário 
que o cliente informasse a aderência do projeto ao planejamento estratégico da 
universidade, por meio do Plano de Desenvolvimento Institucional (PDI). Somente 
após isso, a demanda seguia para aprovação da PROPLAN. 
Esse modelo vigorou até meados de 2017, quando, por ordem direta do Pró-página
345
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 342-347, junho de 2019
Capítulo LV - Grau de Maturidade em Processos: um estudo da evolução da Gestão por Processos na 
UFJFreitor de Planejamento, as demandas passaram a ser estratégicas, emanadas 
diretamente da PROPLAN. Essa mudança de perspectiva estreitou o relacionamento 
entre EP e PROPLAN, viabilizando uma atuação mais próxima da Administração 
Superior.
4. Análises e Discussões
Desde a sua criação, em janeiro de 2016, o EP já mapeou 484 processos na 
instituição, sendo 53 em 2016, 180 em 2017 e 251 em 2018. Para dar conta de tal 
produtividade, foram abertos 27 projetos ao longo desses 3 anos, sendo 16 em 
2016, 5 em 2017 e 6 em 2018.
No que concerne às automações de processos realizadas pelo setor de 
Tecnologia da Informação (TI), em parceria com o EP , foram construídos 12 novos 
sistemas (ou módulos de sistemas), sendo 7 em 2016, 1 em 2017 e 4 em 2018. O 
gráfico a seguir traz, resumidamente, a projeção desses dados nos últimos 3 anos 
de atividade do EP:
Figura 1: Dados consolidados do EP – período 2016 a 2018
Conforme se observa, a produtividade do setor cresceu a uma taxa de 239% 
em 2017 e 39% em 2018, entretanto, o número de projetos abertos no setor caiu 
(em relação a 2016), bem como o número de novos sistemas (ou módulos de 
sistemas) construídos que caiu no segundo ano e teve um pequeno aumento em 
2018, sem, entretanto, alcançar o patamar do primeiro ano de atuação.
Esses números, associados a uma análise qualitativa da atuação do EP 
revelam um crescimento na maturidade em processos na instituição. A redução 
do número de projetos abertos, ligado ao aumento do número de processos 
mapeados, aponta para um maior aprofundamento das ações do EP em seus 
projetos, revelando uma ampliação do seu escopo de atuação, conforme orienta o 
2º ciclo de maturidade em processos.página
346
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 342-347, junho de 2019
Capítulo LV - Grau de Maturidade em Processos: um estudo da evolução da Gestão por Processos na 
UFJFEm projeto realizado em 2017, por exemplo, dentre as proposições de 
melhorias apresentadas ao setor cliente, estava a completa reestruturação do 
setor, a partir de uma estrutura matricial orientada a processos (FIGUEIREDO, et 
al., 2018), além de outras proposições com vistas ao aumento de qualidade de 
atendimento ao cliente.
De igual modo, em recente projeto aberto em 2018, o setor cliente encomendou 
ao EP , além do mapeamento e modelagem de processos com as consequentes 
automações que podem advir desse trabalho, um estudo aprofundado de 
racionalização administrativa, gestão de riscos e reestruturação do setor.
Por fim, ressalte-se que, para 2019, já está previsto, em programa de avaliação 
de desempenho da UF JF, a gestão de portfólio de processos e a implantação da 
agenda de melhoria contínua de processos e ciclos de revisão de processos já 
mapeados (conforme orienta o 3º Ciclo de maturidade em processos). 
5. Considerações Finais
Sem negligenciar a automação de processos, altamente relevante na atuação 
do EP e fator decisivo na estratégia de implantação da área de processos na UF JF 
(FIGUEIREDO, et al., 2017), verificou-se uma ampliação do escopo de atuação 
do EP , para estudos de racionalização, reestruturação administrativa e soluções 
inovadoras nos processos de negócio.  
Esse aprofundamento, de viés mais estratégico, ocorreu tanto em virtude da 
expertise adquirida pelos Analistas de Processos a partir das ações de capacitação 
em BPM que, apesar de estarem circunscritas ao 2º ciclo de maturidade, “na prática, 
são tidas como perenes e essenciais por todos os membros” (FIGUEIREDO, et al., 
2017, p. 5); como também pela percepção de espaço existente para a ampliação da 
perspectiva de automação para uma atuação mais voltada ao âmbito gerencial-
estratégico da UF JF (tal como definido a partir do 2º Ciclo de Maturidade em 
Processos). 
Como se verificou, essa modificação de paradigma, acompanhou as diretrizes 
dos ciclos de maturidade em BPM da UF JF, sendo que as ações do 3º ciclo de 
maturidade, especificamente, as de gestão de portfólio e implantação de agenda 
de melhoria contínua, como relatado, estão sendo introduzidas.
Finalmente, observa-se que, na medida em que a maturidade em processos 
da organização evolui, o EP tende a mudar de papel. A própria organização encontra 
novas necessidades de mudança em suas habilidades gerenciais e na própria 
cultura, pois ganha experiência no gerenciamento de processos e passa a necessitar 
de integrar novas capacidades e habilidades. O EP passa então a se tornar referência 
à Administração e passa a ser reconhecido como centro de excelência, fornecendo 
conhecimento, recursos, padrões, boas práticas, capacitação e as competências 
técnicas adequadas para serem alocadas e distribuídas nos processos de negócio 
da organização. página
347
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 342-347, junho de 2019
Capítulo LV - Grau de Maturidade em Processos: um estudo da evolução da Gestão por Processos na 
UFJFReferências
BPM CBOK (2013) “Guia para o Gerenciamento de Processos de Negócio: corpo comum 
de conhecimento” , versão 3.0, 1 ed., 440p.
BRASIL. Ministério da Educação. Universidade Federal de Juiz de Fora (2016) “Modelo 
de Governança de Processos – Escritório de Processos (EP)” , Juiz de Fora, MGOP , 18p.
FIGUEIREDO, F. S.  et al. (2017) A Trajetória do Surgimento da Área de Processos: o 
estudo da implantação do Escritório de Processos em uma Instituição Federal de 
Ensino Superior. In: XI Workshop de Tecnologia da Informação e Comunicação das 
Instituições Federais de Ensino Superior, mai. 2017, Recife, PE.
FIGUEIREDO, F. S.  et al. (2018) Criando Estruturas Organizacionais Orientadas por 
Processos: o estudo e caso da Pró-reitoria de Gestão de Pessoas da UF JF. In: XII 
Workshop de Tecnologia da Informação e Comunicação das Instituições Federais de 
Ensino Superior, mai. 2018, Foz do Iguaçu, PR.
PORTER, M. (1989) “Vantagem Competitiva: criando e sustentando um desempenho 
superior” , Rio de janeiro: Campus.
SIQUEIRA, J.  (2005) “O Modelo de Maturidade de Processos: como maximizar o 
retorno dos investimentos em melhoria da qualidade e produtividade” , IBQN, Brasil. 
Disponível em: <http://www.ibqn.com.br> Acesso em 30 jan. 2019.
TREGEAR, R.; JESUS, L.; MACIEIRA, A. (2010) “Estabelecendo o Escritório de Processos” , 
ed. Brasileira: ISBN 978-85-64316-003. Elo Group, p. 1-133.
VIEIRA, J. F. F. et al. (2016) Atuação do Escritório de Processos como Apoio Técnico 
na Gestão dos Processos: o caso dos processos de gestão de pessoas da UFRGS. In: X 
Workshop de Tecnologia da Informação e Comunicação das Instituições Federais de 
Ensino Superior, mai. 2016, Gramado, RS.página
348
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 348-351, junho de 2019
Capítulo LVI - Implementação de novo fluxo de processo: case aula de campo da UFMTImplementação de novo fluxo de processo: case 
aula de campo da UFMT
Anne Cristine Betoni Cardoso1, Cleiton Diniz da Silva1, Thais Fernanda Bueno da Silva1,
Greice de Souza Arruda1, Thiago Meirelles Ventura1, Leandro Costa Garcia1
1Escritório de Projetos e Processos – Universidade Federal do Mato Grosso (UFMT)
Av. Fernando Corrêa da Costa, no 2367 – Bairro Boa Esperança – Cuiabá – MT – Brasil
{annebetoni,cleitondiniz,thaisbueno,greicearruda}@ufmt.br, thiago@ic.ufmt.
br, leandro.garcia@aluno.ic.ufmt.br
Resumo
O Escritório de Projetos e Processos da Universidade Federal de Mato Grosso conduziu a implementação do novo 
fluxo do processo de aula de campo com a utilização de técnicas de gestão de projetos e processos a fim de conseguir os 
resultados esperados. Os esforços resultaram no engajamento dos responsáveis pelo processo, situação indispensável 
para a efetivação do trabalho. Os resultados positivos da implementação do projeto geraram celeridade e economia 
financeira para a Instituição.
Palavras-chave: BPM, implementação de melhoria, graduação
1. Introdução
Em [ABPMP 2013] é apresentado o ciclo de vida do Business Process 
Management (BPM) para processos com comportamento previsível, no qual consiste 
das fases de: planejamento, análise, desenho, implementação, monitoramento e 
por fim, refinamento.
A gestão de processos com base nesse método propicia algumas grandes 
entregas, como:
• Mapeamento do processo na situação atual;
• Oportunidade de melhorias e sugestões identificadas;
• Desenho do processo, representando o novo fluxo;
• Novo fluxo implementado;
• Novo fluxo refinado.
A completa e efetiva agregação de valor para a organização pela gestão de 
processos ocorre apenas na execução de todas as fases [Capote 2012].  No entanto, 
normalmente a etapa de implementação é dificultosa, tanto pela resistˆencia dos 
envolvidos à mudança, devido à forte cultura enraizada nas organizações, quanto 
pelo tempo do projeto e dedicação necessária ao Escritório de Processos para 
efetivação dos trabalhos.página
349
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 348-351, junho de 2019
Capítulo LVI - Implementação de novo fluxo de processo: case aula de campo da UFMTPara enfrentar esses obstáculos, os executores e usuários do processo 
devem ser envolvidos intensamente no projeto. A definição do novo fluxo deve ser 
construída em conjunto, aproveitando todo o conhecimento dos servidores que 
lidam diariamente com o processo. Além disso, a definição de um cronograma e o 
acompanhamento pelo Escritório também são ações essenciais.
 Maior que as dificuldades na implementação, são os valores entregues nessa 
fase, que podem gerar diversos benefícios, como: economia financeira, mitigação 
de riscos, redução de tempo, aumento de satisfação dos usuários do processo, 
entre outros [Silva 2015]. Neste contexto, este trabalho apresenta as técnicas 
utilizadas na implementação do novo fluxo do processo de aula de campo da 
Universidade Federal de Mato Grosso (UFMT), idealizado após o término das fases 
de análise e desenho.
2. Métodos
A  análise  e  desenho  do  processo  abordado  neste  trabalho  foi  detalhada  
em [Cardoso et al. 2018]. Após homologação do novo fluxo pela dona e gerente 
do processo, definição dos responsáveis pela execução das ações propostas e dos 
representantes dessas para facilitar a comunicação durante o projeto, realizou-
se uma reunião com tais representantes para elaboração do cronograma de 
implementação. Foi utilizado a ferramenta GPWEB para elaborar o cronograma, 
obtendo uma melhor gestão das atividades predecessoras e sucessoras.
Vale mencionar que a gestão desse processo foi conduzida como um projeto, 
logo, utilizou-se diversos conhecimentos e métodos relacionados à gestão de 
projetos, como planejamento e envolvimento dos interessados.
Em seguida, apresentou-se o cronograma aos coordenadores de curso, a 
fim de informar sobre as sugestões aprovadas, inclusive as indicadas por eles no 
workshop de proposição de melhorias, reforçando, assim, o engajamento dos 
servidores. O Escritório de Projetos e Processos (EPP) da UFMT realizou reuniões 
periódicas com os representantes da Pró–Reitoria de Graduação (PROEG) e da 
Secretaria de Infraestrutura (SINFRA) para monitorar as implementações e auxiliar 
nas dificuldades que surgiam. Ao passar do tempo, conforme a execução do 
cronograma, diminuiu-se a periodicidade dessas reuniões.
De acordo com [Silva 2015], para assegurar a efetividade dos novos 
procedimentos, não é suficiente capacitar e orientar, mas é necessário gerenciar e 
acompanhar até que esses novos padrões se insiram na cultura. Assim, além das 
reuniões periódicas, o EPP mediou diversas outras entre os representantes e demais 
setores, com objetivo de explicar sobre o trabalho e mitigar riscos de desgastes 
entre as unidades. Houve contatos com a Secretaria de Tecnologia da Informação 
(STI) para desenvolvimento de funções no Ambiente virtual de Aprendizagem da 
UFMT , Coordenação de Assistˆencia Social e Saúde do Servidor para revisão dos 
protocolos de segurança das aulas de campo e com a Coordenação Financeira para 
sanar dúvidas sobre o novo fluxo e ajustar rotinas.página
350
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 348-351, junho de 2019
Capítulo LVI - Implementação de novo fluxo de processo: case aula de campo da UFMTOs envolvidos eram previamente comunicados antes de cada implementação. 
As comunicações ocorriam de diversas formas, como: presencialmente, em 
reuniões, por e-mail ou pelo Sistema Eletrônico de Informações (SEI). Foi realizado 
também um treinamento presencial envolvendo os coordenadores de curso 
(Figura 1), no qual era apresentado os conceitos sobre a gestão de processo, com 
intuito de disseminar essa cultura, do novo fluxo de aula de campo, dos ganhos 
previstos, das novas funções de tecnologia da informação e das regras do setor de 
transporte. Além disso, após as primeiras implementações, houve a definição de 
indicadores e responsáveis pelo seu monitoramento, a fim de avaliar os resultados 
das ações implementadas e os pontos necessários a serem ajustados.
Figura 1. Capacitação para implementação do novo fluxo.
3. Resultados
A etapa de implementação está sendo finalizada e ocorre em paralelo a de 
monitoramento. Entre as ações executadas, estão:
• Atualização da ordem de serviço sobre transporte, efetivando a priorização 
de atendimento de transporte para as viagens de aulas de campo;
• Desenvolvimento de uma ferramenta pelo setor de transporte para 
acompanhar os custos de cada aula por período e por curso para melhor 
gestão dos contratos de combustível, manutenção de véıculo e motorista;
• Eliminação de 55% das atividades do processo por não agregação de valor, 
reduzindo o tempo de execução e proporcionando economia financeira;
• Alteração do prazo para programação do transporte de periodicidade 
semestral para a cada cinquenta dias, podendo ser refinada posteriromente;
• Emissão pelo sistema da lista de estudantes com dados bancários para 
pagamento de auxílio aula de campo, eliminando o retrabalho de solicitação 
desses dados aos estudantes por cada docente;
• Emissão pelo sistema da lista de passageiros com envio automático ao setor 
de transporte para mitigação de riscos, como condução de passageiros sem 
seguro;
• Novo fluxo via SEI para pagamento dos auxílios, a fim de monitorar o 
ressarcimento de auxílio dos estudantes ausentes às aulas de campo;página
351
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 348-351, junho de 2019
Capítulo LVI - Implementação de novo fluxo de processo: case aula de campo da UFMT• Adaptação dos ônibus para o transporte de cadeirantes.
Executou-se mais de 65% de todas as ações previstas até o momento.é 
avaliado que este resultado positivo foi devido ao envolvimento e dedicação 
dos representantes e pela condução e colaboração do Escritório de Projetos e 
Processos. Além disso, o auxílio de outras unidades já mencionadas colaboraram 
para este resultado.
Por fim, é necessário destacar que, mesmo com todos os resultados positivos, 
percebe-se a necessidade de refinamento, etapa também presente no ciclo de vida 
BPM com as ações de implementação e monitoramento.
4. Conclusão
A implementação de um novo fluxo de trabalho tende a ser uma das fases mais 
desafiadoras do ciclo de vida BPM. O não envolvimento das partes interessadas 
durante a  definição do desenho a ser implantado pode colocar em risco a 
efetividade do trabalho. Ciente do tamanho do desafio, o EPP desenvolveu diversas 
atividades para a integração dos executores ao novo fluxo de trabalho, por meio 
de entrevistas, capacitações e visitas aos setores. Além disso, foi acompanhado e 
auxiliado os responsáveis na realização das ações de melhoria, com base em um 
cronograma preestabelecido e acordado entre as partes. Conclui-se, então, que o 
fomento da participação dos executores do processo e o monitoramento e apoio 
nas implementações efetivou a implementação do novo fluxo.
Referências
ABPMP (2013).  Guide to the Business Process Management Common Body of 
Knowledge: ABPMP BPM CBOK. Association of Business Process Management 
Professionals, 3 edition.
Capote, G. (2012). BPM Para Todos. Rio de Janeiro, 1 edition.
Cardoso, A., Takagi, N. H., Silva, T . F. B. d., Arruda, G. d. S., Martins, J. R., and Zimmermann, 
K. A. (2018). Analise de Processos: Case Aula de Campo UFMT .
Silva, L. (2015). Gestão e Melhoria de Processos: conceitos, práticas e ferramentas.
BRASPORT , Rio de Janeiro.página
352
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 352-359, junho de 2019
Capítulo LVII - Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal 
de Santa Maria - UFSMIncorporando a Gestão de Riscos à Gestão por 
Processos na Universidade Federal de Santa 
Maria - UFSM 
Evandro G. Flores, Frank L. Casado, Daniele M. Rizzetti, Jonas C. Macedo, Taiani B. 
Kienetz, Rafael F. Neves
Pró-reitoria de Planejamento - PROPLAN
Universidade Federal de Santa Maria (UFSM) – Santa Maria – Brasil
{evandro.flores,frank.casado,}@ufsm.br, {danieadm83, jonascarniel, taianibk, 
felinneves}@gmail.com
Resumo
Este artigo tem a finalidade de apresentar como a Universidade Federal de Santa Maria está incorporando a Gestão 
de Riscos à Gestão por Processos. Fundamentado em alguns conceitos, tais como: BPM, COSO, Norma NBR “ISO 
31000:2009 e Gestão de Riscos, o Projeto Modernização Administrativa da Reitoria da UFSM desenvolveu um 
piloto afim de validar uma metodologia capaz de identificar os riscos em processos, bem como seus impactos, e 
propor ações que possam minimizar ou até eliminar determinados riscos.
Palavras-chave: Gestão por Processos; Gestão de Riscos; Riscos.
1. Introdução
A incerteza ou o risco é inerente a praticamente todas as atividades 
humanas. No mundo corporativo onde as empresas estão expostas a uma miríade 
de incertezas originadas de fatores econômicos, sociais, legais, tecnológicos e 
operacionais, a gestão de integridade, riscos e controles internos é crucial para 
que se alcance os objetivos estratégicos (BRASIL, 2017).
 O Projeto de Modernização Administrativa da Reitoria da Universidade 
Federal de Santa Maria (UFSM) na continuidade da implantação da Gestão por 
Processos na instituição, e seguindo orientações do governo federal em relação a 
Instrução Normativa Número 1 de 2016 de Controles Internos, Gestão de Riscos e 
Governança no âmbito do Poder Executivo Federal, incluiu o escopo de Gestão de 
Riscos no Projeto de Modernização Administrativa.
Assim, foi desenvolvido um modelo piloto para gestão de riscos a nível 
operacional na instituição levando-se em consideração os processos mapeados, 
desta forma, levando-se em consideração algumas metodologias estudadas, foi 
proposta uma metodologia adaptada às realidades da instituição.
Inicialmente foi desenvolvido um projeto piloto para testar a metodologia 
de maneira que fosse possível identificar as dificuldades do processo e corrigir 
algumas falhas que porventura pudessem comprometer a eficácia dos resultados 
esperados.página
353
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 352-359, junho de 2019
Capítulo LVII - Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal 
de Santa Maria - UFSM2. Métodos
Como referências bibliográficas para desenvolver o piloto, foram utilizadas 
a Norma NBR “ISO 31000:2009 de Gestão de Riscos – Princípios e diretrizes” e o 
guia “Gerenciamento de Riscos Corporativos – Estrutura Integrada” do Comitê das 
Organizações Patrocinadoras da Comissão Treadway (COSO), organização sem fins 
lucrativos e referência internacional na gestão de riscos em organizações.
Revisadas as metodologias, definiu-se que a Gestão de Riscos deveria ser 
executada de maneira cíclica, com etapas de: identificação; análise qualitativa; 
análise quantitativa; planejamento e execução de respostas; e monitoramento, 
conforme Figura 1. Além disso, definiu-se a classificação dos riscos como: 
operacionais; de imagem/reputação; legais; e financeiros/orçamentários.
Figura 1: Sequencia do Gerenciamento dos Riscos
Foi proposto um modelo operacional baseado em uma análise por processos, 
sendo que para o piloto foi selecionado o processo de “Afastamento Eventual de 
Servidores” , representado na Figura 2, uma vez que além de já estar devidamente 
mapeado, apresenta 57 etapas, sendo uma grande parcela destas com interação 
entre usuários e sistema, características que convergem para a proposta da 
modernização de administrativa.página
354
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 352-359, junho de 2019
Capítulo LVII - Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal 
de Santa Maria - UFSM
Figura 2: Processo de Afastamento Eventual na UFSM.
 Foram propostos inicialmente quatro eventos presenciais com os servidores 
dos setores envolvidos neste processo. O primeiro tinha como intuito dividi-los 
em três grupos e identificar riscos, tal como compreender características dos 
mesmos, razões, efeitos, e possíveis soluções. Isso se deu através da ferramenta de 
brainstorming. Finalizada a primeira etapa presencial, a equipe responsável pelo 
projeto classificou os riscos identificados. A Tabela 1 lista os riscos identificados 
nesta primeira etapa.página
355
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 352-359, junho de 2019
Capítulo LVII - Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal 
de Santa Maria - UFSM# RISCO CAUSA S EFEITOS POSSÍVES SOLUÇÕES 
1 
Autor izar um 
afasta mento com 
informaç ões 
errada s ou 
incompletas  * Falta de informações 
* Análise pouco criteriosa *Retrabal ho e repetição de 
atividades no processo com 
análise e tramitaçõ es que 
pode riam ser evitadas 1) Capacitação sobre 
afastame nto  
2) Divulgação de informaç ões 
3) Inserir pop -ups no sistema 
de confirmaç ão 
4) Check list de critérios para 
aprovação  
2 
Servidor abre o 
afasta mento com 
informaç ões 
incompleta s ou 
erradas  * Falta de 
respon sabilidade do 
servidor 
* Certeza de que sempre 
que lançar algo errado 
alguém  irá devolver para 
corrigir *Retrabal ho e repetição de 
atividades no processo com 
análise e trami taçõ es que 
pode riam ser evitadas 1) Em  afastamento com ônus 
o sistema abre uma janela 
avisando sobre a inclusão de 
um anexo de autorização  
3 Autor ização de 
afasta mento 
incorre to ou com 
falta de critérios *Chefia imediata não 
analisou devidamente  *Aus ência injustificada  do 
servidor no local de trabal ho 1) Conscientizar as chefias de 
sua respon sabilidade direta 
pela  autorização dos  
afastame ntos 
4 
Processo parado 
na caixa pos tal * Falta de acesso à caixa 
postal 
* Férias de servidores da 
caixa pos tal 
* Afastame nto de 
servidores da caixa postal *Doc umento autorizativo do 
afastamento que deveria ser 
prévio, acaba sendo gerado 
em datas pos terior ao 
afastamento 1) Campa nha de atualização 
periódica de e-mail no SIE 
2) CPD criar uma requisição 
de processos  que estão no 
caixa pos tal pela chefia 
imediata 
3) Outr a forma  sistemát ica de 
visualização e trami tação  
5 
Abrir o tipo de 
processo errado * Falta de interesse na 
busca por informaçõe s 
uma vez que as 
informaçõe s já estão 
disponíveis 
* Falta de organ ização  
* Situaçõ es inesperadas *Gera análises de 
doc umentação e envolve 
tramitações desnecessárias 
até que se detecte o erro no 
tipo de processo 1) Capacitação sobre 
afastame nto   
2)  Divulgação de informaç ões 
3)  Inserir pop -ups no sistema 
de confirmação 
4)  Solicitar justificativa 
quando for fora do prazo 
6 Afastame nto com 
processo parado 
porém não sabe-
se como acessar 
as informaçõ es 
para altera-las * Falta de interesse do 
servidor em tentar 
resolver o problem a 
* Aus ência de um tutorial 
no portal de 
afastamentos que 
explique como fazer *Aus ência do registro nas 
ocorrê ncias funcionais do 
servidor 1) Capacitação sobre 
afastamento 
7 
Abrir o processo 
fora do prazo * Falta de interesse na 
busca por informaçõe s 
uma vez que as 
informaçõe s já estão 
disponíveis 
* Falta de organ ização  
* Situaçõ es inesperadas *Doc umento autorizativo do 
afastame nto que deveria ser 
prévio, acaba sendo gerado 
em datas pos terior ao 
afastame nto 1) Capacitação sobre 
afastame nto   
2) Divulgação de informaç ões 
3) Inserir pop -ups no sistema 
de confirmaç ão 
4) Solicitar justificativa 
quando for fora do prazo 
8 
Falsidade 
ideológica por 
empréstimo de 
senhas para  outra 
pessoa  efetuar o 
registro  *Nã o sabe r como 
registrar no sistema  
*Servidor que emprestou a 
senha assume toda a 
respon sabilidade pelo que 
está lançado no sistema,  
ainda  que não pos sua o 
conhec imento do teor das 
informaçõe s   página
356
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 352-359, junho de 2019
Capítulo LVII - Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal 
de Santa Maria - UFSM9 Minuta da 
publicação no 
DOU sair com 
informaç ões 
errada s ou 
incompletas  *Falha em toda s as 
instâncias de análise do 
processo *Custos com publicação de 
retificaçõe s no DOU 1) Ajustar o sistema para 
gerar a minuta para buscar 
todas informaçõe s 
necessárias (ônus) 
2) Revisão de português pelo 
servidor que gera a minuta 
10 
Afastame nto sem 
abrir o processo * Falta de interesse na 
busca por informaçõe s 
uma vez que as 
informaçõe s já estão 
disponíveis 
* Falta de organ ização  
* Situações inesperadas *Aus ência do registro nas 
ocorrê ncias funcionais do 
servidor 1) Capacitação sobre 
afastame nto   
2) Divulgação de informaç ões 
3) Inserir pop -ups no sistema 
de confirmaç ão 
4) Solicitar justificativa 
quando for fora do prazo 
11 Aus ência de 
chefia designada 
no período 
gerando 
repercussão 
financeira 
desnecessári a * Chefias em férias ou 
afastamento e ausência 
de substituto legal * Não ter chefia para aprovar 
a trami tação do processo 1) Maior atenção dos 
respon sáveis pela  solicitação 
de designação de chefia 
2) Alte ração do sistema de 
substituição permitindo a 
retificação com vários 
períodos 
12  
Pedir o 
afasta mento 
porém não se 
afasta r 
cancelando o 
processo 
 * Não se solicita a 
comprovação da 
efetivação do 
afastamento *Inclusão nas ocorrê ncias 
funcionais do servidor de um 
registro de uma atividade que 
não ocorreu efetivam ente 1) Aud itoria detectar e cobrar 
da Universidade  
13 
Autor izar um 
afasta mento sem 
respaldo legal * Falta de informações 
* Análise pouco criteriosa *ausência injustificada do 
servidor no local de trabal ho 1) Capacitação sobre 
afastame nto  
2) Divulgação de informaç ões 
3) Inserir pop -ups no sistema 
de confirmaç ão 
4) Check list de critérios para 
aprovação  
14 
Responder 
judicialmente por 
possí veis erros  * Falta de informações 
* Análise pouco criteriosa *Desgaste e tempo 
dispendido com análise e 
respos tas a questionamentos 
judiciais 1) Capacitação sobre 
afastame nto  
2) Divulgação de informaç ões 
3) Inserir pop -ups no sistema 
de confirmaç ão 
4) Check list de critérios para 
aprovação  
15 
Abusos e desvios 
de finalidade por 
parte do servidor 
requisitante  
* Falta de compromisso 
do servidor com a 
instituição  
* Irrespon sabilidade da 
chefia na análise das 
solicitações 
 *Pedir mai s tempo que o 
necessário 
* Afasta mento por eventos 
irrelev antes 1)Conscientizar as chefias de 
sua respon sabilidade direta 
pela  autorização dos  
afastame ntos 
16 
Falta de recursos  
após  aprovação 
da direção  * Fluxo do processo *Nã o ocorre  o afastame nto, o 
servidor se ausenta 
independentemente da 
autorização ou o processo 
retorna ao servidor para 
alteração do tipo 1) Processo passar pelo 
Depto.  Adm. antes da Direção 
2) Disponibilizar consultas de 
processo de afastame nto na 
web página
357
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 352-359, junho de 2019
Capítulo LVII - Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal 
de Santa Maria - UFSMTabela 1: Riscos identificados no processo de Afastamento Eventual
Logo, iniciou-se a etapa seguinte com o segundo encontro. Este teve o objetivo 
realizar análises qualitativas e quantitativas dos riscos identificados. É destacado 
que acabou-se realizando a dinâmica em um único grupo. Basicamente, a equipe 
do projeto desenvolveu uma planilha a qual foi preenchida pelos servidores. Esta 
abrangia uma escala Likert de 1 (menos grave) a 5 (mais grave) relacionando as 
dimensões e os riscos (Quadro 1).
Quadro 1: Escala Likert relacionando as dimensões e os riscos
Sobretudo, esta escala tinha o intuito de mensurar o impacto de determinado 
risco ao processo em três dimensões: servidores envolvidos, qualidade no 
processo, e tempo no processo. Foi calculada a média entre cada uma dessas 
dimensões a fim de resultar em um impacto total de cada risco. Além disso, a 
escala também mediu uma dimensão de probabilidade de ocorrência de cada 
risco, gerando a probabilidade total. Os valores de impacto total e probabilidade 
total foram multiplicados, resultando no score para cada risco (Quadro 2). Como 
resultado desta etapa foram identificados 7 riscos prioritários, uma vez que seus 
scores foram superiores à média dos scores de todos os riscos.página
358
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 352-359, junho de 2019
Capítulo LVII - Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal 
de Santa Maria - UFSM
Quadro 2: Resultado da multiplicação dos valores de Impacto x Probabilidade
Ao final da segunda etapa, identificou-se, a não necessidade de um evento 
presencial para a terceira etapa (planos de ações), pois a equipe do projeto foi 
responsável por sugerir propostas de ações para inibir e diminuir as causas dos 
riscos, baseando-se em possíveis soluções apresentadas na primeira etapa pelos 
servidores envolvidos. Por ora, a implementação dos planos de ação propostos, tal 
como a última etapa do ciclo referente ao monitoramento foram postergadas, pois, 
em um primeiro momento, deverão ser definidas e devidamente institucionalizadas 
as diretrizes de Gestão de Riscos a nível estratégico na UFSM.
3. Resultados
Após aplicação e validação do processo metodológico, abaixo estão listados 
os principais resultados obtidos:
• Identificação de 18 riscos para o processo de “Afastamento Eventual”;
• 7 riscos priorizados pelo score calculado estar acima da média;
• Planos de ação 5W2H (faltando prazos e custos associados) para os 7 
riscos prioritários mais 8 riscos não prioritários;
• Foi possível testar a metodologia proposta; e,
• Validação da metodologia até a etapa de proposição de planos de ação.
4. Conclusão 
Conclui-se que a metodologia proposta para a UFSM conseguiu identificar um 
número expressivo de riscos para um processo, porém não podemos considerar 
este piloto como conclusivo visto que antes de padronizar essa metodologia a 
nível institucional, aconselha-se testar a mesma em uma amostra de processos 
administrativos a fim de comprovar sua eficiência. Outro fator é a necessidade de 
esperar a definição das diretrizes estratégicas. página
359
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 352-359, junho de 2019
Capítulo LVII - Incorporando a Gestão de Riscos à Gestão por Processos na Universidade Federal 
de Santa Maria - UFSMConcluindo, este piloto teve como objetivo testar a primeira metodologia 
proposta a nível operacional na instituição, e parcialmente atingiu sua meta dadas 
as restrições atuais.
Referências
ASSOCIAÇÃO BRASILEIRA DE NORMAS TÉCNICAS. NBR ISO 31000: Gestão de riscos — 
Princípios e diretrizes (Tech. Rep.). Rio de Janeiro, 2009.
ASSOCIATION OF BUSINESS PROCESS MANAGEMENT PROFESSIONALS – ABPMN: Guia 
para o Gerenciamento de Processos de Negócio Corpo Comum de Conhecimento, 
ABPMP BPM CBOK V3.0, 1ª Edição, 2013.
BRASIL, Ministério do Planejamento, Desenvolvimento e Gestão. Assessoria Especial 
de Controle Interno. Manual de Gestão de Integridade, Riscos e Controles Internos da 
Gestão: versão 2.0. Brasília-DF, 2017.
COMMITTEE OF SPONSORING ORGANIZATIONS OF THE TREADWAY COMMISSION. 
Enterprise Risk Management — Integrated Framework. Executive Summary. COSO, 
Jersey City, 2004.página
360
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 360-365, junho de 2019
Capítulo LVIII - Melhoria do Processo de Pagamento de Fornecedores do Restaurante Universitário 
da UFRGSMelhoria do Processo de Pagamento de 
Fornecedores do Restaurante Universitário da 
UFRGS
João F. Vieira, Priscilla F. Pontes, Marlon Soliman, Éverson Santos, Erica K. de Oliveira
Escritório de Processos – PROPLAN – Universidade Federal do Rio Grande do Sul 
{joao.vieira@proplan.ufrgs.br, priscilla.pontes@proplan.ufrgs.br, marlon.
soliman@proplan.ufrgs.br, everson.santos@proplan.ufrgs.br, erica.oliveira@
proplan.ufrgs.br} 
Resumo
A Universidade Federal do Rio Grande do Sul (UFRGS) estava atrasando os pagamentos dos fornecedores referentes 
aos serviços prestados pela Pró-reitoria de Assuntos Estudantis (PRAE). O presente trabalho tem por objetivo 
apresentar o projeto de melhoria para o processo de pagamento de fornecedores do principal serviço prestado pela 
PRAE, o Restaurante Universitário (RU). A partir da modelagem AS IS e de uma análise do processo, identificou-se 
que a falta de visão holística e o excesso de burocracia no controle das notas fiscais (NFs) estava gerando o atraso no 
pagamento. A partir disso, melhorias foram propostas para o processo.
Palavras-chave: BPM, melhoria de processos, processo de pagamento
1. Introdução
O Business Process Management (BPM) é uma disciplina gerencial que visa 
enxergar os processos como o fator-chave nas decisões em uma organização 
[Davenport e Prusak 1998; Pavani Júnior e Scucuglia 2011]. Para o BPM, a estratégia 
de uma organização deve ser delineada através de iniciativas de melhorias de 
processos críticos para o alcance desta estratégia [Davenport e Prusak 1998]. O 
presente trabalho visa apresentar a aplicação completa do ciclo de melhoria de 
processos em um processo da Universidade Federal do Rio Grande do Sul (UFRGS), 
destacando o passo a passo utilizado, de forma que possa servir de referência e 
inspiração para futuras aplicações no contexto de Instituições Federais de Ensino 
(IFES). 
O processo em que ocorreu a iniciativa de melhoria apresentada é o de 
pagamento de compras realizadas pela Pró-Reitoria de Assuntos Estudantis 
(PRAE) da UFRGS. A PRAE é responsável pelas iniciativas de assistência estudantil 
para os alunos da universidade (e.g. Restaurante Universitário, Casa do Estudante 
e gerenciamento de bolsas para alunos em vulnerabilidade social).
O problema solucionado neste trabalho era o atraso na realização dos 
pagamentos para os fornecedores do Restaurante Universitário (RU). Ressalta-se 
que, o RU realiza grandes quantidades de mensais, fazendo com que a organização 
das notas fiscais e demais documentos relevantes para o processo de pagamento 
torne-se uma tarefa difícil de ser executada. A UFRGS, na ocasião do projeto, vinha página
361
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 360-365, junho de 2019
Capítulo LVIII - Melhoria do Processo de Pagamento de Fornecedores do Restaurante Universitário 
da UFRGSrecebimento multas por estar excedendo os prazos legais para pagamento das 
compras advindas da PRAE.
Para a intervenção no referido processo, foi realizado o ciclo completo de 
melhoria de processos [Dumas et al., 2012]: Modelagem do processo AS IS, Análise 
do Processo, Modelagem do processo TO BE, implementação e Monitoramento. 
Em relação às iniciativas de melhoria de processos, o presente projeto possuiu 
alguns diferenciais: levantamento dos indicadores e comparação dos indicadores-
chave do processo referente a situação anterior e posterior ao projeto; utilização 
de ferramentas que não são usuais em projetos de melhoria de processos 
(e.g. diagrama de relações); e alcance das melhorias sem a necessidades da 
implementação de sistemas computacionais. Em relação ao último ponto, a 
literatura sobre BPM ressalta que uma iniciativa de melhoria de processo não deve 
ter como objetivo apenas a implementação de um sistema para automação.
2. Método
Para a execução do projeto, foi definido que a PRAE disponibilizaria uma 
pessoa que gerenciaria o projeto. Na etapa de modelagem AS IS, primeiramente, 
foram realizadas reuniões entre os membros do Escritório de Processos (EP), a 
gerentes do projeto e alguns atores-chave do processo com o objetivo de entender 
o escopo do processo e caracterizar o problema em questão. Ainda na modelagem 
AS IS, o próximo passo consistiu em realizar novas reuniões, agora com todos 
os atores do processo, com o objetivo de entender seus detalhes e modelá-lo 
em notação BPMN. Além das reuniões, foram estudados diversos processos de 
pagamentos da PRAE no sistema SEI, pois assim era possível obter algumas etapas 
do processo que, eventualmente, não eram citadas pelos atores.
A análise do processo foi dividida em duas subestapas: diagnóstico e 
proposição de melhorias. Na subetapa diagnóstico, de posse dos dados coletados 
anteriormente, foram realizados brainstormings entre os membros do EP e a 
construção de um diagrama de relações para identificar as causas dos atrasos nos 
pagamentos da PRAE. Além disso, foram levantados os valores dos indicadores-
chave do processo a fim caracterizar seu desempenho. Na subetapa proposição de 
melhorias, foram identificadas ações que deveriam ser realizadas para diminuir 
os atrasos nos pagamentos da PRAE. Nesse momento, benchmarkings com 
outras unidades da UFRGS que conseguiam realizar o pagamento no prazo foram 
realizados. De posse dessas informações, um modelo TO BE, em notação BPMN foi 
construído.
Na etapa de implementação, foi realizada uma reunião para a apresentação 
dos resultados do projeto para os tomadores de decisão da PRAE, bem como foi 
entregue um relatório descrevendo o que foi realizado no projeto e as melhorias 
apontadas. Por fim, na etapa de monitoramento, levantou-se os indicadores-chave 
do processo e comparou-se esses valores com os obtidos antes da implementação 
de melhorias. Na Tabela 1, estão apresentadas as etapas do método e as técnicas 
utilizadas em cada uma delas.página
362
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 360-365, junho de 2019
Capítulo LVIII - Melhoria do Processo de Pagamento de Fornecedores do Restaurante Universitário 
da UFRGSTabela 1. Etapas do método e técnicas utilizadas
Etapa / Técnicas Brainstorming EntrevistasAnálise 
DocumentalNotação 
BPMNDiagrama 
de 
RelaçõesBenchmarking
Modelagem AS IS X X X X
Análise (diagnóstico) X X
Análise (proposição 
de melhorias)X X X
Modelagem TO BE X
Implementação
Monitoramento
3. Resultados
Após as diversas reuniões entre os membros do EP e os atores do processo, e 
de estudos de processos no SEI, foi realizada a modelagem AS IS do processo (Figura 
1). O processo iniciava com o Fornecedor entregando os produtos e notas fiscais 
(NFs) para os RUs, os quais devem ser conferidos pela Chefias dos RUs. Em seguida 
a Divisão de Alimentação (DAL), faz uma nova conferência nas NFs e, caso esteja 
tudo certo, registra a entrada dos produtos no Sistema de Almoxarifado (SAM). 
Logo após, a Coordenação de Compras e Execução Orçamentária (CCE) verifica 
se o Fornecedor está apto a receber o pagamento, organiza as documentações 
devidas e envia o processo para Seção de Escrituração da Despesa (SED), a qual 
realiza a liquidação da despesa e encaminha o processo para a Divisão de Execução 
Financeira (DF), enfim, realizar o pagamento. Durante as reuniões, os atores 
apontaram que os principais atrasos ocorriam nas etapas realizadas pela CCE.página
363
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 360-365, junho de 2019
Capítulo LVIII - Melhoria do Processo de Pagamento de Fornecedores do Restaurante Universitário 
da UFRGS
Figura 1. Modelo AS IS do processo
Na subetapa de diagnóstico da análise do processo, de posse das informações 
coletadas durante as reuniões da modelagem AS IS, foi realizado um diagrama de 
relações para entender as causas que geravam o problema (Figura 2). Constatou-
se que a falta de visão holística por parte dos atores do processo era responsável 
por diversos problemas, visto que as ações realizadas na atividade de um ator 
impactam na atividade de outro ator. Outra causa-raiz verificada foi a utilização 
de controles e métodos ineficientes, principalmente nas atividades realizadas 
pela CCE. A CCE, devido ao grande volume de NFs recebidas da DAL, criou 
diversas planilhas de controle para organizar o encaminhamento das NFs para a 
SED, porém esse excesso de controle acabou gerando um excesso de burocracia 
e, consequente, atraso na execução da atividade. A dificuldade na utilização do 
SEI, o qual era um sistema recém implantado na ocasião do projeto, também era 
causa-raiz para o atraso nos pagamentos. Por fim, a capacidade produtiva limitada 
da CCE devido ao baixo número de servidores trabalhando nesse setor também 
gerava atrasos no encaminhamento do processo de pagamento.página
364
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 360-365, junho de 2019
Capítulo LVIII - Melhoria do Processo de Pagamento de Fornecedores do Restaurante Universitário 
da UFRGS
Figura 2. Diagrama de Relações
Ao analisar a Figura 2, verifica-se uma convergência para o item 
“desorganização no gerenciamento das NFs” , o qual era uma característica 
apontada por todos os atores no início do projeto. Porém, após a análise do 
processo, concluiu-se que a desorganização no gerenciamento das NFs era mais 
uma consequência intermediária do que propriamente uma causa. O excesso de 
voltas de processos para a CCE¸ que gerava um acúmulo de processos, também 
foi apontado como uma consequência intermediária relevante, visto que a CCE 
tinha que tratar tanto dos novos pagamentos quanto dos que voltavam devido a 
erros na instrução do processo. Esse acúmulo de processos fazia com que a CCE 
tivesse dificuldade em priorizar os processos a serem encaminhados para SED 
(relacionado ao item “falta de priorização de processos”), gerando o atraso nos 
pagamentos.
Após uma série de reuniões entre os analistas de processos do EP e os atores 
do processo, foram definidas as ações de melhoria a serem realizadas. Para a causa-
raiz “falta de visão holística” , recomendou-se que sensibilizações e aproximações 
entre os atores do processo fossem realizadas de forma que um conhecesse o 
trabalho do outro e que se soubessem dos impactos das ações realizadas. Para 
isso, poderia ser realizada um workshop entre os atores do processo, onde um 
apresentaria a sua atividade para o outro e apresentaria também os problemas 
que mais incidem na sua atividade. Em relação a utilização de controles e métodos 
ineficientes por parte da CCE, foi solicitada a simplificação dos métodos de trabalho 
deste setor e a criação de planilhas de controle simplificadas de NFs. Além disso, 
foi indicado uma redistribuição de funções entre os integrantes da CCE, visto que a 
carga de trabalho era desequilibrada entre os servidores. Por fim, foi recomendado 
a utilização de sistemas a prova de falhas a fim de diminuir os retornos do processo 
devido a diligências. Destaca-se que não foi realizada a modelagem TO BE dos 
processos, visto que as ações propostas, em geral, não mudavam a sequência de 
atividades e as regras do processo. Por isso, escolheu-se partir, diretamente para 
a implementação.página
365
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 360-365, junho de 2019
Capítulo LVIII - Melhoria do Processo de Pagamento de Fornecedores do Restaurante Universitário 
da UFRGSApós a implementação do projeto, o tempo médio para a realização de um 
pagamento, que, anteriormente, era de em torno de 115 dias, agora está em 45 
dias. Ressalta-se que ainda não se atingiu os 30 dias previstos em lei, no entanto 
o valor das multas recebidas pela UFRGS diminuiu consideravelmente. Uma 
consequência da diminuição do tempo de pagamento foi a queda no número de 
processos de pagamentos advindos da PRAE abertos no SEI, indo de 130 processos 
no início do projeto para 30 processos após as melhorias.
4. Conclusão
O presente trabalho apresentou uma iniciativa de melhoria de processos no 
processo de pagamentos da PRAE da UFRGS. Após a implementação das melhorias, 
constatou-se que o tempo para execução do processo de pagamentos diminuiu 
consideravelmente. Consequentemente, o número de processos de pagamentos 
abertos em média por dia diminuiu também. Uma verificação adicional, percebida 
ao fazer uma reunião pós-projeto com os atores do processo, foi a melhoria do 
ambiente de trabalho, o qual tornou-se muito mais leve após a execução do 
projeto.
 Como próximos passos, pretende-se auxiliar a CCE em refinar ainda 
mais as planilhas de controle de processos recebidos a fim de que se possa 
priorizar adequadamente os processos de pagamento. Além disso, pretende-
se realizar mais um levantamento dos indicadores do processo para verificar se 
o desempenho do processo está melhorando com o passar do tempo. Por fim, 
pretende-se reaproveitar as ações realizadas nesse projeto para a melhoria do 
processo de pagamento de outras Pró-reitoras da UFRGS, visto que os processos 
são semelhantes ao trabalhado neste projeto e, portanto, podem-se beneficiar da 
implementação destas ações.
Referências
Davenport, T .; Prusak, L. (1998) “Conhecimento empresarial: como as organizações 
gerenciam o seu capital intelectual. Campus, Rio de Janeiro
Dumas, M.; La Rosa, M.; Mendling, J.; et al (2012). Fundamentals of Business Process 
Management. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013.
Pavani Júnior, O. and Scucuglia, R. (2011) “Mapeamento e gestão por processos–BPM: 
Gestão orientada à entrega por meio de objetos” , M. Books do Brasil Editora Ltda, São 
Paulo.página
366
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 366-371, junho de 2019
Capítulo LIX - Modelo de Maturidade em Mapeamento de Processos (M3P): Proposta e Aplicação na 
UFCSPAModelo de Maturidade em Mapeamento de Processos 
(M3P): Proposta e Aplicação na UFCSPA
Juliana Silva Herbert1, Andressa Bortolaso de Oliveira1, Marilia Rosa Silveira1, Rodrigo 
de Farias Giglio1
1Núcleo de Qualidade Interna – Universidade Federal de Ciências da Saúde de Porto Alegre 
(UFCSPA)
Rua Sarmento Leite, 245 - Porto Alegre, RS, Brasil - CEP 90050-170
{julianash, andressabo, mariliasi, rodrigofg}@ufcspa.edu.br 
Resumo
Este artigo descreve a motivação, o desenvolvimento e a aplicação de um Modelo de Maturidade em Mapeamento 
de Processos (M3P) pelo Núcleo de Qualidade Interna (NQI) da UFCSPA. É um modelo descritivo e prescritivo, 
composto por 8 níveis de maturidade, desenvolvido para prover mais autonomia aos setores da Universidade na 
implantação da Gestão por Processos. Cada nível é composto por critérios, que são avaliados a partir de checklists 
com itens objetivos de avaliação. Foi também definido um processo de avaliação, com a utilização do aplicativo 
M3P App. O primeiro ciclo de avaliações foi finalizado em novembro de 2018, no qual 18 setores foram avaliados 
e 431 artefatos revisados. Com estas ações, observou-se maior conscientização e o engajamento efetivo das pessoas 
envolvidas.
1. Introdução
Em junho de 2017, o NQI utilizou uma abordagem ágil para o mapeamento de 
processos na UFCSPA, baseada na gerência ágil de projetos [PMI 2017]. No contexto 
dessa abordagem, priorizou-se a geração contínua e frequente de valor, o trabalho 
realizado em ciclos com tempos limitados (time box) e o envolvimento direto 
de todos os profissionais que executam os processos. Foram realizados 7 ciclos, 
denominados de rodadas de mapeamento de processos com objetivos específicos 
e entregáveis bem definidos, a fim de compor gradualmente o “produto final” 
desejado: o mapeamento de processos do setor e sua integração com processos 
de outros setores. 
Apesar do sucesso obtido com essa abordagem, evidenciado pelo 
engajamento dos setores, identificou-se que, para a continuidade sustentável do 
trabalho, a estratégia a ser seguida a partir de 2018 deveria permitir que os setores 
da UFCSPA pudessem:
• ter informações sobre a diretriz a ser seguida para as atividades de 
mapeamento de processos, considerando sempre o objetivo final de 
implantar a gestão por processos na Universidade;
• identificar, de forma objetiva e padronizada, a situação atual e as ações 
necessárias para a sua evolução rumo a uma situação mais avançada com 
relação ao mapeamento de processos;página
367
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 366-371, junho de 2019
Capítulo LIX - Modelo de Maturidade em Mapeamento de Processos (M3P): Proposta e Aplicação na 
UFCSPA• obter uma maior autonomia para a realização e a manutenção deste 
trabalho, considerando a equipe do setor no contexto maior da UFCSPA.
Assim, verificou-se a necessidade de adotar um modelo de maturidade 
que possibilitasse atender esses aspectos. Após realizado o estudo de vários 
modelos apresentados na literatura, a equipe do NQI optou por desenvolver o seu 
próprio modelo de maturidade: o M3P , Modelo de Maturidade em Mapeamento de 
Processos, mais próximo do contexto e das necessidades da Universidade.
Nas seções a seguir, são apresentadas o método, onde constam informações 
sobre a proposição e o desenvolvimento do modelo, os resultados, no qual são 
abordados o primeiro ciclo de avaliação utilizando o M3P e os resultados obtidos, 
além das conclusões, que contemplam também as próximas atividades planejadas.
2. Método
O M3P foi desenvolvido pelo NQI porque, na revisão de literatura, não foram 
encontrados estudos que pudessem ser facilmente adaptados ao contexto de 
uma comunidade acadêmica, de forma a agregar valor objetivo e perceptível aos 
membros da comunidade acadêmica. 
Decidiu-se, portanto, desenvolver um modelo com elementos que fossem 
considerados críticos nas iniciativas realizadas na Universidade, ou seja, um 
modelo que:
• considera a abordagem bottom-up (de baixo para cima). O trabalho de 
mapeamento de processos iniciou na UFCSPA através de ações com os 
setores para que as pessoas pudessem se sentir fazendo parte desse 
trabalho, tivessem consciência dos processos nos quais estivessem 
envolvidos e, posteriormente, considerar a integração entre os setores, 
através de macroprocessos;
• é composto por critérios objetivos, que podem ser fácil e diretamente 
avaliados e entendidos, já que o modelo também pode ser utilizado pelo 
setor para uma autoavaliação; 
• é descritivo e prescritivo, pois é utilizado tanto na identificação e descrição 
da situação atual do setor, como para direcionar ou prescrever ações para 
a continuidade do trabalho no mapeamento de processos.
O modelo proposto é composto por 8 níveis de maturidade, sendo o Nível 
0 o nível com menor maturidade e o Nível 7 o de maior maturidade, conforme 
apresentado na Figura 1.
A abordagem adotada no modelo está baseada nos princípios de atuação do 
NQI:
• o mapeamento de processos deve ser realizado pelas pessoas que os 
executam;
• a adesão dos setores a esta iniciativa é voluntária;página
368
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 366-371, junho de 2019
Capítulo LIX - Modelo de Maturidade em Mapeamento de Processos (M3P): Proposta e Aplicação na 
UFCSPA• o mapeamento de processos deve gerar valor às pessoas e aos setores 
envolvidos;
• busca-se realizar o mapeamento tão completo e com o melhor nível de 
detalhe possível no momento;
• os processos mapeados devem ser vivos, ou seja, devem ser documentados, 
utilizados e revisados continuamente, para a identificação de correções 
e melhorias.
De forma associada à definição dos níveis de maturidade do M3P , foram 
desenvolvidos “combos” compostos por artefatos, tais como checklists, formulários, 
orientações e processos associados aos critérios definidos. Esses combos (Figura 
2) são genéricos, para facilitar sua aplicação, e devem ser customizados por cada 
setor, de acordo com suas características particulares, mantendo a conformidade 
com os critérios e as orientações estabelecidos em cada nível.
Figura 1. Níveis de Maturidade em Mapeamento de Processos do M3P.página
369
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 366-371, junho de 2019
Capítulo LIX - Modelo de Maturidade em Mapeamento de Processos (M3P): Proposta e Aplicação na 
UFCSPA
Figura 2. Composição dos níveis de maturidade.
O processo de avaliação dos setores para identificação do nível de maturidade 
em mapeamento de processos ocorre em três etapas (Figura 3). O processo foi 
modelado utilizando-se BPMN (Business Process Model and Notation) [ABPMP , 2013] 
e incorporado ao Guia de Aplicação do M3P [Herbert et al, 2018], que descreve 
o modelo e está disponível no Portal de Processos da UFCSPA1 (nqi.ufcspa.edu.
br/portaldeprocessos). Para a aplicação desse processo, foi desenvolvido um 
aplicativo, o M3P App, associado a uma base de dados com os níveis, critérios, 
itens de verificação e a avaliação de cada um desses itens para cada setor avaliado. 
Através do M3P App, é possível manter e recuperar o histórico de avaliações, além 
de otimizar recursos como tempo e papel, por exemplo.
Figura 3. Etapas do processo de avaliação do M3P.
1 Disponível em: http://nqi.ufcspa.edu.br/portaldeprocessos. Acesso em: 15 mar, 2019.página
370
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 366-371, junho de 2019
Capítulo LIX - Modelo de Maturidade em Mapeamento de Processos (M3P): Proposta e Aplicação na 
UFCSPAA avaliação dos critérios de cada nível é classificada em verde, amarelo 
e vermelho, de acordo com seu grau de atendimento. É avaliado como verde o 
critério em que todos os itens do checklist foram atendidos. Em amarelo, o 
critério com itens que estiverem parcialmente atendidos e que são possíveis de 
serem corrigidos em período de, no máximo, um mês. Já os itens que não foram 
atendidos ou estiverem parcialmente atendidos, mas que não são possíveis de 
serem corrigidos em até um mês, são classificados como vermelho.
Os níveis também têm sua avaliação classificada em verde, amarelo e 
vermelho. Assim, para que o nível seja classificado em verde, todos os seus critérios 
devem ter sido avaliados em verde na etapa anterior. Quando um nível é avaliado 
como amarelo, ele está pendente, ou seja, requer uma avaliação complementar 
(que deverá ser realizada em, no máximo, um mês após a avaliação principal). Um 
nível só fica pendente (amarelo) se não houver nenhum critério avaliado como 
vermelho. Havendo algum critério avaliado como vermelho, o nível não é atendido.
O processo de avaliação ocorre 2 vezes ao ano: em abril e em setembro.
3. Resultados
O M3P foi lançado e divulgado à Comunidade UFCSPA em maio de 2018 e 
o primeiro ciclo de avaliação ocorreu no período de setembro a novembro do 
mesmo ano. Entre maio e setembro de 2018, quando ocorreram as inscrições 
voluntárias dos setores para a avaliação, foram realizadas diversas reuniões com 
as equipes para explicar o modelo e conscientizar as pessoas sobre a importância 
da participação do setor na avaliação.
Com isso, 18 setores (31,58% dos setores do organograma da UFCSPA) foram 
avaliados, com 431 artefatos revisados pela equipe do NQI. Neste primeiro ciclo 
de avaliação, dos 18 setores avaliados, 3 obtiveram Nível 1, 5 obtiveram o Nível 2, 
6 foram avaliados com o Nível 3 e 4 com o Nível 4. Em alguns casos, o nível obtido 
foi menor e em outros, maior do que o nível solicitado inicialmente pela unidade. 
Além disso, foi desenvolvido um selo correspondente ao alcance de cada nível, 
como forma de registrar e recompensar os setores pelo seu engajamento e servir 
como referência para a comunidade sobre o grau de maturidade desta unidade. A 
entrega do selo impresso foi feita em um encontro com todos os setores, realizado 
no dia 7 de dezembro, ocasião na qual também foram apresentadas informações 
sobre as ações de mapeamento de processos realizadas na Universidade em 
2018, os dados dos indicadores do NQI sobre esta situação e as próximas ações 
planejadas. Além do selo impresso, o selo digital também foi incorporado às 
páginas do Portal de Processos da UFCSPA, de cada setor avaliado. Na Figura 4 
são apresentados os indicadores selecionados para o NQI, valores calculados para 
2018 e comparação com as metas.página
371
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 366-371, junho de 2019
Capítulo LIX - Modelo de Maturidade em Mapeamento de Processos (M3P): Proposta e Aplicação na 
UFCSPA
Figura 4. Indicadores do NQI e valores calculados para 2018.
4. Conclusão
O desenvolvimento e a aplicação do M3P nos setores da UFCSPA foi 
fundamental para o aumento da conscientização das pessoas envolvidas sobre a 
importância da gestão por processos na Universidade e seu impacto nas atividades 
do dia-a-dia.
Além disso, observou-se o aumento de autonomia dos setores na definição 
de objetivos e de planos para o alcance de objetivos, com relação ao mapeamento 
de processos. Houve um crescente grau de adesão dos servidores no uso do Portal 
de Processos da UFCSPA. Percebe-se também um maior entendimento do valor 
gerado pelo mapeamento de processos e a tendência de continuidade de busca do 
setor no alcance de níveis mais altos de maturidade do M3P .
O apoio da alta administração também foi fundamental para o sucesso destas 
iniciativas, embora perceba-se que a falta de obrigatoriedade para a realização 
dessas atividades pelos setores fez com que algumas unidades não se engajassem 
da forma como deveriam. Está sendo realizado um trabalho mais focado nestas 
unidades, atualmente, a fim de incluí-las no próximo ciclo de avaliação do M3P .
A partir da experiência adquirida, foi desenvolvida a segunda versão do 
M3P , com a simplificação de critérios e artefatos associados e a reavaliação de 
composição dos níveis de maturidade. Nos próximos ciclos de avaliação, pretende-
se aumentar o número de setores avaliados e envolver os setores já avaliados na 
obtenção de níveis mais altos de maturidade em mapeamento de processos.
Referências
ABPMP – Association for Business Process Management Professionals (2013). “ABPMP 
BPM CBOK v3.0” . ABPMP – Brasil.
Herbert, J. S. et al. (2018) Guia de Aplicação do M3P (Modelo de Maturidade em 
Mapeamento de Processos. Porto Alegre: UFCSPA, 2018. 50p.: il. color. Texto eletrônico. 
Modo de acesso: http://www.ufcspa.edu.br/proplan/nqi.
PMI – Project Management Institute (2017). “PMBoK Guide – Sixth Edition + Agile 
Practice Guide” . Project Management Institute – Pennsylvania, Estados Unidos.página
372
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 372-377, junho de 2019
Capítulo LX - Monitoramento de indicadores de processos com uso de Business Intelligence (BI)Monitoramento de indicadores de processos com 
uso de Business Intelligence (BI)
Beatriz S. Seidel¹, Vanessa G. Kinoshita¹, João C. S. O. Matos², Reinilton S. Juvenal², 
Naícia K. F. S. B. T. Caten3, Leriane S. Cardozo²
1Pró-Reitoria de Tecnologia da Informação e Comunicação (Protic) – Universidade Federal do 
Oeste da Bahia (UFOB) – Barreiras, BA – Brasil
2Pró-Reitoria de Planejamento e Desenvolvimento Institucional (Proplan) – Universidade Federal 
do Oeste da Bahia (UFOB) – Barreiras, BA – Brasil
3Pró-Reitora de Administração e Infraestrutura (Proadi) – Universidade Federal do Oeste da Bahia 
(UFOB) – Barreiras, BA – Brasil
processos@ufob.edu.br
Resumo
Este artigo apresenta a integração de uma ferramenta de Business Intelligence (BI) ao Gerenciamento de Processos 
de Negócio (BPM), por meio de relatórios gráficos e interativos para avaliar e monitorar o desempenho dos processos 
organizacionais. A ferramenta utilizada para tratamento dos dados foi o Power BI®. Os comandos utilizados no 
software permitiram encontrar os indicadores críticos dos processos, que foram importantes para a definição dos 
principais problemas a serem resolvidos. Além disso, os indicadores viabilizaram a tomada de decisão em relação à 
definição de metas, tais como o tempo máximo de tramitação dos processos.
Palavras-chave: Business Intelligence; Processos organizacionais; Power BI®.
1. Introdução
Num contexto em que a gestão do conhecimento é um diferencial competitivo 
[Reis e Angeloni 2006], as organizações possuem maior quantidade de dados, e 
o número de ferramentas gerenciais capazes de transformá-los em informações 
relevantes também aumenta [Antonelli 2009]. Essas ferramentas permitem 
transformar dados brutos, estruturados ou desestruturados, em informações que 
podem auxiliar o processo de tomada de decisão [Júnior e Souza 2016]. Além 
disso, a utilização de sistemas de informação para análise visual de dados, com o 
uso de gráficos, quadros ou tabelas, resulta na valoração das informações geradas 
[Júnior e Souza 2016].
Uma importante ferramenta é o Business Intelligence (BI), um conjunto 
de metodologias de gestão implementado pela Tecnologia da Informação (TI) 
para coletar, analisar e transformar dados extraídos de uma organização em 
conhecimento, para apoiar o processo decisório [Antonelli 2009; Reis e Angeloni 
2006].
Também direcionado à gestão do desempenho de uma organização, o 
Gerenciamento de Processos de Negócio, ou Business Process Management (BPM), 
é uma disciplina gerencial que trata dos processos organizacionais ponta a ponta, 
visando entregar valor para o cliente [ABPMP 2013]. Apesar do BPM e do BI poderem página
373
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 372-377, junho de 2019
Capítulo LX - Monitoramento de indicadores de processos com uso de Business Intelligence (BI)trabalhar de forma complementar, essa integração não é muito explorada [Vukšić, 
Bach e Popovič 2013]. Assim, este artigo tem por objetivo apresentar o uso do 
BI como ferramenta para monitorar e avaliar indicadores de processos de uma 
Instituição de Ensino Superior (IES) pública.
A seguir serão apresentados os métodos utilizados para a obtenção dos 
indicadores de desempenho dos processos, exibidos e analisados na seção de 
resultados. Por fim, serão apresentadas a conclusão e as referências do trabalho.
2. Métodos 
Com o objetivo de promover melhorias nos processos organizacionais da 
Universidade Federal do Oeste da Bahia (UFOB) e motivadas pela determinação 
da adoção dos processos digitais, em substituição aos físicos, três Pró-Reitorias 
da instituição, num esforço conjunto, criaram, em fevereiro de 2018, o Grupo 
de Trabalho para Mapeamento e Modelagem de Processos (GT de Processos), 
composto por equipe multidisciplinar e dedicação de 20 horas semanais.  
Inicialmente, o GT de Processos concebeu a Estrutura Analítica de Projetos 
e a definição do arcabouço metodológico, sendo escolhido o uso do método 
DMAIC (Definir, Medir, Analisar e Controlar) aplicado na metodologia BPM. A partir 
do diagnóstico iniciado junto às áreas meio, foram selecionados os processos 
organizacionais. Neste estudo, serão apresentados os processos referentes aos 
pagamentos da UFOB: pagamento de materiais de consumo, pagamento de bens 
móveis, pagamento de bolsas de monitoria e pagamento de auxílios financeiros a 
estudantes.
Tendo como escopo o Sistema Integrado de Gestão de Patrimônio, 
Administração e Contratos (SIPAC), módulo do Sistema Integrado de Gestão (SIG), 
adquirido da UFRN e ainda em fase de implantação na UFOB, o GT de Processos 
iniciou o mapeamento (as is) dos processos. Com objetivo de realizar uma análise 
crítica para subsidiar a modelagem (to be), foram definidos os indicadores críticos 
e formas de mensuração. Desta forma, para cada processo foram identificados 06 
(seis) indicadores, sendo: a) quantidade total de processos; b) média de processos 
por mês; c) mediana de dias totais; d) mediana de dias com o fluxo do processo; 
e) média de dias parado aguardando financeiro; e f) total de devoluções dos 
processos. Justifica-se o uso da mediana para dois dos indicadores pelo fato 
de que, ao observar os dados brutos da planilha de dados, percebeu-se grande 
disparidade entre o tempo de duração de processos do mesmo tipo.
Para a extração dessas informações, foi realizado processamento de Big Data 
(gestão de grande volume de dados), com utilização do software Power BI®, uma 
tecnologia de BI desenvolvida pela Microsoft® que, na versão gratuita, é capaz de 
importar dados de tabelas, planilhas eletrônicas, banco de dados, entre outros, e 
processá-los a fim de gerar relatórios visuais e interativos1. Neste caso, os dados 
extraídos (referentes ao período entre a primeira semana de janeiro de 2018 e a 
primeira semana de agosto de 2018) do SIPAC foram dispostos em planilha eletrônica 
1 Fonte: https://docs.microsoft.com/pt-br/power-bi/power-bi-overview. Acesso em 08 de março de 2019.página
374
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 372-377, junho de 2019
Capítulo LX - Monitoramento de indicadores de processos com uso de Business Intelligence (BI)do Excel, compondo um banco de dados que foi posteriormente importado pelo 
Power BI®, que processou e disponibilizou, conforme os comandos inseridos, as 
informações de cada indicador listado, por meio da interface de usuário, como 
apresentado no modelo (Figura 1).
A estrutura do banco de dados fornecida pela instituição não era a ideal 
para a montagem dos indicadores. Portanto, foi necessário o desenvolvimento de 
estratégias através da ferramenta de edição de dados disponibilizada pelo Power 
BI® para a montagem de medidas referentes à duração da tramitação de cada 
processo. 
Figura 1. Entrada e saída do Power BI®
Face a dificuldade, fez-se necessário a identificação de padrões e 
estruturação de filtros para compreender as movimentações específicas para cada 
tipo de processo, uma vez que a única padronização encontrada consistia em 
quais documentos os processos deveriam conter. O tratamento das informações 
demonstrou que: 
• Os processos de pagamento de bens móveis e de pagamento de materiais de 
consumo se caracterizam por terem, respectivamente, o Núcleo de Patrimônio 
e o Núcleo de Materiais como unidade de origem e/ou destino. Considerando 
isso, foram criadas novas tabelas no Power BI®, com a aplicação desses filtros;
• Já os processos de pagamento de bolsas de monitoria e de auxílios financeiros 
a estudantes se apresentaram mais uniformes e, consequentemente, mais 
simples, não havendo necessidade de criar tabelas. Nos relatórios gráficos 
desses processos foi utilizado o recurso filtro de página do Power BI®. Na 
coluna “assunto” da tabela de dados foram utilizadas as palavras-chave 
“monitoria” e “auxílio” .
Também foi utilizado, nos quatro casos, um filtro de página na coluna 
“denominacao” com a palavra-chave “pagamento” . Após isso, de acordo com os 
fluxos as is dos processos mapeados, foram criadas, no Power BI®, as fórmulas das 
medidas para obter cada um dos indicadores exibidos graficamente na interface 
do usuário. 
Ainda, para contabilizar a quantidade de devoluções por setor, também foi 
necessário construir todas as fórmulas das medidas. O exemplo a seguir apresenta 
uma dessas fórmulas, utilizada para encontrar o número de devoluções do Núcleo página
375
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 372-377, junho de 2019
Capítulo LX - Monitoramento de indicadores de processos com uso de Business Intelligence (BI)de Patrimônio para outros setores, no processo de pagamento de bens móveis 
(Figura 2).
Figura 2. Medida criada para o indicador de devoluções do Núcleo de Patrimônio
3. Resultados
A partir da análise dos indicadores de desempenho obtidos foi possível 
constatar a existência de problemas em cada tipo de processo e seus gargalos. Os 
indicadores obtidos são apresentados na Tabela 1:
Tabela 1. Indicadores dos processos
Qtde 
total de 
processosMédia de 
processos 
por mêsMediana 
de dias 
totaisMediana de 
dias sem o 
financeiroMédia de dias 
aguardando 
financeiroTotal de 
devoluções 
Pagamento 
bens móveis94 13,43 24,39 12,53 13,89 19 (20,21%)
Pagamento 
materiais93 13,29 14,19 4,67 14,85 16 (17,20%)
Pagamento 
monitoria57 8,14 5,93 3,03 3,55 7 (12,28%)
Pagamento 
auxílios39 7,80 18,02 4,13 8,94 20 (51,28%)
Mesmo quando o processo chega ao Núcleo de Gestão do Financeiro e 
Pagamento, o pagamento somente será feito quando o Tesouro Nacional liberar o 
financeiro (verba específica para o pagamento do item). Assim, o processo passa a 
integrar uma fila de espera, de acordo com sua prioridade. 
De forma geral, comparando o tempo de duração do fluxo de cada processo 
e o tempo gasto aguardando o financeiro, este representa, em média, mais de 
50% (cinquenta por cento) daquele, o que enquadra essa atividade como o maior 
gargalo nos quatro processos estudados. 
Apesar disso, a liberação do financeiro é uma atividade na qual não foram 
propostas melhorias durante o trabalho do GT de processos, pois envolve a ação de 
um agente externo. Essa observação que foi considerada para o estabelecimento 
das metas de tempo de tramitação. O pagamento de bens móveis, por exemplo, página
376
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 372-377, junho de 2019
Capítulo LX - Monitoramento de indicadores de processos com uso de Business Intelligence (BI)permanece na fila, em média, por 13,89 dias. Dessa forma, o tempo mínimo a ser 
definido é de 14 dias. Considerando-se essa informação, a meta estabelecida para 
este processo foi de 17 dias, sendo 3 dias para a tramitação dos documentos e 14 
dias para efetivação do pagamento. 
Além do tempo gasto, os indicadores revelaram a alta taxa de devoluções 
que ocorrem durante a tramitação, principalmente pela falta de padronização dos 
processos. As devoluções normalmente aconteceram quando os processos foram 
mal instruídos, por ausência de documentos ou encaminhamentos para setores 
incorretos.
As mudanças propostas nos processos pelo grupo de trabalho foram 
aplicadas apenas recentemente. Por esse motivo, não houve tempo suficiente para 
que novos dados fossem coletados para o devido monitoramento dos processos 
redesenhados.
Um dos recursos oferecidos pelo Power BI® é a publicação online dos 
relatórios, o que contribui para a transparência das informações, pois viabiliza o 
acesso e o controle da execução e da eficiência dos processos, não somente para 
os membros do GT e os servidores envolvidos nos processos, mas para qualquer 
pessoa interessada.
Ressalta-se que é possível fazer o monitoramento em tempo real dos 
processos, desde que o Power BI® seja conectado diretamente à fonte de dados. 
Caso essa integração seja feita na instituição, será possível realizar a coleta de 
dados e, ao mesmo tempo, produzir novos indicadores, inclusive gerando alertas 
ao gestor quando um processo não seguir o fluxo determinado ou quando houver 
um grande número de devoluções, ajudando, assim, no aprimoramento constante 
dos fluxos.
4. Conclusão
Ao apresentar o uso de uma solução de BI como suporte ao gerenciamento de 
processos organizacionais, este artigo demonstrou a capacidade do BI como uma 
ferramenta para adquirir conhecimento técnico e prático sobre o desempenho dos 
processos para avaliação e posterior verificação destes. 
As informações obtidas foram importantes para a definição de metas mais 
realísticas para os processos, pois foi possível verificar quanto tempo é gasto, por 
setor, na tramitação de cada um deles. Além disso, o uso da gestão de Big Data  
permitiu identificar altas taxas de devoluções, que consistem na baixa eficiência 
durante a tramitação dos processos. 
O modelo construído no Power BI® pelo GT de Processos se mostrou 
apropriado para obtenção da informação pretendida: indicadores para o 
mapeamento de problemas nos fluxos as is dos processos. Porém, esse modelo 
deve passar por alterações, de forma a se adaptar aos novos fluxos to be, para o 
devido acompanhamento do desempenho dos processos.página
377
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 372-377, junho de 2019
Capítulo LX - Monitoramento de indicadores de processos com uso de Business Intelligence (BI)Referências
ABPMP . Association of Business Process Management Professionals. (2013) Guia para 
o Gerenciamento de Processos de Negócio. Corpo Comum de Conhecimento BPM 
CBOK. v. 3, 1. ed.
ANTONELLI, Ricardo Adriano. (2009) Conhecendo o Business Intelligence (BI): Uma 
Ferramenta de Auxílio à Tomada de Decisão. In: CAP Accounting and Management 
(Online). v. 3, n. 3. Disponível em <http://revistas.utfpr.edu.br/pb/index.php/ CAP/
article/view/933>. Acesso em 08 de março de 2019.
JÚNIOR, Rogerio Henrique de Araújo; SOUZA, Renato Tarciso Barbosa de. (2016) Estudo 
do ecossistema de Big Data para conciliação das demandas de acesso, por meio da 
representação e organização da informação. In: Ciência da Informação. Brasília, v. 45, 
n. 3, p. 187-198.
MICROSOFT DOCS. O que é Power BI? Disponível em <https://docs.microsoft.com/pt-
br/power-bi/power-bi-overview>. Acesso em 07 de março de 2019.
REIS, Eduardo Sguario dos; ANGELONI, Maria Terezinha. (2006) Business Intelligence 
como tecnologia de suporte a definição de estratégias para a melhoria da qualidade 
do ensino. In: Encontro da Associação Nacional de Pós-Graduação e Pesquisa em 
Administração - ENANPAD, 30., 2006, Salvador. Anais... Salvador: ANPAD, p. 1-15.
VUKŠIĆ, Vesna Bosilj; BACH, Mirjana Pejič; POPOVIČ, Aleš. (2013) Supporting 
performance management with business process management and business 
intelligence: A case analysis of integration and orchestration. In: International Journal 
of Information Management. v. 33, n. 4, p. 613–619.página
378
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 378-383, junho de 2019
Capítulo LXI - O mapeamento de processos como elemento facilitador no levantamento e elicitação 
de requisitos do Sistema de Gestão da Universidade Federal de Juiz de ForaO mapeamento de processos como elemento 
facilitador no levantamento e elicitação de 
requisitos do Sistema de Gestão da Universidade 
Federal de Juiz de Fora 
Leonardo Ciuffo1, Fábio Silva de Figueiredo1, Rafael Gurgel Valente Papa1
1Escritório de Processos – Universidade Federal de Juiz de Fora (UF JF)
Juiz de Fora – MG – Brasil – 36.240-000
{Leonardo.ciuffo, fabio.figueiredo, rafael.papa}@ufjf.edu.br
Resumo
A crescente demanda por sistemas de gestão eficientes e que atendam as regras de negócios e as reais necessidades dos 
usuários nas instituições públicas de ensino, tem exigido a busca de novas ferramentas que facilitem o levantamento 
e a elicitação dos requisitos dos sistemas. Este trabalho relata o uso do mapeamento de processos de negócio na 
Universidade Federal de Juiz de fora como facilitador no entendimento entre o usuário e os analistas de TI. Os 
resultados apontam que a partir da adoção desta nova sistemática, os usuários passaram a compreender de forma 
clara os processos e desta forma permitiu o entendimento com os analistas de TI das especificidades do sistema 
requisitado. 
1. Introdução
Nos últimos anos, as universidades federais brasileiras vêm passando por 
um amplo processo de expansão das suas finalidades e atribuições, de forma 
que, atualmente, a estrutura organizacional dessas instituições deve refletir e 
permitir agilidade na prestação de serviços para atender as reais necessidades da 
comunidade acadêmica e dos cidadãos. 
Nesse sentido, é necessária a implantação de sistemas de gestão que 
possibilitem a integração dos processos organizacionais,  a modernização e a 
simplificação dos procedimentos administrativos. Isso requer a implantação de 
sistemas integrados que promovam a eficiência administrativa, através da criação 
e tramitação de processos totalmente em meio eletrônico, tornando-os mais ágeis, 
menos custosos, mais transparentes e eficientes para a prestação de serviços a 
uma sociedade cada vez mais exigente.
No caso das universidades públicas federais o aumento significativo das 
demandas da área finalística e da crescente necessidade da prestação de serviços 
públicos de qualidade, tem exigido formas eficientes de gestão que sejam 
integradas com a Tecnologia da Informação (TI). Conforme Bianchi et al. (2010), 
as instituições são influenciadas por diversas transformações, sejam sociais, 
econômicas, políticas ou tecnológicas, que lhes lançam novos desafios. Nesse 
contexto, elas buscam informação e conhecimento para se diferenciarem e é através página
379
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 378-383, junho de 2019
Capítulo LXI - O mapeamento de processos como elemento facilitador no levantamento e elicitação 
de requisitos do Sistema de Gestão da Universidade Federal de Juiz de Foradas tecnologias da informação que elas conseguem coletar, processar e armazenar 
essas informações e assim otimizar a eficiência e a eficácia da organização.
A Universidade Federal de Juiz de Fora (UF JF) implementou em 2003 o Sistema 
Integrado de Gestão Acadêmica (SIGA), que abrange tanto a área acadêmica quanto 
a área administrativa. Este sistema foi implementado pelos analistas de TI do Centro 
de Gestão do Conhecimento Organizacional (CGCO), que é a unidade responsável 
pelo seu gerenciamento e atualização. É dividido em módulos interligados que 
são implantados de acordo com as demandas da instituição e está adaptado às 
peculiaridades da UF JF. 
O SIGA tem como objetivos principais reduzir a burocracia no trâmite de 
processos, aumentar a transparência, padronizar processos em consonância 
aos padrões internacionais, facilitando assim a execução das tarefas a partir da 
automatização das atividades, gerando melhorias significativas nos processos de 
trabalho.
Entretanto, para que o referido Sistema atenda às reais necessidades dos 
usuários é preciso que as regras de negócio estejam claras e que exista o perfeito 
entendimento entre os analistas de TI e os usuários. O objetivo do presente estudo 
é justamente relatar casos de sucesso da UF JF no uso do mapeamento de processos 
como ferramenta de levantamento e elicitação de requisitos de sistemas de gestão, 
criando uma ligação consistente entre esses dois atores do processo.
2. A TI e o Mapeamento de Processos na UF JF 
Para a implantação de novos módulos do SIGA é necessário se fazer 
primeiramente o levantamento de requisitos do sistema, ou seja, compreender 
com clareza o que o sistema deve fazer para atender às reais necessidades 
do usuário. Na maioria dos casos o usuário não sabe explicar de maneira clara 
e objetiva quais as funcionalidades que o módulo deve apresentar.  Do mesmo 
modo, os analistas de TI, na maioria das vezes, não conseguem compreender as 
informações passadas pelos usuários, o que resulta em falhas, funcionalidades 
incompletas ou inexistentes, retrabalho, módulos mal concebidos etc.
Percebe-se que ao longo do tempo, não foram criadas regras e uma política 
específica para atender às solicitações dos setores para alteração ou criação de 
algum módulo no SIGA. Muitas vezes os pedidos eram feitos sem o devido critério 
e análise por parte do CGCO. Isso passou a acarretar problemas no sistema que 
afetaram seu funcionamento causando erros que, mais tarde, foram detectados e, 
para serem sanados, gerou uma excessiva carga de retrabalho. 
Nesse sentido, no início de 2013, foi criado pela equipe da Pró-Reitoria 
de Planejamento (PROPLAN), um Projeto institucional de Gestão da Qualidade 
(GESQUALI), com intuito de reduzir os problemas administrativos enfrentados 
por gestores nas áreas finalísticas da instituição. Primeiramente foi realizado 
pela PROPLAN um trabalho de campo com coleta de informações diretamente 
nas unidades acadêmicas, para entendimento dos problemas enfrentados página
380
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 378-383, junho de 2019
Capítulo LXI - O mapeamento de processos como elemento facilitador no levantamento e elicitação 
de requisitos do Sistema de Gestão da Universidade Federal de Juiz de Forapelos Diretores. Dentre as demandas levantadas, a necessidade de um sistema 
informatizado de gestão acadêmica eficiente foi uma das três demandas com o 
maior grau de insatisfação (aproximadamente 75%), conforme se pode observar 
na figura a seguir:
Figura 1: Grau de insatisfação com a TI
A partir de uma análise qualitativa desses dados, foi diagnosticada a 
dificuldade de adequação do SIGA em relação às demandas, sem um devido 
levantamento das regras de negócio para que os módulos do sistema atendessem 
de forma plena ao processo de gestão para o qual foram construídos. Como 
consequência, foi levantado que a abordagem de mapeamento de processos seria 
a solução mais adequada para esse problema.
Tal diagnóstico se deve ao fato de que a técnica de mapeamento e modelagem 
dos processos foi utilizada com sucesso na construção do Módulo do Sistema de 
Registro de Preços (SRP) (FIGUEIREDO, et al., 2017), pois para se resolver alguns 
conflitos e facilitar o entendimento dos analistas de TI das regras de negócio do 
sistema, é preciso que exista uma ferramenta facilitadora e uma equipe que faça 
a intermediação entre esses atores, procurando conhecer detalhadamente a 
demanda para, só depois, passá-la à equipe de desenvolvimento. 
Essa prática é definida como o processo de elicitação de requisitos, no qual 
se procura identificar os fatos que compõem os requisitos de sistema, de forma 
a prover o mais correto e completo entendimento do que é demandado naquele 
software. Já o termo engenharia de requisitos refere-se a todas as atividades do 
ciclo de vida relacionadas a requisitos que incluem a coleta, documentação e 
gerenciamento de requisitos (LEITE, 1994).
O mesmo autor afirma que ao final do processo de elicitação de requisitos, 
deve-se demonstrar de maneira documental o entendimento do problema, as 
necessidades do cliente e as oportunidades de melhorias, o que norteará o desenho 
da solução de TI. Nessa atividade, a adoção de diagramas e figuras sempre ajuda 
na documentação e entendimento dos requisitos do sistema.página
381
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 378-383, junho de 2019
Capítulo LXI - O mapeamento de processos como elemento facilitador no levantamento e elicitação 
de requisitos do Sistema de Gestão da Universidade Federal de Juiz de ForaSendo assim, foi criado em 2016 o Escritório de processos (EP), setor que tem 
como missão implantar a Gestão por Processos na UF JF, otimizando a produtividade 
através do mapeamento e automatização dos processos de trabalho. Esse modelo 
de gestão possibilita o correto diagnóstico da situação de cada unidade e permite 
a modelagem desses processos em um patamar de eficiência mais elevado, uma 
vez que, quase invariavelmente, resultam em sistemas de gestão que constituem 
importantes recursos para promover mudanças qualitativas na gestão da 
organização.
A partir desta experiência e da criação do EP , o CGCO passou a adotar uma 
nova sistemática para o desenvolvimento de novos módulos do SIGA. Inicialmente, 
todo o processo de desenvolvimento de sistemas passou a ser previamente 
mapeado e modelado pelo EP para, só então, o processo ser automatizado pelo 
CGCO na plataforma SIGA.
3. Resultados Alcançados
A dinâmica do ciclo BPM (Business Process Management) da UF JF consiste 
basicamente em 3 Fases. Inicialmente em entrevistas feitas pela equipe do EP com 
os usuários envolvidos com o dia a dia dos processos, para se obter informações 
sobre como o processo funciona realmente. Esse processo atual (as is) consiste 
na identificação das atividades, tarefas e dos pontos de decisão, na forma como o 
processo está sendo executado. 
Em seguida à etapa de mapeamento (as is), segue a análise de melhorias do 
processo atual, etapa na qual se identificam as oportunidades de melhorias. Em 
tal fase, apontam-se os gargalos administrativos, os pontos de dificuldade e as 
possíveis otimizações que poderão ser feitas no processo: “melhor distribuição 
de atividades entre os setores envolvidos no processo; formas melhores de se 
executar o processo; possibilidades de informatização do processo; possibilidades 
de desburocratização do processo; entre outras” (BRASIL, 2016, p. 10).
A modelagem do novo processo (to be) é a 3º fase, na qual constrói-se o 
novo diagrama do processo, ou seja, é representado pela sua forma modificada 
realizada na etapa anterior do ciclo BPM, seguindo as oportunidades de melhoria 
levantadas (BRASIL, 2016).
Nas duas fases anteriores, análise de melhorias e modelagem (to be), o 
Escritório de Processos conta com um Analista de TI em sua estrutura para que 
as automatizações necessárias, ou possíveis, no processo sejam devidamente 
analisadas por um profissional da área que, ao mesmo tempo, busca o entendimento 
dos requisitos do negócio (juntamente com os analistas de processos) e promove 
os entendimentos necessários com o CGCO para a correta implementação do novo 
sistema. Essa prática não se restringe apenas ao levantamento de requisitos, mas 
também busca identificar os fatos que compõem aquela solução de negócio e os 
problemas a serem solucionados. página
382
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 378-383, junho de 2019
Capítulo LXI - O mapeamento de processos como elemento facilitador no levantamento e elicitação 
de requisitos do Sistema de Gestão da Universidade Federal de Juiz de ForaNesse sentido, a intermediação da Gestão por Processos através do EP 
na implementação de novos módulos do SIGA, gerou como resultado vários 
benefícios que vão além da elicitação dos requisitos do sistema e automatização 
dos processos. Entre os principais benefícios podemos citar: padronização de 
procedimentos das requisições de novos sistemas, interdependência dos módulos 
(interfuncionalidade dos módulos do SIGA), melhora na comunicação, melhoria 
na interface dos sistemas, mudança na visão dos processos e sistemas (antes, a 
visão era atomizada, agora passa a uma visão mais sistêmica, ou seja, ponta a 
ponta), agilidade na implementação dos novos sistemas, estudo minucioso dos 
instrumentos legais que regem aquele processo e maior visibilidade do usuário 
dos fluxos de processos de trabalho.
Além dos resultados levantados acima, destacam-se dois benefícios que 
foram determinantes na melhoria da eficiência dos sistemas da organização e do 
próprio setor de TI: redução significativa de retrabalho e maior responsabilização 
do usuário pelas funcionalidades do sistema. No que diz respeito à redução de 
retrabalho, relatos dos Analistas de Sistemas do CGCO dão conta de que nos 
sistemas em que os processos foram previamente mapeados e os requisitos foram 
devidamente elicitados, a redução do tempo gasto em correções no sistema 
diminuiu em aproximadamente 80%.
Quanto à responsabilização do usuário pelas funcionalidades do sistema, 
a própria maneira como o ciclo BPM é conduzido na instituição (reuniões 
sistemáticas para mapeamento – as is – e modelagem dos processos – to be) trouxe 
uma mudança de postura dos usuários, no sentido de que eles entenderam que 
são uma peça chave no desenvolvimento do sistema. Com uma participação mais 
intensa, eles deixam de ser participantes passivos do processo e passam a ter 
responsabilidades ativas no produto final.
4. Considerações Finais
O mapeamento de processos vem sendo cada vez mais utilizado no setor 
público e especificamente na Universidade Federal de Juiz de fora (UF JF). Isto por 
adotar uma filosofia de gestão focada na otimização de processos, simplificação 
de fluxos de trabalho, eliminação de gargalos e de atividades que não agregam 
valor, sobretudo por meio da integração com a Tecnologia da Informação (TI) no 
desenvolvimento de sistemas.  Na UF JF tem desempenhado papel primordial na 
integração com a TI como ferramenta de apoio na elicitação de requisitos do SIGA.
Enfim, a adoção da gestão por processos aplicada na UF JF tem sido uma 
ferramenta eficiente no apoio à elicitação e no levantamento de requisitos, 
facilitando o entendimento das regras de negócios pelos usuários e dos requisitos 
pelos analistas de TI. O mapeamento dos processos possibilita que os usuários 
tenham melhor entendimento dos processos e a modelagem dos processos vem 
permitindo a consolidação de uma visão sistêmica dos processos da universidade 
e um avanço na gestão do conhecimento organizacional. A restruturação feita 
a partir da criação do Escritório de Processos na UF JF contribuiu efetivamente 
para a desburocratização de procedimentos administrativos, modernização e 
automatização de processos.página
383
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 378-383, junho de 2019
Capítulo LXI - O mapeamento de processos como elemento facilitador no levantamento e elicitação 
de requisitos do Sistema de Gestão da Universidade Federal de Juiz de ForaReferências
BRASIL. Ministério da Educação. Universidade Federal de Juiz de Fora (2016) “Modelo 
de Governança de Processos – Escritório de Processos (EP)” , Juiz de Fora, MGOP , 18p. 
BPM CBOK (2013) “Guia para o Gerenciamento de Processos de Negócio: corpo comum 
de conhecimento” , versão 3.0, 1 ed., 440p
BIANCHI, I. S. et al. (2010) Tecnologia da Informação no Ambiente Universitário: uma 
contribuição para a gestão do conhecimento. In: X Coloquio Internacional sobre Gestión 
Universitaria en América Del Sur, dez. 2010, Mar del Plata, Argentina. Disponível em: 
<https://repositorio.ufsc.br/handle/123456789/97004> Acesso em: 25 de fev. de 2019.
FIGUEIREDO, F. S.; HONÓRIO, A.; LIMA, S. S. M.; PAPA, R. G. V. (2017), A Trajetória do 
Surgimento da Área de Processos: o estudo da implantação do Escritório de Processos 
em uma Instituição Federal de Ensino Superior, In: XI Workshop de Tecnologia da 
Informação e Comunicação das Instituições Federais de Ensino Superior, mai. 2017, 
Recife, PE. 
LEITE, J.C.S.P . “Engenharia de Requisitos - Notas de Aula” , 1994.
ROCHA, D. T ., TITO, M. T . e TITO, M. A. (2015) “Gestão por Processos na Administração 
Pública” , In: Revista de Direito Público da Procuradoria-Geral do Município de 
Londrina,v. 4, n. 1, p. 51-59.página
384
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 384-389, junho de 2019
Capítulo LXII - Processo de Criação de um Novo Regulamento de Graduação Essencial para a 
Implantação do SIGAAProcesso de Criação de um Novo Regulamento de 
Graduação Essencial para a Implantação do SIGAA
Anne C. O. Rocha1, Raphael F. de A. Patrício1, José A. L. B. de C. Filho1, Fabiana F. do 
Nascimento1, Ayrton N. de S. Silva1
1Superintendência de Tecnologia da Informação (STI) – Universidade Federal da Paraíba (UFPB)
Campus I - Lot. Cidade Universitária, PB, 58051-900 – João Pessoa – PB – Brazil
{caroline,raphael,joseaugusto,fabiana,ayrton}@sti.ufpb.br
Resumo
Ao adquirir o Sistema Integrado de Gestão Acadêmica (SIGAA) da UFRN, foi necessário realizar diversas 
modificações para que ele pudesse ser utilizado pela comunidade da UFPB. Apesar de ambas Instituições Federais 
serem acadêmicas, nem sempre a maneira como elas realizam as atividades são iguais, ou seja, os regulamentos 
que regem o ensino, a pesquisa e a extensão são distintos. Estas diferenças afetam diretamente a especificação de 
requisitos do sistema que é utilizado por essas instituições. Este artigo descreve os desafios enfrentados durante a 
implantação do módulo Graduação e o processo realizado para mapear a relação entre o novo Regulamento dos 
Cursos de Graduação com os requisitos do SIGAA.
Palavras-chave: Processos de Negócio; Desenvolvimento de Sistemas; Regulamento de Graduação;
1. Introdução
Os Sistemas Institucionais Integrados de Gestão (SIG) [UFRN, 2006] foram 
desenvolvidos pela Universidade Federal do Rio Grande do Norte (UFRN) para 
gerir as diferentes áreas da instituição, de forma que todos os dados estivessem 
integrados nos diferentes sistemas da universidade. O SIG possuía 3 subsistemas 
principais: SIPAC, SIGRH e SIGAA. Assim, ao final de 2010, a UFPB firmou um acordo 
de cooperação com UFRN para implantação SIG naquela instituição.
No primeiro ano da cooperação, alguns módulos do SIGRH e do SIPAC 
foram implantados na UFPB. Para isso, foram necessários diversos treinamentos, 
migrações de dados e ajustes no sistema. Nesse tempo, a execução de atividades 
manuais, muitas vezes não padronizadas, compreendeu a maior dificuldade na 
implantação dos subsistemas, demandando cerca de 2 a 4 anos para que alguns 
dos módulos começassem a ser utilizados.
A implantação do SIGAA seguiu em paralelo aos outros 2 subsistemas. Para 
isso, foram definidos vários processos de desenvolvimento na Superintendência 
de Tecnologia da Informação (STI) para adaptar os módulos do SIGAA à realidade 
da UFPB. Nos 3 primeiros anos da cooperação, houve a tentativa de implantar os 
módulos: Biblioteca, Diplomas, Graduação, Lato Sensu, Stricto Sensu e Técnico. 
Porém, como havia muitas divergências entre as regras de negócio da UFRN e da 
UFPB, a implantação do módulo Graduação teve que ser cancelada.página
385
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 384-389, junho de 2019
Capítulo LXII - Processo de Criação de um Novo Regulamento de Graduação Essencial para a 
Implantação do SIGAAA ausência de um regimento geral para a graduação provou ser a maior 
dificuldade para implantação do módulo da Graduação. Desta forma, ao final de 
2013, foi necessário que a Pró-Reitoria de Graduação (PRG) com a colaboração da 
STI, descrevessem um novo regulamento a fim de centralizar todas as regras dos 
cursos de graduação em um único documento e que fosse aprovado pelo Conselho 
Superior de Ensino, Pesquisa e Extensão (CONSEPE).
Neste artigo será apresentada a metodologia desenvolvida pela STI para 
mapear os processos de negócio da PRG, que teve a finalidade de viabilizar a 
validação dos casos de uso (requisitos de sistema) definidos pela UFRN para o 
módulo Graduação do SIGAA. Com isso, foi possível validar se os requisitos do 
sistema atendiam às necessidades da UFPB ou não. Os processos de negócio 
foram descritos na notação BPM [ABPMP , 2013] e toda a documentação do sistema 
foi descrita na Wiki da STI.
2. Métodos
Durante o levantamento de requisitos para implantação do módulo 
Graduação do SIGAA na UFPB, foi iniciado na STI um projeto para Mapeamento 
dos Processos de Negócio da Graduação. Em paralelo, a PRG iniciou a elaboração 
do Regulamento dos Cursos Regulares de Graduação da UFPB [PRG/UFPB, 2015], 
que seria posteriormente aprovado pelo CONSEPE.
De acordo com o processo descrito na Figura 1, inicialmente, analistas 
de sistemas da STI definiram assuntos gerais para o módulo Graduação. Estes 
assuntos foram utilizados pelos analistas de negócio para descreverem, em BPM, 
os processos de negócio da graduação, com isso ficou mais fácil associar cada BPM 
aos casos de uso do módulo. Por fim, após a análise de cada caso de uso, foi criada 
uma lista de divergências entre o sistema da UFRN e os processos de negócio da 
graduação da UFPB. Estas divergências foram avaliadas pela PRG, o que gerou 
atualização nos casos de uso.
Figura 1. Modelo BPM para Mapeamento dos Processos de Negócio da UFPB x Sistema 
de Graduação do SIGAA da UFRN.página
386
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 384-389, junho de 2019
Capítulo LXII - Processo de Criação de um Novo Regulamento de Graduação Essencial para a 
Implantação do SIGAANa Figura 2, cada requisito do sistema foi analisado pela STI e parte das 
divergências entre o sistema e o regulamento da graduação foram aprovadas ou 
negadas pela PRG, de forma que o sistema pudesse atender os principais artigos, 
já que tanto o regulamento quanto o sistema teriam que entrar em vigência ao 
mesmo tempo.
Com esses processos, a STI conseguiu uma maneira de avaliar se o módulo 
Graduação do SIGAA da UFRN poderia ser utilizado pela comunidade acadêmica 
da UFPB, o que contribuiu também para criação e aprovação do novo regulamento 
dos cursos de graduação da UFPB.
Figura 2. Modelo BPMN para Construção do Regulamento de Graduação da UFPB.
3. Resultados
Durante o processo de Mapeamento dos Processos de Negócio da Graduação 
da UFPB, foram criados 12 assuntos gerais. Na Tabela 1, pode-se observar toda 
a análise realizada pela STI para aprovação de cada requisito do sistema, de 
forma que atendessem os artigos definidos na minuta do novo Regulamento da 
Graduação.página
387
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 384-389, junho de 2019
Capítulo LXII - Processo de Criação de um Novo Regulamento de Graduação Essencial para a 
Implantação do SIGAATabela 1. Situação final dos artefatos criados para cada assunto geral até a 
implantação do módulo Graduação.
Como a equipe responsável por criar os BPM para os assuntos gerais era 
composta por estagiários de outro setor da UFPB, com o término do estágio, a 
lista de modelos de negócio em BPM ficou incompleta, como pode ser visto na 
indicação “N” na segunda coluna da Tabela 1. Para contornar este fato, foram 
criados questionários para cada assunto com dúvidas baseadas na minuta do 
novo regulamento da UFPB. Com a resposta da PRG aos questionários, foram 
identificadas divergências entre o sistema da UFRN e o regulamento em todos os 
assuntos, como pode-se observar na indicação “S” , na terceira coluna da Tabela 1. 
Dentre as divergências encontradas, a PRG validou alguns assuntos Parcialmente 
(P) e outros assuntos Totalmente (T), como pode ser visto na quarta coluna da 
mesma tabela. Por fim, na última coluna da tabela, em todos os assuntos gerais 
houve a necessidade de modificar partes do módulo graduação.
Com a criação da minuta do regulamento pela PRG, foi necessário elaborar 
um documento com o escopo inicial do projeto de desenvolvimento. Após 8 
meses de desenvolvimento, percebeu-se que houve alteração em vários artigos 
da minuta. Com isso, foi feito um estudo de viabilidade de implantação do 
sistema. Pois, havia a necessidade de implantar o sistema ao mesmo tempo que o 
regulamento fosse aprovado pelo CONSEPE. A Figura 3 apresenta quantos artigos 
do regulamento eram programáveis e deveriam fazer parte do módulo Graduação 
do SIGAA. Do total dos 308 artigos da minuta, 195 artigos, ou seja, 63,3% deveriam 
ser implementados no sistema.página
388
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 384-389, junho de 2019
Capítulo LXII - Processo de Criação de um Novo Regulamento de Graduação Essencial para a 
Implantação do SIGAA
Figura 3. Gráfico da análise do número de artigos do regulamento poderiam fazer 
parte do SIGAA.
Dentre os 195 artigos que deveriam fazer parte do sistema, foi analisada a 
situação dos ajustes já em andamento e a necessidade de alteração do sistema 
com relação a cada artigo do regulamento.
Como pode ser visto no gráfico da Figura 4, na faixa Verde, 55,3% dos artigos 
já estavam em conformidade com o sistema. Na faixa Azul, 4,5% dos artigos não 
sofreu alteração, mas estava pendente de implementação do sistema. Na faixa 
Vermelha 22% dos artigos foram modificados e precisavam alterar partes do 
sistema que já tinham sido adaptadas para UFPB. Na faixa Laranja, 18,2% dos 
artigos eram novos e precisavam ser implementados completamente.
Figura 4. Gráfico da análise da quantidade de artigos do regulamento que demandam 
implementação ou alteração no SIGAA.página
389
WORKSHOP DE TECNOLOGIA DA INFORMAÇÃO E COMUNICAÇÃO DAS INSTITUIÇÕES FEDERAIS DE ENSINO SUPERIOR
Páginas 384-389, junho de 2019
Capítulo LXII - Processo de Criação de um Novo Regulamento de Graduação Essencial para a 
Implantação do SIGAADesta forma, pode-se perceber que este processo de mapeamento de negócio, 
tanto o sistema quanto os artigos do regulamento tiveram que ser modificados, 
para que fosse viável a implantação do módulo Graduação.
4. Conclusão
Este artigo visa contribuir como relato de experiência para outras instituições 
federais que estão passando pelo mesmo processo de implantação do SIG. Assim, 
foi apresentado o processo de implantação do módulo Graduação do SIGAA na 
UFPB, que durou cerca de 5 anos. Sendo que deste período, os últimos 2 anos 
foi o tempo necessário para aprovação do novo regulamento e implementação 
do sistema adaptado. Quando o módulo Graduação começou a ser utilizado, nem 
todos os artigos do regulamento estavam implementados no sistema, pois ele foi 
descrito prevendo que alguns artigos só entrariam em vigência 1 ou 2 períodos 
letivos após sua aprovação.
A principal dificuldade enfrentada foi a comunicação, pois nem sempre teve 
uma única pessoa para responder as dúvidas sobre as divergências do sistema. 
Com a troca de gestão na Reitoria, parte do que havia sido definido por um gestor 
foi alterado pelo seu sucessor. Hoje, o módulo de Graduação está sendo utilizado 
por todos os 4 Campi da UFPB de forma satisfatória. Implantar o SIG na UFPB trouxe 
diversos benefícios: melhorou a distribuição dos recursos; reduziu custos; reduziu 
tempo de execução de tarefas; melhorou a confiança nas informações; melhorou o 
planejamento; integrou todos os setores da UFPB; entre outros.
Referências
ABPMP . (2013) “Guia BPM CBOK v 3.0 ” . 1ª edição.
PRG/UFPB. (2015) “Regulamento dos Cursos Regulares de Graduação da UFPB” , http://
www.prg.ufpb.br/antigo/node/857/, Fevereiro/2019.
UFRN. (2006) “Sistemas Institucionais Integrados de Gestão - SIG” , https://docs.info.
ufrn.br/, Fevereiro/2019.ORGANIZAÇÃO
PATROCÍNIOPROMOÇÃO
APOIO
ISBN 978-85-67619-01-9